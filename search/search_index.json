{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>SDK for batch processing tabular datasets with LLMs. Built on LlamaIndex for provider abstraction, adds batch orchestration, automatic cost tracking, checkpointing, and YAML configuration for dataset transformation at scale.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Quick API: 3-line hello world with smart defaults and auto-detection</li> <li>Simple API: Fluent builder pattern for full control when needed</li> <li>Reliability: Automatic retries, checkpointing, error policies (99.9% completion rate)</li> <li>Cost Control: Pre-execution estimation, budget limits, real-time tracking</li> <li>Observability: LlamaIndex-powered automatic LLM tracking (Langfuse, OpenTelemetry), progress bars, cost reports</li> <li>Extensibility: Plugin architecture, custom stages, multiple LLM providers</li> <li>Production Ready: Zero data loss on crashes, resume from checkpoint</li> <li>Multiple Providers: OpenAI, Azure OpenAI (with Managed Identity), Anthropic Claude, Groq, MLX (Apple Silicon), and custom APIs</li> <li>Local Inference: Run models locally with MLX (Apple Silicon) or Ollama - 100% free, private, offline-capable</li> <li>Multi-Column Processing: Generate multiple output columns with composition or JSON parsing</li> <li>Custom Providers: Integrate any OpenAI-compatible API (Together.AI, vLLM, Ollama, custom endpoints)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#option-1-quick-api-recommended","title":"Option 1: Quick API (Recommended)","text":"<p>The simplest way to get started - just provide your data, prompt, and model:</p> <pre><code>from ondine import QuickPipeline\n\n# Process data with smart defaults\npipeline = QuickPipeline.create(\n    data=\"data.csv\",\n    prompt=\"Clean this text: {description}\",\n    model=\"gpt-4o-mini\"\n)\n\n# Execute pipeline\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.processed_rows} rows\")\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\n</code></pre> <p>What's auto-detected:</p> <ul> <li>Input columns from <code>{placeholders}</code> in prompt</li> <li>Provider from model name (gpt-4 \u2192 openai, claude \u2192 anthropic)</li> <li>Parser type (JSON for multi-column, text for single column)</li> <li>Sensible batch size and concurrency for the provider</li> </ul>"},{"location":"#option-2-builder-api-full-control","title":"Option 2: Builder API (Full Control)","text":"<p>For advanced use cases requiring explicit configuration:</p> <pre><code>from ondine import PipelineBuilder\n\n# Build with explicit settings\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"description\"],\n              output_columns=[\"cleaned\"])\n    .with_prompt(\"Clean this text: {description}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_batch_size(100)\n    .with_concurrency(5)\n    .build()\n)\n\n# Estimate cost before running\nestimate = pipeline.estimate_cost()\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\n\n# Execute pipeline\nresult = pipeline.execute()\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>Install with pip or uv:</p> <pre><code>pip install ondine\n</code></pre> <p>Or with optional dependencies:</p> <pre><code># For Apple Silicon local inference\npip install ondine[mlx]\n\n# For Azure Managed Identity (keyless auth)\npip install ondine[azure]\n\n# Observability is now built-in (OpenTelemetry + Langfuse)\n# No separate install needed!\n\n# For development\npip install ondine[dev]\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Quickstart - Your first pipeline in 5 minutes</li> <li>Core Concepts - Understanding pipelines, stages, and specifications</li> <li>Execution Modes - When to use sync, async, or streaming</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Ondine excels at:</p> <ul> <li>Data cleaning and normalization (PII detection, standardization)</li> <li>Content enrichment (classification, tagging, summarization)</li> <li>Extraction tasks (structured data from unstructured text)</li> <li>Translation and localization at scale</li> <li>Synthetic data generation with cost controls</li> <li>Quality assurance (validation, scoring, feedback)</li> </ul>"},{"location":"#why-ondine","title":"Why Ondine?","text":"<ul> <li>Reliable: Checkpointing, auto-retry, budget controls, observability</li> <li>Developer-Friendly: Fluent API, YAML config, CLI tools, extensive examples</li> <li>Cost-Aware: Pre-run estimation, real-time tracking, budget limits</li> <li>Flexible: Multiple providers, custom stages, extensible architecture</li> <li>Well-Tested: 95%+ code coverage, integration tests with real APIs</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"contributing/","title":"Contributing to Ondine","text":"<p>Thank you for your interest in contributing to Ondine!</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Fork and Clone <pre><code>git clone https://github.com/YOUR_USERNAME/Ondine.git\ncd Ondine\n</code></pre></p> </li> <li> <p>Set Up Development Environment <pre><code># Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install dependencies\nuv sync --extra dev --extra observability\n\n# Install pre-commit hooks\nuv run pre-commit install\n</code></pre></p> </li> <li> <p>Create a Branch <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=ondine --cov-report=html\n\n# Run specific test file\nuv run pytest tests/unit/test_pipeline_builder.py\n\n# Run with verbose output\nuv run pytest -v\n</code></pre>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<pre><code># Format code\nuv run ruff format ondine/ tests/\n\n# Lint code\nuv run ruff check ondine/ tests/\n\n# Type check\nuv run mypy ondine/\n\n# Run all quality checks\njust lint\n</code></pre>"},{"location":"contributing/#using-justfile","title":"Using Justfile","text":"<p>We provide a <code>justfile</code> for common tasks:</p> <pre><code># Run tests\njust test\n\n# Run tests with coverage\njust test-coverage\n\n# Format and lint\njust format\njust lint\n\n# Run all checks\njust check\n\n# View all available commands\n   just --list\n</code></pre>"},{"location":"contributing/#code-guidelines","title":"Code Guidelines","text":""},{"location":"contributing/#style","title":"Style","text":"<ul> <li>Follow PEP 8 and the Zen of Python</li> <li>Use type hints for all function signatures</li> <li>Keep functions small and focused (KISS principle)</li> <li>Write self-documenting code with clear variable names</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features (TDD encouraged)</li> <li>Maintain or improve test coverage (currently 95%+)</li> <li>Include both unit and integration tests where appropriate</li> <li>Use descriptive test names: <code>test_&lt;what&gt;_&lt;when&gt;_&lt;expected&gt;</code></li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update README.md if adding user-facing features</li> <li>Add docstrings to all public functions and classes</li> <li>Include examples for new features in <code>examples/</code> directory</li> <li>Update CHANGELOG.md following Keep a Changelog</li> </ul>"},{"location":"contributing/#commits","title":"Commits","text":"<ul> <li>Write clear, descriptive commit messages</li> <li>Use conventional commits format:</li> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation changes</li> <li><code>test:</code> - Test additions or changes</li> <li><code>refactor:</code> - Code refactoring</li> <li><code>chore:</code> - Maintenance tasks</li> </ul> <p>Example: <pre><code>feat: add support for custom retry strategies\n\n- Implement RetryStrategy interface\n- Add exponential backoff with jitter\n- Update documentation\n</code></pre></p>"},{"location":"contributing/#architecture","title":"Architecture","text":"<p>Ondine follows a 5-layer clean architecture:</p> <ol> <li>Core - Domain models and business logic</li> <li>Adapters - External integrations (LLM clients, I/O)</li> <li>Stages - Pipeline processing stages</li> <li>Orchestration - Execution strategies and state management</li> <li>API - Public interfaces (builders, composers)</li> </ol>"},{"location":"contributing/#plugin-system","title":"Plugin System","text":"<p>Ondine uses decorators for extensibility:</p> <ul> <li><code>@provider</code> - Register custom LLM providers</li> <li><code>@stage</code> - Register custom pipeline stages</li> </ul> <p>See <code>examples/15_custom_llm_provider.py</code> and <code>examples/16_custom_pipeline_stage.py</code> for details.</p>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"contributing/#adding-a-new-llm-provider","title":"Adding a New LLM Provider","text":"<ol> <li>Create a new class inheriting from <code>BaseLLMProvider</code></li> <li>Implement <code>invoke()</code> and <code>estimate_tokens()</code> methods</li> <li>Register with <code>@provider(\"your_provider_name\")</code></li> <li>Add tests in <code>tests/unit/test_providers.py</code></li> <li>Add example in <code>examples/</code></li> <li>Update README.md</li> </ol>"},{"location":"contributing/#adding-a-new-pipeline-stage","title":"Adding a New Pipeline Stage","text":"<ol> <li>Create a new class inheriting from <code>PipelineStage</code></li> <li>Implement <code>execute()</code> method</li> <li>Register with <code>@stage(\"your_stage_name\")</code></li> <li>Add tests in <code>tests/unit/test_stages.py</code></li> <li>Add example in <code>examples/</code></li> <li>Update documentation</li> </ol>"},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<ol> <li>Check if the bug is already reported in Issues</li> <li>If not, create a new issue with:</li> <li>Clear title and description</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (OS, Python version, Ondine version)</li> <li>Minimal reproducible example</li> </ol>"},{"location":"contributing/#suggesting-features","title":"Suggesting Features","text":"<ol> <li>Check existing feature requests</li> <li>Open a new issue with:</li> <li>Clear use case description</li> <li>Proposed API or interface</li> <li>Example usage</li> <li>Why this would benefit users</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Before submitting:</li> <li>Ensure all tests pass: <code>just test</code></li> <li>Run code quality checks: <code>just check</code></li> <li>Update documentation if needed</li> <li> <p>Add entry to CHANGELOG.md</p> </li> <li> <p>Submit PR:</p> </li> <li>Write a clear title and description</li> <li>Reference related issues (e.g., \"Fixes #123\")</li> <li>Ensure CI passes (tests, linting, security)</li> <li> <p>Respond to code review feedback</p> </li> <li> <p>After approval:</p> </li> <li>Maintainers will merge your PR</li> <li>Your contribution will be included in the next release!</li> </ol>"},{"location":"contributing/#good-first-issues","title":"Good First Issues","text":"<p>Look for issues labeled <code>good first issue</code> - these are great for newcomers!</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Questions? Open a Discussion</li> <li>Bugs? Open an Issue</li> <li>Chat? Join our community (link coming soon)</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Focus on constructive feedback</li> <li>Help others learn and grow</li> </ul>"},{"location":"contributing/#thank-you","title":"Thank You!","text":"<p>Every contribution, no matter how small, makes Ondine better. We appreciate your time and effort!</p> <p>Happy coding!</p>"},{"location":"api/","title":"ondine","text":""},{"location":"api/#ondine","title":"ondine","text":"<p>LLM Dataset Processing Engine.</p> <p>An SDK for processing tabular datasets using Large Language Models with reliability, observability, and cost control.</p>"},{"location":"api/#ondine.DatasetProcessor","title":"DatasetProcessor","text":"<pre><code>DatasetProcessor(data: str | DataFrame, input_column: str, output_column: str, prompt: str, llm_config: dict[str, any])\n</code></pre> <p>Simplified API for single-prompt, single-column use cases.</p> <p>This is a convenience wrapper around PipelineBuilder for users who don't need fine-grained control.</p> Example <p>processor = DatasetProcessor(     data=\"data.csv\",     input_column=\"description\",     output_column=\"cleaned\",     prompt=\"Clean this text: {description}\",     llm_config={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\"} ) result = processor.run()</p> <p>Initialize dataset processor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>CSV file path or DataFrame</p> required <code>input_column</code> <code>str</code> <p>Input column name</p> required <code>output_column</code> <code>str</code> <p>Output column name</p> required <code>prompt</code> <code>str</code> <p>Prompt template</p> required <code>llm_config</code> <code>dict[str, any]</code> <p>LLM configuration dict</p> required Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    input_column: str,\n    output_column: str,\n    prompt: str,\n    llm_config: dict[str, any],\n):\n    \"\"\"\n    Initialize dataset processor.\n\n    Args:\n        data: CSV file path or DataFrame\n        input_column: Input column name\n        output_column: Output column name\n        prompt: Prompt template\n        llm_config: LLM configuration dict\n    \"\"\"\n    self.data = data\n    self.input_column = input_column\n    self.output_column = output_column\n    self.prompt = prompt\n    self.llm_config = llm_config\n\n    # Build pipeline internally\n    builder = PipelineBuilder.create()\n\n    # Configure data source\n    if isinstance(data, str):\n        builder.from_csv(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    elif isinstance(data, pd.DataFrame):\n        builder.from_dataframe(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    else:\n        raise ValueError(\"data must be file path or DataFrame\")\n\n    # Configure prompt\n    builder.with_prompt(prompt)\n\n    # Configure LLM\n    provider = llm_config.get(\"provider\", \"openai\")\n    model = llm_config.get(\"model\", \"gpt-4o-mini\")\n    api_key = llm_config.get(\"api_key\")\n    temperature = llm_config.get(\"temperature\", 0.0)\n    max_tokens = llm_config.get(\"max_tokens\")\n\n    builder.with_llm(\n        provider=provider,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    # Build pipeline\n    self.pipeline = builder.build()\n</code></pre>"},{"location":"api/#ondine.DatasetProcessor.run","title":"run","text":"<pre><code>run() -&gt; pd.DataFrame\n</code></pre> <p>Execute processing and return results.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute processing and return results.\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    result = self.pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/#ondine.DatasetProcessor.run_sample","title":"run_sample","text":"<pre><code>run_sample(n: int = 10) -&gt; pd.DataFrame\n</code></pre> <p>Test on first N rows.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of rows to process</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with sample results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run_sample(self, n: int = 10) -&gt; pd.DataFrame:\n    \"\"\"\n    Test on first N rows.\n\n    Args:\n        n: Number of rows to process\n\n    Returns:\n        DataFrame with sample results\n    \"\"\"\n    # Create sample pipeline\n    if isinstance(self.data, str):\n        df = pd.read_csv(self.data).head(n)\n    else:\n        df = self.data.head(n)\n\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df,\n            input_columns=[self.input_column],\n            output_columns=[self.output_column],\n        )\n        .with_prompt(self.prompt)\n        .with_llm(\n            provider=self.llm_config.get(\"provider\", \"openai\"),\n            model=self.llm_config.get(\"model\", \"gpt-4o-mini\"),\n            api_key=self.llm_config.get(\"api_key\"),\n            temperature=self.llm_config.get(\"temperature\", 0.0),\n        )\n    )\n\n    sample_pipeline = builder.build()\n    result = sample_pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/#ondine.DatasetProcessor.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; float\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost in USD</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def estimate_cost(self) -&gt; float:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Estimated cost in USD\n    \"\"\"\n    estimate = self.pipeline.estimate_cost()\n    return float(estimate.total_cost)\n</code></pre>"},{"location":"api/#ondine.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(specifications: PipelineSpecifications, dataframe: DataFrame | None = None, executor: ExecutionStrategy | None = None)\n</code></pre> <p>Main pipeline class - Facade for dataset processing.</p> <p>Provides high-level interface for building and executing LLM-powered data transformations. Handles orchestration, state management, cost tracking, checkpointing, and error handling.</p> <p>This is typically created via PipelineBuilder or QuickPipeline, not directly.</p> Example <pre><code>from ondine import PipelineBuilder\n\n# Create via builder (recommended)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Execute\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.total_rows} rows\")\nprint(f\"Cost: ${result.costs.total_cost}\")\n</code></pre> Note <p>Use PipelineBuilder for construction, not direct instantiation.</p> <p>Initialize pipeline with specifications.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Complete pipeline configuration</p> required <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded DataFrame</p> <code>None</code> <code>executor</code> <code>ExecutionStrategy | None</code> <p>Optional execution strategy (default: SyncExecutor)</p> <code>None</code> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def __init__(\n    self,\n    specifications: PipelineSpecifications,\n    dataframe: pd.DataFrame | None = None,\n    executor: ExecutionStrategy | None = None,\n):\n    \"\"\"\n    Initialize pipeline with specifications.\n\n    Args:\n        specifications: Complete pipeline configuration\n        dataframe: Optional pre-loaded DataFrame\n        executor: Optional execution strategy (default: SyncExecutor)\n    \"\"\"\n    self.id = uuid4()\n    self.specifications = specifications\n    self.dataframe = dataframe\n    self.executor = executor or SyncExecutor()\n    self.observers: list[ExecutionObserver] = []\n    self.logger = get_logger(f\"{__name__}.{self.id}\")\n</code></pre>"},{"location":"api/#ondine.Pipeline.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; Pipeline\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"Pipeline\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/#ondine.Pipeline.validate","title":"validate","text":"<pre><code>validate() -&gt; ValidationResult\n</code></pre> <p>Validate pipeline configuration.</p> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with any errors/warnings</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def validate(self) -&gt; ValidationResult:\n    \"\"\"\n    Validate pipeline configuration.\n\n    Returns:\n        ValidationResult with any errors/warnings\n    \"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Validate dataset spec\n    if not self.specifications.dataset.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    if not self.specifications.dataset.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    # Validate that input columns exist in dataframe (if dataframe is provided)\n    if self.dataframe is not None and self.specifications.dataset.input_columns:\n        df_cols = set(self.dataframe.columns)\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_cols = input_cols - df_cols\n        if missing_cols:\n            result.add_error(\n                f\"Input columns not found in dataframe: {missing_cols}\"\n            )\n\n    # Validate prompt spec\n    if not self.specifications.prompt.template:\n        result.add_error(\"No prompt template specified\")\n    else:\n        # Check that template variables match input columns\n        import re\n\n        template_vars = set(\n            re.findall(r\"\\{(\\w+)\\}\", self.specifications.prompt.template)\n        )\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_vars = template_vars - input_cols\n        if missing_vars:\n            result.add_error(\n                f\"Template variables not in input columns: {missing_vars}\"\n            )\n\n    # Validate LLM spec\n    if not self.specifications.llm.model:\n        result.add_error(\"No LLM model specified\")\n\n    return result\n</code></pre>"},{"location":"api/#ondine.Pipeline.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; CostEstimate\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def estimate_cost(self) -&gt; CostEstimate:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    # Create stages\n    loader = DataLoaderStage(self.dataframe)\n\n    # Load first few rows for estimation\n    df = loader.process(self.specifications.dataset, ExecutionContext())\n    sample_size = min(10, len(df))\n    sample_df = df.head(sample_size)\n\n    # Create formatter and get prompts\n    formatter = PromptFormatterStage(\n        self.specifications.processing.batch_size,\n        use_jinja2=self.specifications.processing.use_jinja2,\n    )\n    batches = formatter.process(\n        (sample_df, self.specifications.prompt), ExecutionContext()\n    )\n\n    # Create LLM client and estimate\n    llm_client = create_llm_client(self.specifications.llm)\n    llm_stage = LLMInvocationStage(llm_client)\n\n    sample_estimate = llm_stage.estimate_cost(batches)\n\n    # Scale to full dataset\n    scale_factor = Decimal(len(df)) / Decimal(sample_size)\n\n    return CostEstimate(\n        total_cost=sample_estimate.total_cost * scale_factor,\n        total_tokens=int(sample_estimate.total_tokens * float(scale_factor)),\n        input_tokens=int(sample_estimate.input_tokens * float(scale_factor)),\n        output_tokens=int(sample_estimate.output_tokens * float(scale_factor)),\n        rows=len(df),\n        confidence=\"sample-based\",\n    )\n</code></pre>"},{"location":"api/#ondine.Pipeline.execute","title":"execute","text":"<pre><code>execute(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Runs all stages: data loading, prompt formatting, LLM invocation, response parsing, and result writing. Handles checkpointing, cost tracking, and error recovery.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint (for fault tolerance)</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult containing: - data: DataFrame with results - metrics: Processing statistics (total_rows, success_count, etc.) - costs: Cost breakdown (total_cost, input_tokens, output_tokens) - duration: Execution time in seconds - errors: List of any errors encountered</p> Example <pre><code># Execute pipeline\nresult = pipeline.execute()\n\n# Access results\nprint(f\"Processed: {result.metrics.total_rows} rows\")\nprint(f\"Successful: {result.metrics.success_count} rows\")\nprint(f\"Cost: ${result.costs.total_cost}\")\nprint(f\"Time: {result.duration:.2f}s\")\n\n# Access output data\nresult.data.to_csv(\"output.csv\", index=False)\n\n# Resume from checkpoint (if pipeline was interrupted)\nresult = pipeline.execute(resume_from=previous_session_id)\n</code></pre> Note <p>Progress is automatically saved via checkpoints. If execution fails, use resume_from to continue from the last checkpoint.</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Runs all stages: data loading, prompt formatting, LLM invocation, response parsing,\n    and result writing. Handles checkpointing, cost tracking, and error recovery.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint (for fault tolerance)\n\n    Returns:\n        ExecutionResult containing:\n            - data: DataFrame with results\n            - metrics: Processing statistics (total_rows, success_count, etc.)\n            - costs: Cost breakdown (total_cost, input_tokens, output_tokens)\n            - duration: Execution time in seconds\n            - errors: List of any errors encountered\n\n    Example:\n        ```python\n        # Execute pipeline\n        result = pipeline.execute()\n\n        # Access results\n        print(f\"Processed: {result.metrics.total_rows} rows\")\n        print(f\"Successful: {result.metrics.success_count} rows\")\n        print(f\"Cost: ${result.costs.total_cost}\")\n        print(f\"Time: {result.duration:.2f}s\")\n\n        # Access output data\n        result.data.to_csv(\"output.csv\", index=False)\n\n        # Resume from checkpoint (if pipeline was interrupted)\n        result = pipeline.execute(resume_from=previous_session_id)\n        ```\n\n    Note:\n        Progress is automatically saved via checkpoints. If execution fails,\n        use resume_from to continue from the last checkpoint.\n    \"\"\"\n    # Validate first\n    validation = self.validate()\n    if not validation.is_valid:\n        raise ValueError(f\"Pipeline validation failed: {validation.errors}\")\n\n    # Create or restore execution context\n    state_manager = StateManager(\n        storage=LocalFileCheckpointStorage(\n            self.specifications.processing.checkpoint_dir\n        ),\n        checkpoint_interval=self.specifications.processing.checkpoint_interval,\n    )\n\n    if resume_from:\n        # Resume from checkpoint\n        context = state_manager.load_checkpoint(resume_from)\n        if not context:\n            raise ValueError(f\"No checkpoint found for session {resume_from}\")\n        self.logger.info(\n            f\"Resuming from checkpoint at row {context.last_processed_row}\"\n        )\n    else:\n        # Create new context\n        context = ExecutionContext(pipeline_id=self.id)\n\n    # Add default observers if none specified\n    if not self.observers:\n        self.observers = [\n            ProgressBarObserver(),\n            LoggingObserver(),\n            CostTrackingObserver(),\n        ]\n\n    # Attach observers to context for progress notifications\n    context.observers = self.observers\n\n    # Initialize new observability system if observers configured\n    observer_configs = self.specifications.metadata.get(\"observers\", [])\n    if observer_configs:\n        from ondine.observability.dispatcher import ObserverDispatcher\n        from ondine.observability.registry import ObserverRegistry\n\n        # Instantiate observers from configuration\n        new_observers = []\n        for observer_name, observer_config in observer_configs:\n            try:\n                observer_class = ObserverRegistry.get(observer_name)\n                observer_instance = observer_class(config=observer_config)\n                new_observers.append(observer_instance)\n                self.logger.info(f\"Initialized observer: {observer_name}\")\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to initialize observer '{observer_name}': {e}\"\n                )\n\n        # Create dispatcher and attach to context\n        if new_observers:\n            context.observer_dispatcher = ObserverDispatcher(new_observers)\n\n            # Emit pipeline start event\n            from ondine.observability.events import PipelineStartEvent\n\n            start_event = PipelineStartEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                config={},\n                metadata=self.specifications.metadata,\n                total_rows=0,  # Will be updated after data loading\n            )\n            context.observer_dispatcher.dispatch(\"pipeline_start\", start_event)\n\n    # Notify legacy observers of start\n    for observer in self.observers:\n        observer.on_pipeline_start(self, context)\n\n    try:\n        # Execute stages (preprocessing happens inside if enabled)\n        result_df = self._execute_stages(context, state_manager)\n\n        # Mark completion\n        context.end_time = datetime.now()\n\n        # Create execution result\n        # Extract token tracking from intermediate_data (populated by LLMInvocationStage)\n        token_tracking = context.intermediate_data.get(\"token_tracking\", {})\n        input_tokens = token_tracking.get(\"input_tokens\", 0)\n        output_tokens = token_tracking.get(\"output_tokens\", 0)\n\n        result = ExecutionResult(\n            data=result_df,\n            metrics=context.get_stats(),\n            costs=CostEstimate(\n                total_cost=context.accumulated_cost,\n                total_tokens=context.accumulated_tokens,\n                input_tokens=input_tokens,\n                output_tokens=output_tokens,\n                rows=context.total_rows,\n                confidence=\"actual\",\n            ),\n            execution_id=context.session_id,\n            start_time=context.start_time,\n            end_time=context.end_time,\n            success=True,\n        )\n\n        # Optional: Auto-retry failed rows\n        if self.specifications.processing.auto_retry_failed:\n            # Get preprocessed data from context (or loaded data if no preprocessing)\n            retry_source_df = context.intermediate_data.get(\"preprocessed_data\")\n            if retry_source_df is None:\n                retry_source_df = context.intermediate_data.get(\"loaded_data\")\n            result = self._auto_retry_failed_rows(result, retry_source_df)\n\n        # Cleanup checkpoints on success\n        state_manager.cleanup_checkpoints(context.session_id)\n\n        # Notify legacy observers of completion\n        for observer in self.observers:\n            observer.on_pipeline_complete(context, result)\n\n        # Emit pipeline end event for new observability system\n        if context.observer_dispatcher:\n            from ondine.observability.events import PipelineEndEvent\n\n            end_event = PipelineEndEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                success=True,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                total_duration_ms=(\n                    (context.end_time - context.start_time).total_seconds() * 1000\n                    if context.end_time\n                    else 0\n                ),\n                rows_processed=result.metrics.processed_rows,\n                rows_succeeded=result.metrics.processed_rows\n                - result.metrics.failed_rows,\n                rows_failed=result.metrics.failed_rows,\n                rows_skipped=result.metrics.skipped_rows,\n                total_cost=result.costs.total_cost,\n                total_tokens=result.costs.total_tokens,\n                input_tokens=result.costs.input_tokens,\n                output_tokens=result.costs.output_tokens,\n            )\n            context.observer_dispatcher.dispatch(\"pipeline_end\", end_event)\n\n            # Flush and close observers\n            context.observer_dispatcher.flush_all()\n            context.observer_dispatcher.close_all()\n\n        return result\n\n    except Exception as e:\n        # Save checkpoint on error\n        state_manager.save_checkpoint(context)\n        self.logger.error(\n            f\"Pipeline failed. Checkpoint saved. \"\n            f\"Resume with: pipeline.execute(resume_from=UUID('{context.session_id}'))\"\n        )\n\n        # Notify legacy observers of error\n        for observer in self.observers:\n            observer.on_pipeline_error(context, e)\n\n        # Emit error event for new observability system\n        if context.observer_dispatcher:\n            from ondine.observability.events import ErrorEvent\n\n            error_event = ErrorEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                error=e,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                stack_trace=\"\",  # Could add full traceback if needed\n            )\n            context.observer_dispatcher.dispatch(\"error\", error_event)\n\n            # Flush and close observers even on error\n            context.observer_dispatcher.flush_all()\n            context.observer_dispatcher.close_all()\n\n        raise\n</code></pre>"},{"location":"api/#ondine.Pipeline.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline asynchronously.</p> <p>Uses AsyncExecutor for non-blocking execution. Ideal for integration with FastAPI, aiohttp, and other async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support async</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>async def execute_async(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline asynchronously.\n\n    Uses AsyncExecutor for non-blocking execution. Ideal for integration\n    with FastAPI, aiohttp, and other async frameworks.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        ValueError: If executor doesn't support async\n    \"\"\"\n    if not self.executor.supports_async():\n        raise ValueError(\n            \"Current executor doesn't support async. \"\n            \"Use AsyncExecutor: Pipeline(specs, executor=AsyncExecutor())\"\n        )\n\n    # For now, wrap synchronous execution in async\n    # TODO: Implement fully async execution pipeline\n    import asyncio\n\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, self.execute, resume_from)\n</code></pre>"},{"location":"api/#ondine.Pipeline.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(chunk_size: int | None = None) -&gt; Iterator[ExecutionResult]\n</code></pre> <p>Execute pipeline in streaming mode.</p> <p>Processes data in chunks for memory-efficient handling of large datasets. Ideal for datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int | None</code> <p>Number of rows per chunk (uses executor's chunk_size if None)</p> <code>None</code> <p>Yields:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult objects for each processed chunk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support streaming</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute_stream(\n    self, chunk_size: int | None = None\n) -&gt; Iterator[ExecutionResult]:\n    \"\"\"\n    Execute pipeline in streaming mode.\n\n    Processes data in chunks for memory-efficient handling of large datasets.\n    Ideal for datasets that don't fit in memory.\n\n    Args:\n        chunk_size: Number of rows per chunk (uses executor's chunk_size if None)\n\n    Yields:\n        ExecutionResult objects for each processed chunk\n\n    Raises:\n        ValueError: If executor doesn't support streaming\n    \"\"\"\n    if not self.executor.supports_streaming():\n        raise ValueError(\n            \"Current executor doesn't support streaming. \"\n            \"Use StreamingExecutor: Pipeline(specs, executor=StreamingExecutor())\"\n        )\n\n    # Use executor's chunk_size if not provided\n    if chunk_size is None and isinstance(self.executor, StreamingExecutor):\n        chunk_size = self.executor.chunk_size\n    elif chunk_size is None:\n        chunk_size = 1000  # Default fallback\n\n    # For now, execute the full pipeline and split result into chunks\n    # TODO: Implement proper streaming execution that processes chunks independently\n    result = self.execute()\n\n    # Split the result data into chunks and yield as separate ExecutionResults\n    total_rows = len(result.data)\n    for start_idx in range(0, total_rows, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_data = result.data.iloc[start_idx:end_idx].copy()\n\n        # Create a chunk result with proportional metrics\n        chunk_rows = len(chunk_data)\n        chunk_result = ExecutionResult(\n            data=chunk_data,\n            metrics=ProcessingStats(\n                total_rows=chunk_rows,\n                processed_rows=chunk_rows,\n                failed_rows=0,\n                skipped_rows=0,\n                rows_per_second=result.metrics.rows_per_second,\n                total_duration_seconds=result.metrics.total_duration_seconds\n                * (chunk_rows / total_rows),\n                stage_durations=result.metrics.stage_durations,\n            ),\n            costs=CostEstimate(\n                total_cost=result.costs.total_cost\n                * Decimal(chunk_rows / total_rows),\n                total_tokens=int(\n                    result.costs.total_tokens * (chunk_rows / total_rows)\n                ),\n                input_tokens=int(\n                    result.costs.input_tokens * (chunk_rows / total_rows)\n                ),\n                output_tokens=int(\n                    result.costs.output_tokens * (chunk_rows / total_rows)\n                ),\n                rows=chunk_rows,\n                confidence=result.costs.confidence,\n            ),\n            execution_id=result.execution_id,\n            start_time=result.start_time,\n            end_time=result.end_time,\n            success=True,\n        )\n        yield chunk_result\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder","title":"PipelineBuilder","text":"<pre><code>PipelineBuilder()\n</code></pre> <p>Fluent builder for constructing pipelines.</p> <p>Provides an intuitive, chainable API for common use cases.</p> Example <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p> <p>Initialize builder with None values.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with None values.\"\"\"\n    self._dataset_spec: DatasetSpec | None = None\n    self._prompt_spec: PromptSpec | None = None\n    self._llm_spec: LLMSpec | None = None\n    self._processing_spec: ProcessingSpec = ProcessingSpec()\n    self._output_spec: OutputSpec | None = None\n    self._dataframe: pd.DataFrame | None = None\n    self._executor: ExecutionStrategy | None = None\n    self._custom_parser: any | None = None\n    self._custom_llm_client: any | None = None\n    self._custom_stages: list[dict] = []  # For custom stage injection\n    self._observers: list[tuple[str, dict]] = []  # For observability\n    self._custom_metadata: dict[str, Any] = {}  # For arbitrary metadata\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create() -&gt; PipelineBuilder\n</code></pre> <p>Start builder chain.</p> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>New PipelineBuilder instance</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef create() -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Start builder chain.\n\n    Returns:\n        New PipelineBuilder instance\n    \"\"\"\n    return PipelineBuilder()\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_specifications","title":"from_specifications  <code>staticmethod</code>","text":"<pre><code>from_specifications(specs: PipelineSpecifications) -&gt; PipelineBuilder\n</code></pre> <p>Create builder from existing specifications.</p> <p>Useful for loading from YAML and modifying programmatically.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>PipelineSpecifications</code> <p>Complete pipeline specifications</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>PipelineBuilder pre-configured with specs</p> Example <p>specs = load_pipeline_config(\"config.yaml\") builder = PipelineBuilder.from_specifications(specs) pipeline = builder.build()</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef from_specifications(specs: PipelineSpecifications) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Create builder from existing specifications.\n\n    Useful for loading from YAML and modifying programmatically.\n\n    Args:\n        specs: Complete pipeline specifications\n\n    Returns:\n        PipelineBuilder pre-configured with specs\n\n    Example:\n        specs = load_pipeline_config(\"config.yaml\")\n        builder = PipelineBuilder.from_specifications(specs)\n        pipeline = builder.build()\n    \"\"\"\n    builder = PipelineBuilder()\n    builder._dataset_spec = specs.dataset\n    builder._prompt_spec = specs.prompt\n    builder._llm_spec = specs.llm\n    builder._processing_spec = specs.processing\n    builder._output_spec = specs.output\n    return builder\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_csv","title":"from_csv","text":"<pre><code>from_csv(path: str, input_columns: list[str], output_columns: list[str], delimiter: str = ',', encoding: str = 'utf-8') -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names to use in prompts</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names to generate</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding (default: utf-8)</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>builder = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"description\"],\n        output_columns=[\"category\"]\n    )\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_csv(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV data source.\n\n    Args:\n        path: Path to CSV file\n        input_columns: Input column names to use in prompts\n        output_columns: Output column names to generate\n        delimiter: CSV delimiter (default: comma)\n        encoding: File encoding (default: utf-8)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        builder = (\n            PipelineBuilder.create()\n            .from_csv(\n                \"products.csv\",\n                input_columns=[\"description\"],\n                output_columns=[\"category\"]\n            )\n        )\n        ```\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.CSV,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        delimiter=delimiter,\n        encoding=encoding,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_excel","title":"from_excel","text":"<pre><code>from_excel(path: str, input_columns: list[str], output_columns: list[str], sheet_name: str | int = 0) -&gt; PipelineBuilder\n</code></pre> <p>Configure Excel data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Excel file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_excel(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    sheet_name: str | int = 0,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Excel data source.\n\n    Args:\n        path: Path to Excel file\n        input_columns: Input column names\n        output_columns: Output column names\n        sheet_name: Sheet name or index\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.EXCEL,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        sheet_name=sheet_name,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_parquet","title":"from_parquet","text":"<pre><code>from_parquet(path: str, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure Parquet data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_parquet(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Parquet data source.\n\n    Args:\n        path: Path to Parquet file\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.PARQUET,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_dataframe","title":"from_dataframe","text":"<pre><code>from_dataframe(df: DataFrame, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure DataFrame source.</p> <p>Useful for processing in-memory data or chaining pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame with input data</p> required <code>input_columns</code> <code>list[str]</code> <p>Column names to use in prompts</p> required <code>output_columns</code> <code>list[str]</code> <p>Column names to generate</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\"text\": [\"Hello\", \"World\"]})\nbuilder = (\n    PipelineBuilder.create()\n    .from_dataframe(df, input_columns=[\"text\"], output_columns=[\"result\"])\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_dataframe(\n    self,\n    df: pd.DataFrame,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure DataFrame source.\n\n    Useful for processing in-memory data or chaining pipelines.\n\n    Args:\n        df: Pandas DataFrame with input data\n        input_columns: Column names to use in prompts\n        output_columns: Column names to generate\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        import pandas as pd\n\n        df = pd.DataFrame({\"text\": [\"Hello\", \"World\"]})\n        builder = (\n            PipelineBuilder.create()\n            .from_dataframe(df, input_columns=[\"text\"], output_columns=[\"result\"])\n        )\n        ```\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.DATAFRAME,\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    self._dataframe = df\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_prompt","title":"with_prompt","text":"<pre><code>with_prompt(template: str, system_message: str | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Configure prompt template.</p> <p>Use {variable} syntax to reference input columns. The LLM will process each row using this template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {variable} placeholders matching input_columns</p> required <code>system_message</code> <code>str | None</code> <p>Optional system message for chat models</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>builder.with_prompt(\n    \"Categorize this product: {title}\\n\\nCategory:\",\n    system_message=\"You are a product categorization expert.\"\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_prompt(\n    self,\n    template: str,\n    system_message: str | None = None,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure prompt template.\n\n    Use {variable} syntax to reference input columns. The LLM will process\n    each row using this template.\n\n    Args:\n        template: Prompt template with {variable} placeholders matching input_columns\n        system_message: Optional system message for chat models\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        builder.with_prompt(\n            \"Categorize this product: {title}\\\\n\\\\nCategory:\",\n            system_message=\"You are a product categorization expert.\"\n        )\n        ```\n    \"\"\"\n    self._prompt_spec = PromptSpec(\n        template=template,\n        system_message=system_message,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_system_prompt","title":"with_system_prompt","text":"<pre><code>with_system_prompt(system_prompt: str) -&gt; PipelineBuilder\n</code></pre> <p>Set system prompt for caching optimization.</p> <p>System prompts are cached by providers (OpenAI, Anthropic) and reused across all rows, reducing costs by 50-90% for the cached portion.</p> <p>This method should be called after with_prompt() to set or update the system message separately from the user prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>Static instructions/context (will be cached)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Review: {text}\")  # Dynamic per row\n    .with_system_prompt('''\n        You are a sentiment classifier.\n        Classify reviews as: positive, negative, or neutral.\n        Return only the label, nothing else.\n    ''')  # Cached across all rows\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <p>Can also be set via with_prompt(template, system_message=...). This method provides a more explicit API for caching optimization.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_system_prompt(self, system_prompt: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set system prompt for caching optimization.\n\n    System prompts are cached by providers (OpenAI, Anthropic) and reused\n    across all rows, reducing costs by 50-90% for the cached portion.\n\n    This method should be called after with_prompt() to set or update the\n    system message separately from the user prompt template.\n\n    Args:\n        system_prompt: Static instructions/context (will be cached)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"])\n            .with_prompt(\"Review: {text}\")  # Dynamic per row\n            .with_system_prompt('''\n                You are a sentiment classifier.\n                Classify reviews as: positive, negative, or neutral.\n                Return only the label, nothing else.\n            ''')  # Cached across all rows\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        Can also be set via with_prompt(template, system_message=...).\n        This method provides a more explicit API for caching optimization.\n    \"\"\"\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_system_prompt()\")\n\n    self._prompt_spec.system_message = system_prompt\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_batch_size","title":"with_batch_size","text":"<pre><code>with_batch_size(batch_size: int) -&gt; PipelineBuilder\n</code></pre> <p>Set batch size for multi-row processing.</p> <p>Process multiple rows in a single API call to reduce costs and latency by up to 100\u00d7. Batch size of 1 (default) processes rows individually.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of rows to process per API call (1-500)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt; 1 or prompt not set</p> Example <pre><code># Process 100 rows per API call (100\u00d7 fewer calls)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Classify: {text}\")\n    .with_batch_size(100)  # 5M rows = 50K API calls!\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <ul> <li>Batch size is limited by model context window</li> <li>Larger batches = fewer API calls but higher risk of partial failures</li> <li>Recommended: Start with 10-50, increase based on results</li> </ul> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_size(self, batch_size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set batch size for multi-row processing.\n\n    Process multiple rows in a single API call to reduce costs and latency\n    by up to 100\u00d7. Batch size of 1 (default) processes rows individually.\n\n    Args:\n        batch_size: Number of rows to process per API call (1-500)\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If batch_size &lt; 1 or prompt not set\n\n    Example:\n        ```python\n        # Process 100 rows per API call (100\u00d7 fewer calls)\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"])\n            .with_prompt(\"Classify: {text}\")\n            .with_batch_size(100)  # 5M rows = 50K API calls!\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        - Batch size is limited by model context window\n        - Larger batches = fewer API calls but higher risk of partial failures\n        - Recommended: Start with 10-50, increase based on results\n    \"\"\"\n    if batch_size &lt; 1:\n        raise ValueError(f\"batch_size must be &gt;= 1, got {batch_size}\")\n\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_batch_size()\")\n\n    # Update batch_size directly (consistent with with_system_prompt)\n    self._prompt_spec.batch_size = batch_size\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_batch_strategy","title":"with_batch_strategy","text":"<pre><code>with_batch_strategy(strategy: str) -&gt; PipelineBuilder\n</code></pre> <p>Set batch formatting strategy.</p> <p>Choose how multiple rows are formatted in a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Strategy name (\"json\" or \"csv\")</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy is not supported or prompt not set</p> Example <pre><code># Use JSON array format (default, most reliable)\npipeline.with_batch_size(100).with_batch_strategy(\"json\")\n\n# Use CSV format (more compact, experimental)\npipeline.with_batch_size(100).with_batch_strategy(\"csv\")\n</code></pre> Supported Strategies <ul> <li>\"json\": JSON array format (default, most reliable)</li> <li>\"csv\": CSV format (more compact, experimental)</li> </ul> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_strategy(self, strategy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set batch formatting strategy.\n\n    Choose how multiple rows are formatted in a single prompt.\n\n    Args:\n        strategy: Strategy name (\"json\" or \"csv\")\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If strategy is not supported or prompt not set\n\n    Example:\n        ```python\n        # Use JSON array format (default, most reliable)\n        pipeline.with_batch_size(100).with_batch_strategy(\"json\")\n\n        # Use CSV format (more compact, experimental)\n        pipeline.with_batch_size(100).with_batch_strategy(\"csv\")\n        ```\n\n    Supported Strategies:\n        - \"json\": JSON array format (default, most reliable)\n        - \"csv\": CSV format (more compact, experimental)\n    \"\"\"\n    allowed = [\"json\", \"csv\"]\n    if strategy not in allowed:\n        raise ValueError(\n            f\"batch_strategy must be one of {allowed}, got '{strategy}'\"\n        )\n\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_batch_strategy()\")\n\n    # Update batch_strategy directly (consistent with with_system_prompt)\n    self._prompt_spec.batch_strategy = strategy\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_jinja2","title":"with_jinja2","text":"<pre><code>with_jinja2(enabled: bool = True) -&gt; PipelineBuilder\n</code></pre> <p>Enable Jinja2 template rendering for advanced prompt control.</p> <p>Jinja2 provides powerful features for dynamic prompts: - Conditionals: {% if condition %}...{% endif %} - Loops: {% for item in items %}...{% endfor %} - Filters: {{ text | upper | truncate(100) }} - Complex logic within templates</p> <p>By default, Ondine uses Python's .format() for simple {variable} substitution. Enable Jinja2 when you need programmatic control flow.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Enable (True) or disable (False) Jinja2 rendering</p> <code>True</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Conditional prompt based on data\ntemplate = '''Extract from: {{ description }}\n{% if category == \"beverage\" %}\nFocus on volume units (oz, ml, L)\n{% elif category == \"food\" %}\nFocus on weight units (lb, oz, g)\n{% endif %}'''\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"products.csv\", input_columns=[\"description\", \"category\"])\n    .with_prompt(template)\n    .with_jinja2(True)  # Enable Jinja2\n    .with_llm(\"openai\", \"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <p>Simple {variable} syntax works with both modes. Only enable Jinja2 if you need conditionals, loops, or filters.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_jinja2(self, enabled: bool = True) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Enable Jinja2 template rendering for advanced prompt control.\n\n    Jinja2 provides powerful features for dynamic prompts:\n    - Conditionals: {% if condition %}...{% endif %}\n    - Loops: {% for item in items %}...{% endfor %}\n    - Filters: {{ text | upper | truncate(100) }}\n    - Complex logic within templates\n\n    By default, Ondine uses Python's .format() for simple {variable}\n    substitution. Enable Jinja2 when you need programmatic control flow.\n\n    Args:\n        enabled: Enable (True) or disable (False) Jinja2 rendering\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Conditional prompt based on data\n        template = '''Extract from: {{ description }}\n        {% if category == \"beverage\" %}\n        Focus on volume units (oz, ml, L)\n        {% elif category == \"food\" %}\n        Focus on weight units (lb, oz, g)\n        {% endif %}'''\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"products.csv\", input_columns=[\"description\", \"category\"])\n            .with_prompt(template)\n            .with_jinja2(True)  # Enable Jinja2\n            .with_llm(\"openai\", \"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        Simple {variable} syntax works with both modes. Only enable\n        Jinja2 if you need conditionals, loops, or filters.\n    \"\"\"\n    self._processing_spec.use_jinja2 = enabled\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_llm","title":"with_llm","text":"<pre><code>with_llm(provider: str, model: str, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, **kwargs: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM provider.</p> <p>Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom providers. API keys can be provided explicitly or via environment variables.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (openai, azure_openai, anthropic, groq, mlx) or custom provider ID</p> required <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\")</p> required <code>api_key</code> <code>str | None</code> <p>API key (optional, reads from environment if not provided)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-1.0, default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum output tokens (optional, uses model default)</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Provider-specific parameters (e.g., azure_endpoint, azure_deployment)</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># OpenAI\nbuilder.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n# Groq (fast and affordable)\nbuilder.with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n\n# Azure OpenAI with Managed Identity\nbuilder.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    use_managed_identity=True\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm(\n    self,\n    provider: str,\n    model: str,\n    api_key: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    **kwargs: any,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM provider.\n\n    Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom providers.\n    API keys can be provided explicitly or via environment variables.\n\n    Args:\n        provider: Provider name (openai, azure_openai, anthropic, groq, mlx) or custom provider ID\n        model: Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\")\n        api_key: API key (optional, reads from environment if not provided)\n        temperature: Sampling temperature (0.0-1.0, default: 0.0 for deterministic)\n        max_tokens: Maximum output tokens (optional, uses model default)\n        **kwargs: Provider-specific parameters (e.g., azure_endpoint, azure_deployment)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # OpenAI\n        builder.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n        # Groq (fast and affordable)\n        builder.with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n\n        # Azure OpenAI with Managed Identity\n        builder.with_llm(\n            provider=\"azure_openai\",\n            model=\"gpt-4\",\n            azure_endpoint=\"https://your-resource.openai.azure.com/\",\n            azure_deployment=\"gpt-4-deployment\",\n            use_managed_identity=True\n        )\n        ```\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Try to convert to enum for built-in providers\n    try:\n        provider_enum = LLMProvider(provider.lower())\n    except ValueError:\n        # Not a built-in provider - check if it's a custom provider\n        if ProviderRegistry.is_registered(provider):\n            # Use a dummy enum value for validation, but store the actual provider string\n            provider_enum = LLMProvider.OPENAI  # Dummy for Pydantic validation\n            kwargs[\"_custom_provider_id\"] = provider\n        else:\n            raise ValueError(\n                f\"Unknown provider: {provider}. \"\n                f\"Available providers: {', '.join(ProviderRegistry.list_providers())}\"\n            )\n\n    self._llm_spec = LLMSpec(\n        provider=provider_enum,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec","title":"with_llm_spec","text":"<pre><code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM using a pre-built LLMSpec object.</p> <p>This method allows using LLMSpec objects directly, enabling: - Reusable provider configurations - Use of LLMProviderPresets for common providers - Custom LLMSpec instances for advanced use cases</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification object</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If spec is not an LLMSpec instance</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm_spec(self, spec: LLMSpec) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM using a pre-built LLMSpec object.\n\n    This method allows using LLMSpec objects directly, enabling:\n    - Reusable provider configurations\n    - Use of LLMProviderPresets for common providers\n    - Custom LLMSpec instances for advanced use cases\n\n    Args:\n        spec: LLM specification object\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        TypeError: If spec is not an LLMSpec instance\n\n    Example:\n        # Use preset\n        from ondine.core.specifications import LLMProviderPresets\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Process: {text}\")\n            .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n            .build()\n        )\n\n        # Custom spec\n        custom = LLMSpec(\n            provider=LLMProvider.OPENAI,\n            model=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n        pipeline.with_llm_spec(custom)\n\n        # Override preset\n        spec = LLMProviderPresets.GPT4O_MINI.model_copy(\n            update={\"temperature\": 0.9}\n        )\n        pipeline.with_llm_spec(spec)\n    \"\"\"\n    if not isinstance(spec, LLMSpec):\n        raise TypeError(\n            f\"Expected LLMSpec, got {type(spec).__name__}. \"\n            f\"Use with_llm() for parameter-based configuration.\"\n        )\n\n    self._llm_spec = spec\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec--use-preset","title":"Use preset","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec--custom-spec","title":"Custom spec","text":"<p>custom = LLMSpec(     provider=LLMProvider.OPENAI,     model=\"gpt-4o-mini\",     temperature=0.7 ) pipeline.with_llm_spec(custom)</p>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec--override-preset","title":"Override preset","text":"<p>spec = LLMProviderPresets.GPT4O_MINI.model_copy(     update={\"temperature\": 0.9} ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/#ondine.PipelineBuilder.with_custom_llm_client","title":"with_custom_llm_client","text":"<pre><code>with_custom_llm_client(client: any) -&gt; PipelineBuilder\n</code></pre> <p>Provide a custom LLM client instance directly.</p> <p>This allows advanced users to create their own LLM client implementations by extending the LLMClient base class. The custom client will be used instead of the factory-created client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>any</code> <p>Custom LLM client instance (must inherit from LLMClient)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <p>class MyCustomClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         # Custom implementation         ...</p> <p>pipeline = (     PipelineBuilder.create()     .from_dataframe(df, ...)     .with_prompt(\"...\")     .with_custom_llm_client(MyCustomClient(spec))     .build() )</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_custom_llm_client(self, client: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Provide a custom LLM client instance directly.\n\n    This allows advanced users to create their own LLM client implementations\n    by extending the LLMClient base class. The custom client will be used\n    instead of the factory-created client.\n\n    Args:\n        client: Custom LLM client instance (must inherit from LLMClient)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        class MyCustomClient(LLMClient):\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                # Custom implementation\n                ...\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_dataframe(df, ...)\n            .with_prompt(\"...\")\n            .with_custom_llm_client(MyCustomClient(spec))\n            .build()\n        )\n    \"\"\"\n    from ondine.adapters.llm_client import LLMClient\n\n    if not isinstance(client, LLMClient):\n        raise TypeError(\n            f\"Custom client must inherit from LLMClient, got {type(client).__name__}\"\n        )\n\n    self._custom_llm_client = client\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_processing_batch_size","title":"with_processing_batch_size","text":"<pre><code>with_processing_batch_size(size: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure internal batch size for PromptFormatterStage.</p> <p>This is different from with_batch_size() which enables multi-row batching. This method controls how many prompts are grouped together internally for processing efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Rows per internal batch</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Note <p>This is an internal optimization parameter. Most users should use with_batch_size() for multi-row batching instead.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_processing_batch_size(self, size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure internal batch size for PromptFormatterStage.\n\n    This is different from with_batch_size() which enables multi-row batching.\n    This method controls how many prompts are grouped together internally\n    for processing efficiency.\n\n    Args:\n        size: Rows per internal batch\n\n    Returns:\n        Self for chaining\n\n    Note:\n        This is an internal optimization parameter. Most users should use\n        with_batch_size() for multi-row batching instead.\n    \"\"\"\n    self._processing_spec.batch_size = size\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_concurrency","title":"with_concurrency","text":"<pre><code>with_concurrency(threads: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure concurrent requests.</p> <p>Higher concurrency = faster processing but more API load. Adjust based on your provider's rate limits.</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int</code> <p>Number of concurrent threads (1-100, typical: 5-20)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Conservative (free tier)\nbuilder.with_concurrency(5)\n\n# Aggressive (paid tier)\nbuilder.with_concurrency(20)\n\n# Maximum (enterprise)\nbuilder.with_concurrency(50)\n</code></pre> Note <p>Groq supports high concurrency (~100), while OpenAI free tier is limited to ~5.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_concurrency(self, threads: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure concurrent requests.\n\n    Higher concurrency = faster processing but more API load. Adjust based on\n    your provider's rate limits.\n\n    Args:\n        threads: Number of concurrent threads (1-100, typical: 5-20)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Conservative (free tier)\n        builder.with_concurrency(5)\n\n        # Aggressive (paid tier)\n        builder.with_concurrency(20)\n\n        # Maximum (enterprise)\n        builder.with_concurrency(50)\n        ```\n\n    Note:\n        Groq supports high concurrency (~100), while OpenAI free tier is limited to ~5.\n    \"\"\"\n    self._processing_spec.concurrency = threads\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_checkpoint_interval","title":"with_checkpoint_interval","text":"<pre><code>with_checkpoint_interval(rows: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint frequency.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Rows between checkpoints</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_interval(self, rows: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint frequency.\n\n    Args:\n        rows: Rows between checkpoints\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_interval = rows\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_rate_limit","title":"with_rate_limit","text":"<pre><code>with_rate_limit(rpm: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure rate limiting.</p> <p>Prevents hitting API rate limits by throttling requests using token bucket algorithm. Set this below your provider's actual limit for safety.</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute (typical: 20-60 for free tiers, 100+ for paid)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># OpenAI free tier (60 RPM limit)\nbuilder.with_rate_limit(50)\n\n# Groq free tier (30 RPM limit)\nbuilder.with_rate_limit(25)\n\n# Paid tier with high limits\nbuilder.with_rate_limit(100)\n</code></pre> Note <p>Rate limiting is applied per pipeline execution, not globally.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_rate_limit(self, rpm: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure rate limiting.\n\n    Prevents hitting API rate limits by throttling requests using token bucket algorithm.\n    Set this below your provider's actual limit for safety.\n\n    Args:\n        rpm: Requests per minute (typical: 20-60 for free tiers, 100+ for paid)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # OpenAI free tier (60 RPM limit)\n        builder.with_rate_limit(50)\n\n        # Groq free tier (30 RPM limit)\n        builder.with_rate_limit(25)\n\n        # Paid tier with high limits\n        builder.with_rate_limit(100)\n        ```\n\n    Note:\n        Rate limiting is applied per pipeline execution, not globally.\n    \"\"\"\n    self._processing_spec.rate_limit_rpm = rpm\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_max_retries","title":"with_max_retries","text":"<pre><code>with_max_retries(retries: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>Maximum number of retry attempts</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_retries(self, retries: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum retry attempts.\n\n    Args:\n        retries: Maximum number of retry attempts\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_retries = retries\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_max_budget","title":"with_max_budget","text":"<pre><code>with_max_budget(budget: float) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum budget.</p> <p>Pipeline will halt execution if cost exceeds this limit. Warnings are shown at 75% and 90% of budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget</code> <code>float</code> <p>Maximum budget in USD (e.g., 5.0 for $5)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Set $5 budget limit\nbuilder.with_max_budget(5.0)\n\n# For testing with small budget\nbuilder.with_max_budget(0.50)\n\n# For production runs\nbuilder.with_max_budget(100.0)\n</code></pre> Note <p>Budget enforcement uses Decimal precision to avoid floating-point errors.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_budget(self, budget: float) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum budget.\n\n    Pipeline will halt execution if cost exceeds this limit. Warnings are shown\n    at 75% and 90% of budget.\n\n    Args:\n        budget: Maximum budget in USD (e.g., 5.0 for $5)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Set $5 budget limit\n        builder.with_max_budget(5.0)\n\n        # For testing with small budget\n        builder.with_max_budget(0.50)\n\n        # For production runs\n        builder.with_max_budget(100.0)\n        ```\n\n    Note:\n        Budget enforcement uses Decimal precision to avoid floating-point errors.\n    \"\"\"\n    self._processing_spec.max_budget = Decimal(str(budget))\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_error_policy","title":"with_error_policy","text":"<pre><code>with_error_policy(policy: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure error handling policy.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>Error policy ('skip', 'fail', 'retry', 'use_default')</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_error_policy(self, policy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure error handling policy.\n\n    Args:\n        policy: Error policy ('skip', 'fail', 'retry', 'use_default')\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.core.specifications import ErrorPolicy\n\n    self._processing_spec.error_policy = ErrorPolicy(policy.lower())\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_checkpoint_dir","title":"with_checkpoint_dir","text":"<pre><code>with_checkpoint_dir(directory: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to checkpoint directory</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_dir(self, directory: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint directory.\n\n    Args:\n        directory: Path to checkpoint directory\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_dir = Path(directory)\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_parser","title":"with_parser","text":"<pre><code>with_parser(parser: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure response parser.</p> <p>This method allows setting a custom parser. The parser type determines the response_format in the prompt spec.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>any</code> <p>Parser instance (JSONParser, RegexParser, PydanticParser, etc.)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_parser(self, parser: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure response parser.\n\n    This method allows setting a custom parser. The parser type\n    determines the response_format in the prompt spec.\n\n    Args:\n        parser: Parser instance (JSONParser, RegexParser, PydanticParser, etc.)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    # Store the parser for later use in the pipeline\n    # We'll configure response_format based on parser type\n    if hasattr(parser, \"__class__\"):\n        parser_name = parser.__class__.__name__\n        if \"JSON\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            # Update the existing prompt spec's response_format\n            self._prompt_spec.response_format = \"json\"\n        elif \"Regex\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            self._prompt_spec.response_format = \"regex\"\n            if hasattr(parser, \"patterns\"):\n                self._prompt_spec.regex_patterns = parser.patterns\n\n    # Store the parser instance in metadata for the pipeline to use\n    if not hasattr(self, \"_custom_parser\"):\n        self._custom_parser = parser\n\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.to_csv","title":"to_csv","text":"<pre><code>to_csv(path: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV output destination.</p> <p>Alias for with_output(path, format='csv').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output CSV file path</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def to_csv(self, path: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV output destination.\n\n    Alias for with_output(path, format='csv').\n\n    Args:\n        path: Output CSV file path\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    return self.with_output(path, format=\"csv\")\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_output","title":"with_output","text":"<pre><code>with_output(path: str, format: str = 'csv', merge_strategy: str = 'replace') -&gt; PipelineBuilder\n</code></pre> <p>Configure output destination.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path</p> required <code>format</code> <code>str</code> <p>Output format (csv, excel, parquet)</p> <code>'csv'</code> <code>merge_strategy</code> <code>str</code> <p>Merge strategy (replace, append, update)</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_output(\n    self,\n    path: str,\n    format: str = \"csv\",\n    merge_strategy: str = \"replace\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure output destination.\n\n    Args:\n        path: Output file path\n        format: Output format (csv, excel, parquet)\n        merge_strategy: Merge strategy (replace, append, update)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    format_map = {\n        \"csv\": DataSourceType.CSV,\n        \"excel\": DataSourceType.EXCEL,\n        \"parquet\": DataSourceType.PARQUET,\n    }\n\n    merge_map = {\n        \"replace\": MergeStrategy.REPLACE,\n        \"append\": MergeStrategy.APPEND,\n        \"update\": MergeStrategy.UPDATE,\n    }\n\n    self._output_spec = OutputSpec(\n        destination_type=format_map[format.lower()],\n        destination_path=Path(path),\n        merge_strategy=merge_map[merge_strategy.lower()],\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_executor","title":"with_executor","text":"<pre><code>with_executor(executor: ExecutionStrategy) -&gt; PipelineBuilder\n</code></pre> <p>Set custom execution strategy.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ExecutionStrategy</code> <p>ExecutionStrategy instance</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_executor(self, executor: ExecutionStrategy) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set custom execution strategy.\n\n    Args:\n        executor: ExecutionStrategy instance\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = executor\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_async_execution","title":"with_async_execution","text":"<pre><code>with_async_execution(max_concurrency: int = 10) -&gt; PipelineBuilder\n</code></pre> <p>Use async execution strategy.</p> <p>Enables async/await for non-blocking execution. Ideal for FastAPI, aiohttp, and async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_async_execution(self, max_concurrency: int = 10) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use async execution strategy.\n\n    Enables async/await for non-blocking execution.\n    Ideal for FastAPI, aiohttp, and async frameworks.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = AsyncExecutor(max_concurrency=max_concurrency)\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_streaming","title":"with_streaming","text":"<pre><code>with_streaming(chunk_size: int = 1000) -&gt; PipelineBuilder\n</code></pre> <p>Use streaming execution strategy.</p> <p>Processes data in chunks for memory-efficient handling. Ideal for large datasets (100K+ rows).</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_streaming(self, chunk_size: int = 1000) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use streaming execution strategy.\n\n    Processes data in chunks for memory-efficient handling.\n    Ideal for large datasets (100K+ rows).\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = StreamingExecutor(chunk_size=chunk_size)\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_progress_mode","title":"with_progress_mode","text":"<pre><code>with_progress_mode(mode: str = 'auto') -&gt; PipelineBuilder\n</code></pre> <p>Configure progress tracking mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Progress tracking mode - \"auto\": Auto-detect (rich if TTY, else logging) [default] - \"rich\": Beautiful progress bars with ETA - \"logging\": Simple log messages - \"none\": Disable progress tracking</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Use rich progress (beautiful UI)\nbuilder.with_progress_mode(\"rich\")\n\n# Disable progress (faster, cleaner logs)\nbuilder.with_progress_mode(\"none\")\n\n# Auto-detect (recommended)\nbuilder.with_progress_mode(\"auto\")\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_progress_mode(self, mode: str = \"auto\") -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure progress tracking mode.\n\n    Args:\n        mode: Progress tracking mode\n            - \"auto\": Auto-detect (rich if TTY, else logging) [default]\n            - \"rich\": Beautiful progress bars with ETA\n            - \"logging\": Simple log messages\n            - \"none\": Disable progress tracking\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Use rich progress (beautiful UI)\n        builder.with_progress_mode(\"rich\")\n\n        # Disable progress (faster, cleaner logs)\n        builder.with_progress_mode(\"none\")\n\n        # Auto-detect (recommended)\n        builder.with_progress_mode(\"auto\")\n        ```\n    \"\"\"\n    self._processing_spec.progress_mode = mode\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_stage","title":"with_stage","text":"<pre><code>with_stage(stage_name: str, position: str = 'before_prompt', **stage_kwargs) -&gt; PipelineBuilder\n</code></pre> <p>Add a custom pipeline stage by name.</p> <p>Enables injection of custom processing stages at specific points in the pipeline. Stages must be registered via StageRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Registered stage name (e.g., \"rag_retrieval\")</p> required <code>position</code> <code>str</code> <p>Where to inject the stage. Options: - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing - \"after_parser\": After response parsing</p> <code>'before_prompt'</code> <code>**stage_kwargs</code> <p>Arguments to pass to stage constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name not registered or position invalid</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_stage(\n    self,\n    stage_name: str,\n    position: str = \"before_prompt\",\n    **stage_kwargs,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add a custom pipeline stage by name.\n\n    Enables injection of custom processing stages at specific points\n    in the pipeline. Stages must be registered via StageRegistry.\n\n    Args:\n        stage_name: Registered stage name (e.g., \"rag_retrieval\")\n        position: Where to inject the stage. Options:\n            - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting\n            - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation\n            - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing\n            - \"after_parser\": After response parsing\n        **stage_kwargs: Arguments to pass to stage constructor\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If stage_name not registered or position invalid\n\n    Example:\n        # RAG retrieval example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])\n            .with_stage(\n                \"rag_retrieval\",\n                position=\"before_prompt\",\n                vector_store=\"pinecone\",\n                index_name=\"my-docs\",\n                top_k=5\n            )\n            .with_prompt(\"Context: {retrieved_context}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o\")\n            .build()\n        )\n\n        # Content moderation example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])\n            .with_stage(\n                \"content_moderation\",\n                position=\"before_llm\",\n                block_patterns=[\"spam\", \"offensive\"]\n            )\n            .with_prompt(\"Moderate: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n    \"\"\"\n    from ondine.stages.stage_registry import StageRegistry\n\n    # Validate position\n    valid_positions = [\n        \"after_loader\",\n        \"before_prompt\",\n        \"after_prompt\",\n        \"before_llm\",\n        \"after_llm\",\n        \"before_parser\",\n        \"after_parser\",\n    ]\n    if position not in valid_positions:\n        raise ValueError(\n            f\"Invalid position '{position}'. Must be one of: {', '.join(valid_positions)}\"\n        )\n\n    # Get stage class from registry (this will raise ValueError if not found)\n    stage_class = StageRegistry.get(stage_name)\n\n    # Store stage config for later instantiation\n    self._custom_stages.append(\n        {\n            \"name\": stage_name,\n            \"class\": stage_class,\n            \"position\": position,\n            \"kwargs\": stage_kwargs,\n        }\n    )\n\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_stage--rag-retrieval-example","title":"RAG retrieval example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])     .with_stage(         \"rag_retrieval\",         position=\"before_prompt\",         vector_store=\"pinecone\",         index_name=\"my-docs\",         top_k=5     )     .with_prompt(\"Context: {retrieved_context}\\n\\nQuestion: {question}\\n\\nAnswer:\")     .with_llm(provider=\"openai\", model=\"gpt-4o\")     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_stage--content-moderation-example","title":"Content moderation example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])     .with_stage(         \"content_moderation\",         position=\"before_llm\",         block_patterns=[\"spam\", \"offensive\"]     )     .with_prompt(\"Moderate: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_observer","title":"with_observer","text":"<pre><code>with_observer(name: str, config: dict[str, any] | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Add observability observer to the pipeline.</p> <p>Observers receive events during pipeline execution for monitoring, logging, and tracing.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier (e.g., \"langfuse\", \"opentelemetry\", \"logging\")</p> required <code>config</code> <code>dict[str, any] | None</code> <p>Observer-specific configuration dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If observer not registered</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_observer(\n    self, name: str, config: dict[str, any] | None = None\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add observability observer to the pipeline.\n\n    Observers receive events during pipeline execution for monitoring,\n    logging, and tracing.\n\n    Args:\n        name: Observer identifier (e.g., \"langfuse\", \"opentelemetry\", \"logging\")\n        config: Observer-specific configuration dictionary\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If observer not registered\n\n    Example:\n        # OpenTelemetry for infrastructure monitoring\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"opentelemetry\", config={\n                \"tracer_name\": \"my_pipeline\",\n                \"include_prompts\": False\n            })\n            .build()\n        )\n\n        # Langfuse for LLM-specific observability\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"langfuse\", config={\n                \"public_key\": \"pk-lf-...\",\n                \"secret_key\": \"sk-lf-...\"  # pragma: allowlist secret\n            })\n            .build()\n        )\n\n        # Multiple observers\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"langfuse\", config={...})\n            .with_observer(\"opentelemetry\", config={...})\n            .with_observer(\"logging\", config={\"log_level\": \"DEBUG\"})\n            .build()\n        )\n    \"\"\"\n    from ondine.observability.registry import ObserverRegistry\n\n    # Validate observer is registered\n    if not ObserverRegistry.is_registered(name):\n        available = \", \".join(ObserverRegistry.list_observers())\n        raise ValueError(\n            f\"Observer '{name}' not registered. \"\n            f\"Available observers: {available or 'none'}\"\n        )\n\n    # Store observer config for later instantiation\n    self._observers.append((name, config or {}))\n\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_observer--opentelemetry-for-infrastructure-monitoring","title":"OpenTelemetry for infrastructure monitoring","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"opentelemetry\", config={         \"tracer_name\": \"my_pipeline\",         \"include_prompts\": False     })     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_observer--langfuse-for-llm-specific-observability","title":"Langfuse for LLM-specific observability","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"langfuse\", config={         \"public_key\": \"pk-lf-...\",         \"secret_key\": \"sk-lf-...\"  # pragma: allowlist secret     })     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_observer--multiple-observers","title":"Multiple observers","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"langfuse\", config={...})     .with_observer(\"opentelemetry\", config={...})     .with_observer(\"logging\", config={\"log_level\": \"DEBUG\"})     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_structured_output","title":"with_structured_output","text":"<pre><code>with_structured_output(schema: Any) -&gt; PipelineBuilder\n</code></pre> <p>Configure structured output using a Pydantic model.</p> <p>This enables native schema enforcement, parsing, and auto-retry logic using LlamaIndex's structured_predict capabilities.</p> <p>Automatically configures JSONParser to handle the structured JSON output, unless a custom parser was already configured.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Any</code> <p>Pydantic model class defining the expected output structure</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_structured_output(self, schema: Any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure structured output using a Pydantic model.\n\n    This enables native schema enforcement, parsing, and auto-retry logic\n    using LlamaIndex's structured_predict capabilities.\n\n    Automatically configures JSONParser to handle the structured JSON output,\n    unless a custom parser was already configured.\n\n    Args:\n        schema: Pydantic model class defining the expected output structure\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    if not hasattr(self, \"_custom_metadata\"):\n        self._custom_metadata = {}\n    self._custom_metadata[\"structured_output_model\"] = schema\n\n    # Auto-inject JSONParser if no parser configured\n    # Structured output always returns JSON, so we need a JSON parser\n    if self._custom_parser is None:\n        from ondine.stages.response_parser_stage import JSONParser\n\n        self._custom_parser = JSONParser()\n\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.build","title":"build","text":"<pre><code>build() -&gt; Pipeline\n</code></pre> <p>Build final Pipeline.</p> <p>Validates all configurations and constructs the Pipeline object ready for execution. This is the final step in the builder chain.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required specifications missing (dataset, prompt, or LLM)</p> Example <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_concurrency(10)\n    .with_max_budget(5.0)\n    .build()  # Returns Pipeline object\n)\n\n# Execute the pipeline\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.total_rows} rows\")\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def build(self) -&gt; Pipeline:\n    \"\"\"\n    Build final Pipeline.\n\n    Validates all configurations and constructs the Pipeline object ready for execution.\n    This is the final step in the builder chain.\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If required specifications missing (dataset, prompt, or LLM)\n\n    Example:\n        ```python\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Summarize: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_concurrency(10)\n            .with_max_budget(5.0)\n            .build()  # Returns Pipeline object\n        )\n\n        # Execute the pipeline\n        result = pipeline.execute()\n        print(f\"Processed {result.metrics.total_rows} rows\")\n        ```\n    \"\"\"\n    # Validate required specs\n    if not self._dataset_spec:\n        raise ValueError(\"Dataset specification required\")\n    if not self._prompt_spec:\n        raise ValueError(\"Prompt specification required\")\n\n    # LLM spec is optional if custom client is provided\n    if not self._llm_spec and not self._custom_llm_client:\n        raise ValueError(\"Either LLM specification or custom LLM client required\")\n\n    # Prepare metadata with custom parser, custom client, custom stages, and observers\n    metadata = {}\n    if self._custom_metadata:\n        metadata.update(self._custom_metadata)\n    if self._custom_parser is not None:\n        metadata[\"custom_parser\"] = self._custom_parser\n    if self._custom_llm_client is not None:\n        metadata[\"custom_llm_client\"] = self._custom_llm_client\n    if self._custom_stages:\n        metadata[\"custom_stages\"] = self._custom_stages\n    if self._observers:\n        metadata[\"observers\"] = self._observers\n\n    # Create specifications bundle\n    # If custom client provided but no llm_spec, create a dummy spec\n    llm_spec = self._llm_spec\n    if llm_spec is None and self._custom_llm_client is not None:\n        # Create minimal spec using custom client's attributes\n        llm_spec = LLMSpec(\n            provider=LLMProvider.OPENAI,  # Dummy provider\n            model=self._custom_llm_client.model,\n            temperature=self._custom_llm_client.temperature,\n            max_tokens=self._custom_llm_client.max_tokens,\n        )\n\n    specifications = PipelineSpecifications(\n        dataset=self._dataset_spec,\n        prompt=self._prompt_spec,\n        llm=llm_spec,\n        processing=self._processing_spec,\n        output=self._output_spec,\n        metadata=metadata,\n    )\n\n    # Create and return pipeline\n    return Pipeline(\n        specifications,\n        dataframe=self._dataframe,\n        executor=self._executor,\n    )\n</code></pre>"},{"location":"api/#ondine.QuickPipeline","title":"QuickPipeline","text":"<p>Simplified pipeline API with smart defaults.</p> <p>Designed for rapid prototyping and common use cases. Automatically detects: - Input columns from prompt template placeholders - Provider from model name (e.g., gpt-4 \u2192 openai, claude \u2192 anthropic) - Parser type (JSON for multi-column, text for single column) - Reasonable defaults for batch size, concurrency, retries</p> <p>Examples:</p> <p>Minimal usage:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"data.csv\",\n...     prompt=\"Categorize this text: {text}\"\n... )\n&gt;&gt;&gt; result = pipeline.execute()\n</code></pre> <p>With explicit outputs:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"products.csv\",\n...     prompt=\"Extract: {description}\",\n...     output_columns=[\"brand\", \"model\", \"price\"]\n... )\n</code></pre> <p>Override defaults:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=df,\n...     prompt=\"Summarize: {content}\",\n...     model=\"gpt-4o\",\n...     temperature=0.7,\n...     max_budget=Decimal(\"5.0\")\n... )\n</code></pre>"},{"location":"api/#ondine.QuickPipeline.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(data: str | Path | DataFrame, prompt: str, model: str = 'gpt-4o-mini', output_columns: list[str] | str | None = None, provider: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, max_budget: Decimal | float | str | None = None, batch_size: int | None = None, concurrency: int | None = None, **kwargs: Any) -&gt; Pipeline\n</code></pre> <p>Create a pipeline with smart defaults.</p> <p>The simplest way to process data with LLMs. Automatically detects input columns from prompt placeholders and configures optimal settings based on data size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path | DataFrame</code> <p>CSV/Excel/Parquet file path or DataFrame</p> required <code>prompt</code> <code>str</code> <p>Prompt template with {placeholders} matching column names</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o-mini). Provider auto-detected from model name.</p> <code>'gpt-4o-mini'</code> <code>output_columns</code> <code>list[str] | str | None</code> <p>Output column name(s). If None, uses [\"output\"]</p> <code>None</code> <code>provider</code> <code>str | None</code> <p>LLM provider. If None, auto-detected (gpt-4 \u2192 openai, claude \u2192 anthropic)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-1.0, default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens (optional, uses provider default)</p> <code>None</code> <code>max_budget</code> <code>Decimal | float | str | None</code> <p>Maximum cost budget in USD (optional, no limit if not set)</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Rows per batch (optional, auto-sized: 10-500 based on data size)</p> <code>None</code> <code>concurrency</code> <code>int | None</code> <p>Parallel requests (optional, auto-sized: 5-100 based on provider)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to PipelineBuilder</p> <code>{}</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data cannot be loaded or prompt is invalid</p> Example <pre><code>from ondine import QuickPipeline\n\n# Minimal - auto-detects everything\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Categorize: {description}\"\n)\nresult = pipeline.execute()\n\n# With budget control\npipeline = QuickPipeline.create(\n    data=\"reviews.csv\",\n    prompt=\"Sentiment: {review_text}\",\n    model=\"gpt-4o-mini\",\n    max_budget=5.0\n)\n\n# Multi-column output\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Extract from {title}: brand, price, category as JSON\",\n    output_columns=[\"brand\", \"price\", \"category\"]\n)\n\n# Custom provider\npipeline = QuickPipeline.create(\n    data=df,\n    prompt=\"Summarize: {text}\",\n    model=\"llama-3.3-70b-versatile\",\n    provider=\"groq\"\n)\n</code></pre> Note <p>Input columns are automatically detected from {placeholders} in the prompt. Provider is auto-detected from model name (gpt-4 \u2192 openai, claude \u2192 anthropic, llama \u2192 groq).</p> Source code in <code>ondine/api/quick.py</code> <pre><code>@staticmethod\ndef create(\n    data: str | Path | pd.DataFrame,\n    prompt: str,\n    model: str = \"gpt-4o-mini\",\n    output_columns: list[str] | str | None = None,\n    provider: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    max_budget: Decimal | float | str | None = None,\n    batch_size: int | None = None,\n    concurrency: int | None = None,\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"\n    Create a pipeline with smart defaults.\n\n    The simplest way to process data with LLMs. Automatically detects input columns\n    from prompt placeholders and configures optimal settings based on data size.\n\n    Args:\n        data: CSV/Excel/Parquet file path or DataFrame\n        prompt: Prompt template with {placeholders} matching column names\n        model: Model name (default: gpt-4o-mini). Provider auto-detected from model name.\n        output_columns: Output column name(s). If None, uses [\"output\"]\n        provider: LLM provider. If None, auto-detected (gpt-4 \u2192 openai, claude \u2192 anthropic)\n        temperature: Sampling temperature (0.0-1.0, default: 0.0 for deterministic)\n        max_tokens: Max output tokens (optional, uses provider default)\n        max_budget: Maximum cost budget in USD (optional, no limit if not set)\n        batch_size: Rows per batch (optional, auto-sized: 10-500 based on data size)\n        concurrency: Parallel requests (optional, auto-sized: 5-100 based on provider)\n        **kwargs: Additional arguments passed to PipelineBuilder\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If input data cannot be loaded or prompt is invalid\n\n    Example:\n        ```python\n        from ondine import QuickPipeline\n\n        # Minimal - auto-detects everything\n        pipeline = QuickPipeline.create(\n            data=\"products.csv\",\n            prompt=\"Categorize: {description}\"\n        )\n        result = pipeline.execute()\n\n        # With budget control\n        pipeline = QuickPipeline.create(\n            data=\"reviews.csv\",\n            prompt=\"Sentiment: {review_text}\",\n            model=\"gpt-4o-mini\",\n            max_budget=5.0\n        )\n\n        # Multi-column output\n        pipeline = QuickPipeline.create(\n            data=\"products.csv\",\n            prompt=\"Extract from {title}: brand, price, category as JSON\",\n            output_columns=[\"brand\", \"price\", \"category\"]\n        )\n\n        # Custom provider\n        pipeline = QuickPipeline.create(\n            data=df,\n            prompt=\"Summarize: {text}\",\n            model=\"llama-3.3-70b-versatile\",\n            provider=\"groq\"\n        )\n        ```\n\n    Note:\n        Input columns are automatically detected from {placeholders} in the prompt.\n        Provider is auto-detected from model name (gpt-4 \u2192 openai, claude \u2192 anthropic, llama \u2192 groq).\n    \"\"\"\n    # 1. Load data\n    df = QuickPipeline._load_data(data)\n\n    # 2. Auto-detect input columns from prompt template\n    input_columns = QuickPipeline._extract_placeholders(prompt)\n    if not input_columns:\n        raise ValueError(\n            f\"No placeholders found in prompt: {prompt}\\n\"\n            \"Expected format: 'Your prompt with {{column_name}} placeholders'\"\n        )\n\n    # Validate input columns exist in data\n    missing = [col for col in input_columns if col not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Input columns {missing} not found in data. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    # 3. Normalize output columns\n    if output_columns is None:\n        output_columns = [\"output\"]\n    elif isinstance(output_columns, str):\n        output_columns = [output_columns]\n\n    # 4. Auto-detect provider from model name\n    if provider is None:\n        provider = QuickPipeline._detect_provider(model)\n\n    # 5. Auto-select parser (JSON for multi-column, text for single)\n    parser = QuickPipeline._select_parser(output_columns)\n\n    # 6. Smart defaults for batch_size and concurrency\n    if batch_size is None:\n        batch_size = QuickPipeline._default_batch_size(len(df))\n    if concurrency is None:\n        concurrency = QuickPipeline._default_concurrency(provider)\n\n    # 7. Build pipeline\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df, input_columns=input_columns, output_columns=output_columns\n        )\n        .with_prompt(template=prompt)\n        .with_llm(\n            provider=provider,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n    )\n\n    # Add optional parser if multi-column\n    if parser:\n        builder = builder.with_parser(parser)\n\n    # Add batch/concurrency settings\n    # Note: QuickPipeline uses processing_batch_size (internal batching)\n    # not with_batch_size (multi-row batching)\n    builder = builder.with_processing_batch_size(batch_size).with_concurrency(\n        concurrency\n    )\n\n    # Add budget if specified\n    if max_budget is not None:\n        # Convert to float for PipelineBuilder (it expects float)\n        if isinstance(max_budget, Decimal | str):\n            max_budget = float(max_budget)\n        builder = builder.with_max_budget(budget=max_budget)\n\n    # Add sensible retry defaults\n    builder = builder.with_max_retries(3)\n\n    return builder.build()\n</code></pre>"},{"location":"api/#ondine.CostEstimate","title":"CostEstimate  <code>dataclass</code>","text":"<pre><code>CostEstimate(total_cost: Decimal, total_tokens: int, input_tokens: int, output_tokens: int, rows: int, breakdown_by_stage: dict[str, Decimal] = dict(), confidence: str = 'estimate')\n</code></pre> <p>Cost estimation for pipeline execution.</p> <p>Provides detailed cost breakdown with Decimal precision to avoid floating-point errors.</p> <p>Attributes:</p> Name Type Description <code>total_cost</code> <code>Decimal</code> <p>Total cost in USD (Decimal for precision)</p> <code>total_tokens</code> <code>int</code> <p>Total tokens consumed (input + output)</p> <code>input_tokens</code> <code>int</code> <p>Input tokens sent to LLM</p> <code>output_tokens</code> <code>int</code> <p>Output tokens generated by LLM</p> <code>rows</code> <code>int</code> <p>Number of rows processed</p> <code>breakdown_by_stage</code> <code>dict[str, Decimal]</code> <p>Cost breakdown by pipeline stage</p> <code>confidence</code> <code>str</code> <p>Confidence level (estimate, sample-based, actual)</p> Example <pre><code>result = pipeline.execute()\n\n# Access costs\nprint(f\"Total: ${result.costs.total_cost}\")\nprint(f\"Input tokens: {result.costs.input_tokens:,}\")\nprint(f\"Output tokens: {result.costs.output_tokens:,}\")\nprint(f\"Cost per row: ${result.costs.total_cost / result.costs.rows:.4f}\")\n</code></pre>"},{"location":"api/#ondine.ExecutionResult","title":"ExecutionResult  <code>dataclass</code>","text":"<pre><code>ExecutionResult(data: DataFrame, metrics: ProcessingStats, costs: CostEstimate, errors: list[ErrorInfo] = list(), execution_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, success: bool = True, metadata: dict[str, Any] = dict())\n</code></pre> <p>Complete result from pipeline execution.</p> <p>Contains all output data, metrics, costs, and execution metadata from a pipeline run.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>DataFrame with input data and generated output columns</p> <code>metrics</code> <code>ProcessingStats</code> <p>Processing statistics (total_rows, success_count, failed_rows, etc.)</p> <code>costs</code> <code>CostEstimate</code> <p>Cost breakdown (total_cost, input_tokens, output_tokens)</p> <code>errors</code> <code>list[ErrorInfo]</code> <p>List of errors encountered during execution</p> <code>execution_id</code> <code>UUID</code> <p>Unique ID for this execution session</p> <code>start_time</code> <code>datetime</code> <p>When execution started</p> <code>end_time</code> <code>datetime | None</code> <p>When execution completed (None if still running)</p> <code>success</code> <code>bool</code> <p>Whether execution completed successfully</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional execution metadata</p> Example <pre><code>result = pipeline.execute()\n\n# Access output data\nprint(result.data.head())\nresult.data.to_csv(\"output.csv\")\n\n# Check metrics\nprint(f\"Total: {result.metrics.total_rows}\")\nprint(f\"Success: {result.metrics.success_count}\")\nprint(f\"Failed: {result.metrics.failed_rows}\")\n\n# Check costs\nprint(f\"Cost: ${result.costs.total_cost}\")\nprint(f\"Tokens: {result.costs.total_tokens}\")\n\n# Check execution time\nprint(f\"Duration: {result.duration:.2f}s\")\n</code></pre>"},{"location":"api/#ondine.ExecutionResult.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Get execution duration in seconds.</p>"},{"location":"api/#ondine.ExecutionResult.error_rate","title":"error_rate  <code>property</code>","text":"<pre><code>error_rate: float\n</code></pre> <p>Get error rate as percentage.</p>"},{"location":"api/#ondine.ExecutionResult.validate_output_quality","title":"validate_output_quality","text":"<pre><code>validate_output_quality(output_columns: list[str]) -&gt; QualityReport\n</code></pre> <p>Validate the quality of output data by checking for null/empty values.</p> <p>Parameters:</p> Name Type Description Default <code>output_columns</code> <code>list[str]</code> <p>List of output column names to check</p> required <p>Returns:</p> Type Description <code>QualityReport</code> <p>QualityReport with quality metrics and warnings</p> Source code in <code>ondine/core/models.py</code> <pre><code>def validate_output_quality(self, output_columns: list[str]) -&gt; \"QualityReport\":\n    \"\"\"\n    Validate the quality of output data by checking for null/empty values.\n\n    Args:\n        output_columns: List of output column names to check\n\n    Returns:\n        QualityReport with quality metrics and warnings\n    \"\"\"\n    total_rows = len(self.data)\n    total_columns = len(output_columns)\n    total_cells = total_rows * total_columns\n\n    # Count null and empty values across ALL output columns\n    null_count = 0\n    empty_count = 0\n\n    for col in output_columns:\n        if col in self.data.columns:\n            # Count nulls (None, NaN, NaT)\n            null_count += self.data[col].isna().sum()\n            # Count empty strings (only for string columns)\n            if self.data[col].dtype == \"object\":\n                empty_count += (self.data[col].astype(str).str.strip() == \"\").sum()\n\n    # Count rows with at least one valid output column\n    # A row is \"valid\" if at least one output column has non-null, non-empty data\n    valid_row_mask = False\n    for col in output_columns:\n        if col in self.data.columns:\n            non_null = ~self.data[col].isna()\n            if self.data[col].dtype == \"object\":\n                non_empty = self.data[col].astype(str).str.strip() != \"\"\n                valid_row_mask |= non_null &amp; non_empty\n            else:\n                valid_row_mask |= non_null\n\n    valid_outputs = (\n        valid_row_mask.sum() if isinstance(valid_row_mask, pd.Series) else 0\n    )\n    success_rate = (valid_outputs / total_rows * 100) if total_rows &gt; 0 else 0.0\n\n    # Determine quality score\n    if success_rate &gt;= 95.0:\n        quality_score = \"excellent\"\n    elif success_rate &gt;= 80.0:\n        quality_score = \"good\"\n    elif success_rate &gt;= 50.0:\n        quality_score = \"poor\"\n    else:\n        quality_score = \"critical\"\n\n    # Generate warnings and issues\n    warnings = []\n    issues = []\n\n    if success_rate &lt; 70.0:\n        issues.append(\n            f\"\u26a0\ufe0f  LOW SUCCESS RATE: Only {success_rate:.1f}% of rows have valid data \"\n            f\"({valid_outputs}/{total_rows} rows with at least one valid column)\"\n        )\n\n    if null_count &gt; total_cells * 0.3:  # &gt; 30% of all cells are null\n        issues.append(\n            f\"\u26a0\ufe0f  HIGH NULL RATE: {null_count} null cells out of {total_cells} total \"\n            f\"({null_count / total_cells * 100:.1f}% of all output cells)\"\n        )\n\n    if empty_count &gt; total_cells * 0.1:  # &gt; 10% of all cells are empty\n        warnings.append(\n            f\"Empty outputs detected: {empty_count} empty cells out of {total_cells} total \"\n            f\"({empty_count / total_cells * 100:.1f}% of all output cells)\"\n        )\n\n    # Check if reported metrics match actual data quality\n    if self.metrics.failed_rows == 0 and null_count &gt; 0:\n        issues.append(\n            f\"\u26a0\ufe0f  METRICS MISMATCH: Pipeline reported 0 failures but \"\n            f\"{null_count} cells have null outputs. This may indicate silent errors.\"\n        )\n\n    return QualityReport(\n        total_rows=total_rows,\n        valid_outputs=valid_outputs,\n        null_outputs=null_count,\n        empty_outputs=empty_count,\n        success_rate=success_rate,\n        quality_score=quality_score,\n        warnings=warnings,\n        issues=issues,\n    )\n</code></pre>"},{"location":"api/#ondine.ProcessingStats","title":"ProcessingStats  <code>dataclass</code>","text":"<pre><code>ProcessingStats(total_rows: int, processed_rows: int, failed_rows: int, skipped_rows: int, rows_per_second: float, total_duration_seconds: float, stage_durations: dict[str, float] = dict())\n</code></pre> <p>Statistics from pipeline execution.</p> <p>Tracks processing metrics for monitoring and debugging.</p> <p>Attributes:</p> Name Type Description <code>total_rows</code> <code>int</code> <p>Total number of rows in dataset</p> <code>processed_rows</code> <code>int</code> <p>Rows successfully processed</p> <code>failed_rows</code> <code>int</code> <p>Rows that failed processing</p> <code>skipped_rows</code> <code>int</code> <p>Rows skipped due to errors</p> Example <pre><code>result = pipeline.execute()\n\n# Check success rate\nsuccess_rate = result.metrics.success_count / result.metrics.total_rows * 100\nprint(f\"Success rate: {success_rate:.1f}%\")\n\n# Check for failures\nif result.metrics.failed_rows &gt; 0:\n    print(f\"Warning: {result.metrics.failed_rows} rows failed\")\n</code></pre>"},{"location":"api/#ondine.QualityReport","title":"QualityReport  <code>dataclass</code>","text":"<pre><code>QualityReport(total_rows: int, valid_outputs: int, null_outputs: int, empty_outputs: int, success_rate: float, quality_score: str, warnings: list[str] = list(), issues: list[str] = list())\n</code></pre> <p>Quality assessment of pipeline output.</p>"},{"location":"api/#ondine.QualityReport.is_acceptable","title":"is_acceptable  <code>property</code>","text":"<pre><code>is_acceptable: bool\n</code></pre> <p>Check if quality is acceptable (&gt;= 70% success).</p>"},{"location":"api/#ondine.QualityReport.has_issues","title":"has_issues  <code>property</code>","text":"<pre><code>has_issues: bool\n</code></pre> <p>Check if there are any issues.</p>"},{"location":"api/#ondine.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for data source configuration.</p> <p>Defines how to load input data and which columns to process. Supports CSV, Excel, Parquet, and in-memory DataFrames.</p> <p>Attributes:</p> Name Type Description <code>source_type</code> <code>DataSourceType</code> <p>Type of data source (csv, excel, parquet, dataframe)</p> <code>source_path</code> <code>str | Path | None</code> <p>Path to file (None for DataFrame sources)</p> <code>input_columns</code> <code>list[str]</code> <p>Column names to use in prompts (must exist in data)</p> <code>output_columns</code> <code>list[str]</code> <p>Column names for LLM results (must not overlap with input)</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional filters to apply when loading data</p> <code>sheet_name</code> <code>str | int | None</code> <p>Sheet name or index for Excel files (default: 0)</p> <code>delimiter</code> <code>str</code> <p>CSV delimiter character (default: \",\")</p> <code>encoding</code> <code>str</code> <p>File encoding (default: \"utf-8\")</p> Example <pre><code>spec = DatasetSpec(\n    source_type=DataSourceType.CSV,\n    source_path=\"products.csv\",\n    input_columns=[\"title\", \"description\"],\n    output_columns=[\"category\", \"price_range\"]\n)\n</code></pre>"},{"location":"api/#ondine.DatasetSpec.validate_source_path","title":"validate_source_path  <code>classmethod</code>","text":"<pre><code>validate_source_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"source_path\")\n@classmethod\ndef validate_source_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/#ondine.DatasetSpec.validate_no_overlap","title":"validate_no_overlap  <code>classmethod</code>","text":"<pre><code>validate_no_overlap(v: list[str], info: Any) -&gt; list[str]\n</code></pre> <p>Ensure output columns don't overlap with input columns.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"output_columns\")\n@classmethod\ndef validate_no_overlap(cls, v: list[str], info: Any) -&gt; list[str]:\n    \"\"\"Ensure output columns don't overlap with input columns.\"\"\"\n    if \"input_columns\" in info.data:\n        input_cols = set(info.data[\"input_columns\"])\n        output_cols = set(v)\n        overlap = input_cols &amp; output_cols\n        if overlap:\n            raise ValueError(f\"Output columns overlap with input: {overlap}\")\n    return v\n</code></pre>"},{"location":"api/#ondine.LLMSpec","title":"LLMSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for LLM provider configuration.</p> <p>Defines which LLM provider to use, model settings, and authentication. Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom OpenAI-compatible APIs.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>LLMProvider</code> <p>LLM provider (openai, azure_openai, anthropic, groq, mlx, openai_compatible)</p> <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\", \"llama-3.3-70b-versatile\")</p> <code>api_key</code> <code>str | None</code> <p>API key (optional, reads from environment if not provided)</p> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-2.0, default: 0.0 for deterministic output)</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum output tokens (optional, uses model default)</p> <code>top_p</code> <code>float</code> <p>Nucleus sampling parameter (0.0-1.0, default: 1.0)</p> Example <pre><code># OpenAI\nspec = LLMSpec(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    temperature=0.3\n)\n\n# Groq (fast and affordable)\nspec = LLMSpec(\n    provider=LLMProvider.GROQ,\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.0\n)\n\n# Azure with Managed Identity (no API key needed)\nspec = LLMSpec(\n    provider=LLMProvider.AZURE_OPENAI,\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    use_managed_identity=True\n)\n</code></pre> Note <p>Use LLMProviderPresets for pre-configured common providers.</p>"},{"location":"api/#ondine.LLMSpec.validate_base_url_format","title":"validate_base_url_format  <code>classmethod</code>","text":"<pre><code>validate_base_url_format(v: str | None) -&gt; str | None\n</code></pre> <p>Validate base_url is a valid HTTP(S) URL with a host.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"base_url\")\n@classmethod\ndef validate_base_url_format(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate base_url is a valid HTTP(S) URL with a host.\"\"\"\n    if v is None:\n        return v\n    from urllib.parse import urlparse\n\n    parsed = urlparse(v)\n    if parsed.scheme not in {\"http\", \"https\"}:\n        raise ValueError(\"base_url must start with http:// or https://\")\n    if not parsed.netloc:\n        raise ValueError(\n            \"base_url must include a host (e.g., localhost, api.example.com)\"\n        )\n    return v\n</code></pre>"},{"location":"api/#ondine.LLMSpec.validate_azure_config","title":"validate_azure_config  <code>classmethod</code>","text":"<pre><code>validate_azure_config(v: str | None, info: Any) -&gt; str | None\n</code></pre> <p>Validate Azure-specific configuration.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"azure_endpoint\", \"azure_deployment\")\n@classmethod\ndef validate_azure_config(cls, v: str | None, info: Any) -&gt; str | None:\n    \"\"\"Validate Azure-specific configuration.\"\"\"\n    if info.data.get(\"provider\") == LLMProvider.AZURE_OPENAI and v is None:\n        field_name = info.field_name\n        raise ValueError(f\"{field_name} required for Azure OpenAI provider\")\n    return v\n</code></pre>"},{"location":"api/#ondine.LLMSpec.validate_provider_requirements","title":"validate_provider_requirements","text":"<pre><code>validate_provider_requirements() -&gt; LLMSpec\n</code></pre> <p>Validate provider-specific requirements.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    \"\"\"Validate provider-specific requirements.\"\"\"\n    # Check openai_compatible requires base_url\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineSpecifications","title":"PipelineSpecifications","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for all pipeline specifications.</p>"},{"location":"api/#ondine.ProcessingSpec","title":"ProcessingSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for processing parameters.</p> <p>Controls how the pipeline executes: batch sizes, concurrency, error handling, rate limiting, and budget constraints.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of rows per batch (1-1000, default: 100)</p> <code>concurrency</code> <code>int</code> <p>Number of parallel LLM requests (1-20, default: 5)</p> <code>checkpoint_interval</code> <code>int</code> <p>Save checkpoint every N rows (default: 500)</p> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts for failed requests (default: 3)</p> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds (default: 1.0)</p> <code>error_policy</code> <code>ErrorPolicy</code> <p>How to handle errors (retry, skip, fail, use_default)</p> <code>rate_limit_rpm</code> <code>int | None</code> <p>Requests per minute limit (optional, no limit if None)</p> <code>max_budget</code> <code>Decimal | None</code> <p>Maximum cost in USD (optional, no limit if None)</p> <code>enable_preprocessing</code> <code>bool</code> <p>Enable input text preprocessing (default: False)</p> <code>preprocessing_max_length</code> <code>int</code> <p>Max characters after preprocessing (default: 500)</p> <code>auto_retry_failed</code> <code>bool</code> <p>Auto-retry rows with null/empty outputs (default: False)</p> <code>max_retry_attempts</code> <code>int</code> <p>Max retry attempts for failed rows (1-3, default: 1)</p> <code>use_jinja2</code> <code>bool</code> <p>Use Jinja2 for template rendering, enables loops/conditionals (default: False)</p> <code>progress_mode</code> <code>str</code> <p>Progress tracking mode (auto, rich, tqdm, logging, none)</p> Example <pre><code># Conservative settings (free tier)\nspec = ProcessingSpec(\n    batch_size=50,\n    concurrency=5,\n    rate_limit_rpm=25,\n    max_budget=Decimal(\"5.0\")\n)\n\n# Aggressive settings (paid tier)\nspec = ProcessingSpec(\n    batch_size=200,\n    concurrency=20,\n    rate_limit_rpm=100,\n    max_budget=Decimal(\"50.0\")\n)\n\n# Fault-tolerant settings\nspec = ProcessingSpec(\n    max_retries=5,\n    error_policy=ErrorPolicy.RETRY,\n    checkpoint_interval=100\n)\n</code></pre>"},{"location":"api/#ondine.ProcessingSpec.validate_checkpoint_dir","title":"validate_checkpoint_dir  <code>classmethod</code>","text":"<pre><code>validate_checkpoint_dir(v: str | Path) -&gt; Path\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"checkpoint_dir\")\n@classmethod\ndef validate_checkpoint_dir(cls, v: str | Path) -&gt; Path:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/#ondine.PromptSpec","title":"PromptSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for prompt template configuration.</p>"},{"location":"api/#ondine.PromptSpec.validate_template","title":"validate_template  <code>classmethod</code>","text":"<pre><code>validate_template(v: str) -&gt; str\n</code></pre> <p>Validate template has at least one variable.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"template\")\n@classmethod\ndef validate_template(cls, v: str) -&gt; str:\n    \"\"\"Validate template has at least one variable.\"\"\"\n    if \"{\" not in v or \"}\" not in v:\n        raise ValueError(\n            \"Template must contain at least one variable in {var} format\"\n        )\n    return v\n</code></pre>"},{"location":"api/#ondine.PromptSpec.validate_response_format","title":"validate_response_format  <code>classmethod</code>","text":"<pre><code>validate_response_format(v: str) -&gt; str\n</code></pre> <p>Validate response format is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"response_format\")\n@classmethod\ndef validate_response_format(cls, v: str) -&gt; str:\n    \"\"\"Validate response format is supported.\"\"\"\n    allowed = [\"raw\", \"json\", \"regex\"]\n    if v not in allowed:\n        raise ValueError(f\"response_format must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/#ondine.PromptSpec.validate_batch_strategy","title":"validate_batch_strategy  <code>classmethod</code>","text":"<pre><code>validate_batch_strategy(v: str) -&gt; str\n</code></pre> <p>Validate batch strategy is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"batch_strategy\")\n@classmethod\ndef validate_batch_strategy(cls, v: str) -&gt; str:\n    \"\"\"Validate batch strategy is supported.\"\"\"\n    allowed = [\"json\", \"csv\"]\n    if v not in allowed:\n        raise ValueError(f\"batch_strategy must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>ondine<ul> <li>adapters<ul> <li>checkpoint_storage</li> <li>data_io</li> <li>llm_client</li> <li>provider_registry</li> </ul> </li> <li>api<ul> <li>dataset_processor</li> <li>health_check</li> <li>pipeline</li> <li>pipeline_builder</li> <li>pipeline_composer</li> <li>quick</li> </ul> </li> <li>cli<ul> <li>main</li> </ul> </li> <li>config<ul> <li>config_loader</li> </ul> </li> <li>core<ul> <li>error_handler</li> <li>exceptions</li> <li>models</li> <li>specifications</li> </ul> </li> <li>integrations<ul> <li>airflow</li> <li>prefect</li> </ul> </li> <li>observability<ul> <li>base</li> <li>dispatcher</li> <li>events</li> <li>llamaindex_handlers</li> <li>observer</li> <li>observers<ul> <li>langfuse_observer</li> <li>logging_observer</li> <li>opentelemetry_observer</li> </ul> </li> <li>registry</li> <li>sanitizer</li> <li>tracer</li> </ul> </li> <li>orchestration<ul> <li>async_executor</li> <li>execution_context</li> <li>execution_strategy</li> <li>observers</li> <li>pipeline_executor</li> <li>progress_tracker</li> <li>state_manager</li> <li>streaming_executor</li> <li>sync_executor</li> </ul> </li> <li>stages<ul> <li>batch_aggregator_stage</li> <li>batch_disaggregator_stage</li> <li>data_loader_stage</li> <li>llm_invocation_stage</li> <li>multi_run_stage</li> <li>parser_factory</li> <li>pipeline_stage</li> <li>prompt_formatter_stage</li> <li>response_parser_stage</li> <li>result_writer_stage</li> <li>stage_registry</li> <li>streaming_loader_stage</li> </ul> </li> <li>strategies<ul> <li>batch_formatting</li> <li>json_batch_strategy</li> <li>models</li> </ul> </li> <li>utils<ul> <li>budget_controller</li> <li>cost_calculator</li> <li>cost_tracker</li> <li>input_preprocessing</li> <li>logging_utils</li> <li>metrics_exporter</li> <li>model_context_limits</li> <li>rate_limiter</li> <li>retry_handler</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/adapters/","title":"adapters","text":""},{"location":"api/adapters/#ondine.adapters","title":"adapters","text":"<p>Infrastructure adapters for external systems.</p>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage","title":"CheckpointStorage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for checkpoint storage implementations.</p> <p>Follows Strategy pattern for pluggable storage backends.</p>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Unique session identifier</p> required <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Save checkpoint data.\n\n    Args:\n        session_id: Unique session identifier\n        data: Checkpoint data to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load latest checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Checkpoint data or None if not found</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Load latest checkpoint data.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Checkpoint data or None if not found\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.list_checkpoints","title":"list_checkpoints  <code>abstractmethod</code>","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if exists</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if checkpoint exists.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if exists\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage","title":"LocalFileCheckpointStorage","text":"<pre><code>LocalFileCheckpointStorage(checkpoint_dir: Path = Path('.checkpoints'), use_json: bool = True)\n</code></pre> <p>               Bases: <code>CheckpointStorage</code></p> <p>Local filesystem checkpoint storage implementation.</p> <p>Stores checkpoints as JSON files for human readability and debugging.</p> <p>Initialize local file checkpoint storage.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory for checkpoints</p> <code>Path('.checkpoints')</code> <code>use_json</code> <code>bool</code> <p>Use JSON format (True) or pickle (False)</p> <code>True</code> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path = Path(\".checkpoints\"),\n    use_json: bool = True,\n):\n    \"\"\"\n    Initialize local file checkpoint storage.\n\n    Args:\n        checkpoint_dir: Directory for checkpoints\n        use_json: Use JSON format (True) or pickle (False)\n    \"\"\"\n    self.checkpoint_dir = checkpoint_dir\n    self.use_json = use_json\n\n    # Create directory if doesn't exist\n    self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.save","title":"save","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint to local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"Save checkpoint to local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    # Add metadata\n    checkpoint_data = {\n        \"version\": \"1.0\",\n        \"session_id\": str(session_id),\n        \"timestamp\": datetime.now().isoformat(),\n        \"data\": data,\n    }\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path, \"w\") as f:\n                json.dump(\n                    checkpoint_data,\n                    f,\n                    indent=2,\n                    default=str,  # Handle non-serializable types\n                )\n        else:\n            with open(checkpoint_path, \"wb\") as f:\n                pickle.dump(checkpoint_data, f)\n\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.load","title":"load","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load checkpoint from local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"Load checkpoint from local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if not checkpoint_path.exists():\n        return None\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path) as f:\n                checkpoint_data = json.load(f)\n        else:\n            with open(checkpoint_path, \"rb\") as f:\n                checkpoint_data = pickle.load(f)\n\n        return checkpoint_data.get(\"data\")\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all checkpoints in directory.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"List all checkpoints in directory.\"\"\"\n    checkpoints = []\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        try:\n            # Extract session ID from filename\n            session_id_str = checkpoint_file.stem.replace(\"checkpoint_\", \"\")\n            session_id = UUID(session_id_str)\n\n            # Get file stats\n            stat = checkpoint_file.stat()\n\n            # Try to load checkpoint for additional info\n            data = self.load(session_id)\n            row_index = data.get(\"last_processed_row\", 0) if data else 0\n            stage_index = data.get(\"current_stage_index\", 0) if data else 0\n\n            checkpoints.append(\n                CheckpointInfo(\n                    session_id=session_id,\n                    checkpoint_path=str(checkpoint_file),\n                    row_index=row_index,\n                    stage_index=stage_index,\n                    timestamp=datetime.fromtimestamp(stat.st_mtime),\n                    size_bytes=stat.st_size,\n                )\n            )\n        except Exception:  # nosec B112\n            # Skip invalid checkpoint files\n            continue\n\n    return sorted(checkpoints, key=lambda x: x.timestamp, reverse=True)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.delete","title":"delete","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"Delete checkpoint file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if checkpoint_path.exists():\n        try:\n            checkpoint_path.unlink()\n            return True\n        except Exception:\n            return False\n    return False\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.exists","title":"exists","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"Check if checkpoint exists.\"\"\"\n    return self._get_checkpoint_path(session_id).exists()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.cleanup_old_checkpoints","title":"cleanup_old_checkpoints","text":"<pre><code>cleanup_old_checkpoints(days: int = 7) -&gt; int\n</code></pre> <p>Delete checkpoints older than specified days.</p> <p>Parameters:</p> Name Type Description Default <code>days</code> <code>int</code> <p>Age threshold in days</p> <code>7</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of checkpoints deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def cleanup_old_checkpoints(self, days: int = 7) -&gt; int:\n    \"\"\"\n    Delete checkpoints older than specified days.\n\n    Args:\n        days: Age threshold in days\n\n    Returns:\n        Number of checkpoints deleted\n    \"\"\"\n    deleted = 0\n    cutoff = datetime.now().timestamp() - (days * 86400)\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        if checkpoint_file.stat().st_mtime &lt; cutoff:\n            try:\n                checkpoint_file.unlink()\n                deleted += 1\n            except Exception:  # nosec B112\n                continue\n\n    return deleted\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVReader","title":"CSVReader","text":"<pre><code>CSVReader(file_path: Path, delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>CSV file reader implementation.</p> <p>Initialize CSV reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to CSV file</p> required <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(\n    self,\n    file_path: Path,\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n):\n    \"\"\"\n    Initialize CSV reader.\n\n    Args:\n        file_path: Path to CSV file\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.file_path = file_path\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire CSV file.\"\"\"\n    return pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read CSV in chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Read CSV in chunks.\"\"\"\n    yield from pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n        chunksize=chunk_size,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVWriter","title":"CSVWriter","text":"<pre><code>CSVWriter(delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataWriter</code></p> <p>CSV file writer implementation.</p> <p>Initialize CSV writer.</p> <p>Parameters:</p> Name Type Description Default <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, delimiter: str = \",\", encoding: str = \"utf-8\"):\n    \"\"\"\n    Initialize CSV writer.\n\n    Args:\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV file.\"\"\"\n    data.to_csv(\n        path,\n        sep=self.delimiter,\n        encoding=self.encoding,\n        index=False,\n    )\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        # Write to temp file\n        data.to_csv(\n            temp_path,\n            sep=self.delimiter,\n            encoding=self.encoding,\n            index=False,\n        )\n\n        # Atomic rename\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        # Cleanup on failure\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(dataframe: DataFrame)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>In-memory DataFrame reader (pass-through).</p> <p>Initialize DataFrame reader.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame):\n    \"\"\"\n    Initialize DataFrame reader.\n\n    Args:\n        dataframe: Pandas DataFrame\n    \"\"\"\n    self.dataframe = dataframe.copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataFrameReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Return DataFrame copy.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame copy.\"\"\"\n    return self.dataframe.copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataFrameReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Yield DataFrame chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Yield DataFrame chunks.\"\"\"\n    for i in range(0, len(self.dataframe), chunk_size):\n        yield self.dataframe.iloc[i : i + chunk_size].copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataReader","title":"DataReader","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data readers.</p> <p>Follows Open/Closed principle: open for extension via new readers, closed for modification.</p>"},{"location":"api/adapters/#ondine.adapters.DataReader.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with all data</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Read entire dataset.\n\n    Returns:\n        DataFrame with all data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataReader.read_chunked","title":"read_chunked  <code>abstractmethod</code>","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read data in chunks for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrame chunks</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read data in chunks for memory efficiency.\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Yields:\n        DataFrame chunks\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataWriter","title":"DataWriter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data writers.</p> <p>Follows Single Responsibility: only handles data persistence.</p>"},{"location":"api/adapters/#ondine.adapters.DataWriter.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data to destination.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data to destination.\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataWriter.atomic_write","title":"atomic_write  <code>abstractmethod</code>","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data atomically (with rollback on failure).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data atomically (with rollback on failure).\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelReader","title":"ExcelReader","text":"<pre><code>ExcelReader(file_path: Path, sheet_name: str | int = 0)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Excel file reader implementation.</p> <p>Initialize Excel reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Excel file</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path, sheet_name: str | int = 0):\n    \"\"\"\n    Initialize Excel reader.\n\n    Args:\n        file_path: Path to Excel file\n        sheet_name: Sheet name or index\n    \"\"\"\n    self.file_path = file_path\n    self.sheet_name = sheet_name\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Excel file.\"\"\"\n    return pd.read_excel(self.file_path, sheet_name=self.sheet_name)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Excel in chunks.</p> <p>Note: Excel doesn't support native chunking, so we load all and yield chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Excel in chunks.\n\n    Note: Excel doesn't support native chunking, so we load all\n    and yield chunks.\n    \"\"\"\n    df = self.read()\n    for i in range(0, len(df), chunk_size):\n        yield df.iloc[i : i + chunk_size]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelWriter","title":"ExcelWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Excel file writer implementation.</p>"},{"location":"api/adapters/#ondine.adapters.ExcelWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel file.\"\"\"\n    data.to_excel(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_excel(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetReader","title":"ParquetReader","text":"<pre><code>ParquetReader(file_path: Path)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Parquet file reader implementation.</p> <p>Initialize Parquet reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Parquet file</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path):\n    \"\"\"\n    Initialize Parquet reader.\n\n    Args:\n        file_path: Path to Parquet file\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Parquet file.\"\"\"\n    return pd.read_parquet(self.file_path)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Parquet in chunks using Polars for efficiency.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Parquet in chunks using Polars for efficiency.\n    \"\"\"\n    # Use Polars for efficient chunked reading\n    lf = pl.scan_parquet(self.file_path)\n\n    # Read in batches\n    total_rows = lf.select(pl.len()).collect().item()\n\n    for i in range(0, total_rows, chunk_size):\n        chunk = lf.slice(i, chunk_size).collect().to_pandas()\n        yield chunk\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetWriter","title":"ParquetWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Parquet file writer implementation.</p>"},{"location":"api/adapters/#ondine.adapters.ParquetWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet file.\"\"\"\n    data.to_parquet(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_parquet(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AnthropicClient","title":"AnthropicClient","text":"<pre><code>AnthropicClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Anthropic Claude LLM client implementation.</p> <p>Initialize Anthropic client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Anthropic client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        raise ValueError(\"ANTHROPIC_API_KEY not found in spec or environment\")\n\n    # Support custom base_url for Anthropic-compatible endpoints (e.g., Z.AI)\n    client_kwargs = {\n        \"model\": spec.model,\n        \"api_key\": api_key,\n        \"temperature\": spec.temperature,\n        \"max_tokens\": spec.max_tokens or 1024,\n    }\n\n    if spec.base_url:\n        client_kwargs[\"base_url\"] = spec.base_url\n\n    self.client = Anthropic(**client_kwargs)\n\n    # Anthropic uses approximate token counting\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AnthropicClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Anthropic API with prompt caching.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Anthropic API with prompt caching.\"\"\"\n    start_time = time.time()\n\n    # Build messages array with optional system message\n    messages = []\n\n    # Extract system message from kwargs\n    system_message = kwargs.get(\"system_message\")\n\n    # Anthropic uses a separate system parameter with cache_control\n    # Until explicit cache_control is wired up, send system message to avoid dropping it\n    if system_message:\n        if self.spec.enable_prefix_caching:\n            # TODO: Wire up explicit cache_control system param when LlamaIndex supports it\n            # For now, send as system message (Anthropic caches automatically)\n            messages.append(ChatMessage(role=\"system\", content=system_message))\n        else:\n            # Fallback: prepend to user message if caching disabled\n            prompt = f\"{system_message}\\n\\n{prompt}\"\n\n    # User message (dynamic, not cached)\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n\n    response = self.client.chat(messages)\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Approximate token usage (include system message if present)\n    total_prompt = prompt\n    if system_message:\n        total_prompt = system_message + \"\\n\" + prompt\n    tokens_in = len(self.tokenizer.encode(total_prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AnthropicClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens (approximate for Anthropic).</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens (approximate for Anthropic).\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AzureOpenAIClient","title":"AzureOpenAIClient","text":"<pre><code>AzureOpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Azure OpenAI LLM client implementation.</p> <p>Initialize Azure OpenAI client with API key or Managed Identity.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Azure OpenAI client with API key or Managed Identity.\"\"\"\n    super().__init__(spec)\n\n    if not spec.azure_endpoint:\n        raise ValueError(\"azure_endpoint required for Azure OpenAI\")\n\n    if not spec.azure_deployment:\n        raise ValueError(\"azure_deployment required for Azure OpenAI\")\n\n    # Authentication: Three options in priority order\n    # 1. Managed Identity (preferred for Azure deployments)\n    # 2. Pre-fetched Azure AD token\n    # 3. API key (backward compatible)\n\n    if spec.use_managed_identity:\n        # Use Azure Managed Identity\n        try:\n            from azure.identity import DefaultAzureCredential\n        except ImportError:\n            raise ImportError(\n                \"Azure Managed Identity requires azure-identity. \"\n                \"Install with: pip install ondine[azure]\"\n            )\n\n        try:\n            credential = DefaultAzureCredential()\n            token = credential.get_token(\n                \"https://cognitiveservices.azure.com/.default\"\n            )\n\n            self.client = AzureOpenAI(\n                model=spec.model,\n                deployment_name=spec.azure_deployment,\n                azure_ad_token=token.token,\n                azure_endpoint=spec.azure_endpoint,\n                api_version=spec.api_version or \"2024-02-15-preview\",\n                temperature=spec.temperature,\n                max_tokens=spec.max_tokens,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to authenticate with Azure Managed Identity: {e}. \"\n                \"Ensure the resource has a Managed Identity assigned with \"\n                \"'Cognitive Services OpenAI User' role.\"\n            ) from e\n\n    elif spec.azure_ad_token:\n        # Use pre-fetched token\n        self.client = AzureOpenAI(\n            model=spec.model,\n            deployment_name=spec.azure_deployment,\n            azure_ad_token=spec.azure_ad_token,\n            azure_endpoint=spec.azure_endpoint,\n            api_version=spec.api_version or \"2024-02-15-preview\",\n            temperature=spec.temperature,\n            max_tokens=spec.max_tokens,\n        )\n\n    else:\n        # Use API key (existing behavior - backward compatible)\n        api_key = spec.api_key or os.getenv(\"AZURE_OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"Azure OpenAI requires either:\\n\"\n                \"  1. use_managed_identity=True (for keyless auth), or\\n\"\n                \"  2. api_key parameter, or\\n\"\n                \"  3. AZURE_OPENAI_API_KEY environment variable\"\n            )\n\n        self.client = AzureOpenAI(\n            model=spec.model,\n            deployment_name=spec.azure_deployment,\n            api_key=api_key,\n            azure_endpoint=spec.azure_endpoint,\n            api_version=spec.api_version or \"2024-02-15-preview\",\n            temperature=spec.temperature,\n            max_tokens=spec.max_tokens,\n        )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AzureOpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Azure OpenAI API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Azure OpenAI API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AzureOpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.GroqClient","title":"GroqClient","text":"<pre><code>GroqClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Groq LLM client implementation.</p> <p>Initialize Groq client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Groq client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"GROQ_API_KEY\")\n    if not api_key:\n        raise ValueError(\"GROQ_API_KEY not found in spec or environment\")\n\n    self.client = Groq(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Use tiktoken for token estimation\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\n    # Initialize logger for debug logging\n    from ondine.utils import get_logger\n\n    self.logger = get_logger(f\"{__name__}.GroqClient\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.GroqClient.structured_invoke","title":"structured_invoke","text":"<pre><code>structured_invoke(prompt: str, output_cls: type[BaseModel], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Groq-specific override: Use LLMTextCompletionProgram instead of tool calling.</p> <p>REASON: After extensive testing, LlamaIndex's structured_predict with Groq produces XML-wrapped tool calls () that Groq's API rejects with 400 tool_use_failed, regardless of client configuration. <p>SOLUTION: Use prompt-based extraction with JSON mode enforcement. This is reliable and produces valid structured output.</p> <p>See: https://github.com/run-llama/llama_index/issues/17082</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def structured_invoke(\n    self,\n    prompt: str,\n    output_cls: type[BaseModel],\n    **kwargs: Any,\n) -&gt; LLMResponse:\n    \"\"\"\n    Groq-specific override: Use LLMTextCompletionProgram instead of tool calling.\n\n    REASON: After extensive testing, LlamaIndex's structured_predict with Groq\n    produces XML-wrapped tool calls (&lt;function=...&gt;) that Groq's API rejects\n    with 400 tool_use_failed, regardless of client configuration.\n\n    SOLUTION: Use prompt-based extraction with JSON mode enforcement.\n    This is reliable and produces valid structured output.\n\n    See: https://github.com/run-llama/llama_index/issues/17082\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        from llama_index.core.program import LLMTextCompletionProgram\n\n        if isinstance(prompt, str):\n            prompt_tmpl = PromptTemplate(prompt)\n        else:\n            prompt_tmpl = prompt\n\n        # Use text completion with JSON mode (bypasses tool calling XML bug)\n        program = LLMTextCompletionProgram.from_defaults(\n            output_cls=output_cls,\n            llm=self.client,\n            prompt_template_str=\"{prompt}\",\n        )\n\n        # Enforce JSON mode to prevent trailing text/malformed output\n        result_obj = program(\n            prompt=prompt_tmpl,\n            llm_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n        )\n\n        latency_ms = (time.time() - start_time) * 1000\n        response_text = result_obj.model_dump_json()\n\n        tokens_in = self.estimate_tokens(prompt)\n        tokens_out = self.estimate_tokens(response_text)\n        cost = self.calculate_cost(tokens_in, tokens_out)\n\n        return LLMResponse(\n            text=response_text,\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            model=self.model,\n            cost=cost,\n            latency_ms=latency_ms,\n        )\n\n    except Exception as e:\n        raise ValueError(f\"Groq structured prediction failed: {e}\") from e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.GroqClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Groq API with optional system message support.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Groq API with optional system message support.\"\"\"\n    start_time = time.time()\n\n    # Build messages array (support system message for caching)\n    messages = []\n\n    system_message = kwargs.get(\"system_message\")\n    if system_message and self.spec.enable_prefix_caching:\n        messages.append(ChatMessage(role=\"system\", content=system_message))\n\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n\n    # Call Groq API\n    response = self.client.chat(messages)\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract text from response\n    if hasattr(response, \"message\") and hasattr(response.message, \"content\"):\n        response_text = response.message.content or \"\"\n    elif hasattr(response, \"content\"):\n        response_text = response.content or \"\"\n    else:\n        response_text = str(response) if response else \"\"\n\n    # Extract token usage from LlamaIndex response.raw (ChatCompletion object)\n    tokens_in = 0\n    tokens_out = 0\n    cached_tokens = 0\n\n    # LlamaIndex provides actual token counts in response.raw.usage\n    if hasattr(response, \"raw\") and response.raw and hasattr(response.raw, \"usage\"):\n        usage = response.raw.usage\n        tokens_in = getattr(usage, \"prompt_tokens\", 0)\n        tokens_out = getattr(usage, \"completion_tokens\", 0)\n\n        # Extract cached tokens (OpenAI/Groq format: nested in prompt_tokens_details)\n        cached_tokens = 0\n        if hasattr(usage, \"prompt_tokens_details\") and usage.prompt_tokens_details:\n            cached_tokens = getattr(usage.prompt_tokens_details, \"cached_tokens\", 0)\n\n        # Debug: Log first response to see what Groq returns\n        if not hasattr(self, \"_debug_logged\"):\n            self._debug_logged = True\n            self.logger.debug(\n                f\"First API response: {tokens_in} input + {tokens_out} output tokens \"\n                f\"({cached_tokens} cached)\"\n            )\n\n        # Log if caching is detected\n        # Track cache hits (use DEBUG level to avoid spam in production)\n        if cached_tokens &gt; 0:\n            cache_pct = (cached_tokens / tokens_in * 100) if tokens_in &gt; 0 else 0\n            self.logger.info(\n                f\"\u2705 Cache hit! {cached_tokens}/{tokens_in} tokens cached ({cache_pct:.0f}%)\"\n            )\n\n    # Fallback to tiktoken estimation if API doesn't provide counts\n    if tokens_in == 0:\n        full_prompt = (system_message or \"\") + prompt\n        tokens_in = len(self.tokenizer.encode(full_prompt))\n    if tokens_out == 0:\n        tokens_out = len(self.tokenizer.encode(response_text))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.GroqClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient","title":"LLMClient","text":"<pre><code>LLMClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for LLM clients.</p> <p>Defines the contract that all LLM provider implementations must follow, enabling easy swapping of providers (Strategy pattern).</p> <p>Initialize LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"\n    Initialize LLM client.\n\n    Args:\n        spec: LLM specification\n    \"\"\"\n    self.spec = spec\n    self.model = spec.model\n    self.temperature = spec.temperature\n    self.max_tokens = spec.max_tokens\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.invoke","title":"invoke  <code>abstractmethod</code>","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke LLM with a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke LLM with a single prompt.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional model parameters\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.structured_invoke","title":"structured_invoke","text":"<pre><code>structured_invoke(prompt: str, output_cls: type[BaseModel], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke LLM with structured output enforcement (LlamaIndex native).</p> <p>Leverages LlamaIndex's structured_predict to guarantee schema compliance via JSON mode or function calling. Handles prompt formatting, parsing, validation, and retries automatically.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>output_cls</code> <code>type[BaseModel]</code> <p>Pydantic model class for output validation</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters (e.g., system_message)</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with validated structured result (serialized JSON)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If structured prediction fails after retries</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def structured_invoke(\n    self,\n    prompt: str,\n    output_cls: type[BaseModel],\n    **kwargs: Any,\n) -&gt; LLMResponse:\n    \"\"\"\n    Invoke LLM with structured output enforcement (LlamaIndex native).\n\n    Leverages LlamaIndex's structured_predict to guarantee schema compliance\n    via JSON mode or function calling. Handles prompt formatting, parsing,\n    validation, and retries automatically.\n\n    Args:\n        prompt: Text prompt\n        output_cls: Pydantic model class for output validation\n        **kwargs: Additional model parameters (e.g., system_message)\n\n    Returns:\n        LLMResponse with validated structured result (serialized JSON)\n\n    Raises:\n        ValueError: If structured prediction fails after retries\n    \"\"\"\n    start_time = time.time()\n\n    # Extract system_message from kwargs for chat-based models\n    system_message = kwargs.pop(\"system_message\", None)\n\n    # Use LlamaIndex's native structured prediction\n    try:\n        if hasattr(self.client, \"structured_predict\"):\n            # Modern LlamaIndex (v0.10+) - direct method\n            # Ensure prompt is wrapped in PromptTemplate for validation\n            if isinstance(prompt, str):\n                prompt_tmpl = PromptTemplate(prompt)\n            else:\n                prompt_tmpl = prompt\n\n            # Build messages array if system_message is provided\n            if system_message:\n                messages = [\n                    ChatMessage(role=\"system\", content=system_message),\n                    ChatMessage(role=\"user\", content=prompt),\n                ]\n                result_obj = self.client.structured_predict(\n                    output_cls,\n                    messages=messages,\n                )\n            else:\n                result_obj = self.client.structured_predict(\n                    output_cls,\n                    prompt=prompt_tmpl,\n                )\n        else:\n            # Fallback: Use LLMTextCompletionProgram for older versions\n            from llama_index.core.program import LLMTextCompletionProgram\n\n            program = LLMTextCompletionProgram.from_defaults(\n                output_cls=output_cls,\n                llm=self.client,\n                prompt_template_str=\"{prompt}\",\n            )\n            result_obj = program(prompt=prompt)\n\n        latency_ms = (time.time() - start_time) * 1000\n\n        # FIX: LlamaIndex bug - Anthropic returns validation error as string\n        # See: https://github.com/run-llama/llama_index/issues/16604\n        if isinstance(result_obj, str):\n            # Validation failed, LlamaIndex returned error message as string\n            raise ValueError(\n                f\"Model returned validation error instead of structured object: {result_obj[:200]}\"\n            )\n\n        # Serialize result to JSON for pipeline consistency\n        response_text = result_obj.model_dump_json()\n\n        # Estimate tokens (structured_predict doesn't expose usage directly)\n        tokens_in = self.estimate_tokens(prompt)\n        tokens_out = self.estimate_tokens(response_text)\n        cost = self.calculate_cost(tokens_in, tokens_out)\n\n        return LLMResponse(\n            text=response_text,\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            model=self.model,\n            cost=cost,\n            latency_ms=latency_ms,\n        )\n\n    except Exception as e:\n        # Re-raise for RetryHandler to manage\n        raise ValueError(f\"Structured prediction failed: {e}\") from e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.estimate_tokens","title":"estimate_tokens  <code>abstractmethod</code>","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.batch_invoke","title":"batch_invoke","text":"<pre><code>batch_invoke(prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]\n</code></pre> <p>Invoke LLM with multiple prompts.</p> <p>Default implementation: sequential invocation. Subclasses can override for provider-optimized batch processing.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of text prompts</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[LLMResponse]</code> <p>List of LLMResponse objects</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def batch_invoke(self, prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]:\n    \"\"\"\n    Invoke LLM with multiple prompts.\n\n    Default implementation: sequential invocation.\n    Subclasses can override for provider-optimized batch processing.\n\n    Args:\n        prompts: List of text prompts\n        **kwargs: Additional model parameters\n\n    Returns:\n        List of LLMResponse objects\n    \"\"\"\n    return [self.invoke(prompt, **kwargs) for prompt in prompts]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for token usage.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost in USD</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for token usage.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost in USD\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.spec.input_cost_per_1k_tokens or Decimal(\"0.0\"),\n        output_cost_per_1k=self.spec.output_cost_per_1k_tokens or Decimal(\"0.0\"),\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.OpenAIClient","title":"OpenAIClient","text":"<pre><code>OpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>OpenAI LLM client implementation.</p> <p>Initialize OpenAI client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize OpenAI client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY not found in spec or environment\")\n\n    self.client = OpenAI(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.OpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke OpenAI API with optional prompt caching.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke OpenAI API with optional prompt caching.\"\"\"\n    start_time = time.time()\n\n    # Build messages array (OpenAI auto-caches system messages)\n    messages = []\n\n    # Extract system message from kwargs\n    system_message = kwargs.get(\"system_message\")\n    if system_message and self.spec.enable_prefix_caching:\n        messages.append(ChatMessage(role=\"system\", content=system_message))\n\n    # User message (dynamic, not cached)\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n\n    response = self.client.chat(messages)\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage from API response (OpenAI returns actual counts)\n    tokens_in = 0\n    tokens_out = 0\n    cached_tokens = 0\n\n    if hasattr(response, \"raw\") and response.raw and hasattr(response.raw, \"usage\"):\n        usage = response.raw.usage\n        tokens_in = getattr(usage, \"prompt_tokens\", 0)\n        tokens_out = getattr(usage, \"completion_tokens\", 0)\n\n        # Extract cached tokens (OpenAI format: nested in prompt_tokens_details)\n        if hasattr(usage, \"prompt_tokens_details\") and usage.prompt_tokens_details:\n            cached_tokens = getattr(usage.prompt_tokens_details, \"cached_tokens\", 0)\n\n        # Debug: Log first response to see what OpenAI returns\n        if not hasattr(self, \"_debug_logged\"):\n            self._debug_logged = True\n            from ondine.utils import get_logger\n\n            logger = get_logger(f\"{__name__}.OpenAIClient\")\n            logger.debug(\n                f\"First API response: {tokens_in} input + {tokens_out} output tokens \"\n                f\"({cached_tokens} cached)\"\n            )\n\n        # Log if caching is detected\n        if cached_tokens &gt; 0:\n            from ondine.utils import get_logger\n\n            logger = get_logger(f\"{__name__}.OpenAIClient\")\n            cache_pct = (cached_tokens / tokens_in * 100) if tokens_in &gt; 0 else 0\n            logger.info(\n                f\"\u2705 Cache hit! {cached_tokens}/{tokens_in} tokens cached ({cache_pct:.0f}%)\"\n            )\n\n    # Fallback to tiktoken estimation if API doesn't provide counts\n    if tokens_in == 0:\n        total_prompt = prompt\n        if system_message and self.spec.enable_prefix_caching:\n            total_prompt = system_message + \"\\n\" + prompt\n        tokens_in = len(self.tokenizer.encode(total_prompt))\n    if tokens_out == 0:\n        tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.OpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry","title":"ProviderRegistry","text":"<p>Global registry for LLM provider plugins.</p> <p>Enables registration and discovery of custom LLM providers without modifying core code. Uses lazy initialization for built-in providers.</p> Example"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry--register-custom-provider","title":"Register custom provider","text":"<p>@ProviderRegistry.register(\"my_llm\") class MyLLMClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         ...</p>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline.with_llm(provider=\"my_llm\", model=\"my-model\")</p>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(provider_id: str, client_class: type) -&gt; type\n</code></pre> <p>Register an LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")</p> required <code>client_class</code> <code>type</code> <p>LLM client class implementing LLMClient interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered client class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider_id already registered</p> Example <p>@ProviderRegistry.register(\"replicate\") class ReplicateClient(LLMClient):     ...</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef register(cls, provider_id: str, client_class: type) -&gt; type:\n    \"\"\"\n    Register an LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")\n        client_class: LLM client class implementing LLMClient interface\n\n    Returns:\n        The registered client class (enables use as decorator)\n\n    Raises:\n        ValueError: If provider_id already registered\n\n    Example:\n        @ProviderRegistry.register(\"replicate\")\n        class ReplicateClient(LLMClient):\n            ...\n    \"\"\"\n    if provider_id in cls._providers:\n        raise ValueError(\n            f\"Provider '{provider_id}' already registered. \"\n            f\"Use a different provider_id or unregister first.\"\n        )\n\n    cls._providers[provider_id] = client_class\n    return client_class\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(provider_id: str) -&gt; type\n</code></pre> <p>Get provider class by ID.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>LLM client class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Example <p>client_class = ProviderRegistry.get(\"openai\") client = client_class(spec)</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef get(cls, provider_id: str) -&gt; type:\n    \"\"\"\n    Get provider class by ID.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        LLM client class\n\n    Raises:\n        ValueError: If provider not found\n\n    Example:\n        client_class = ProviderRegistry.get(\"openai\")\n        client = client_class(spec)\n    \"\"\"\n    cls._ensure_builtins_registered()\n\n    if provider_id not in cls._providers:\n        available = \", \".join(sorted(cls._providers.keys()))\n        raise ValueError(\n            f\"Unknown provider: '{provider_id}'. Available providers: {available}\"\n        )\n\n    return cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.list_providers","title":"list_providers  <code>classmethod</code>","text":"<pre><code>list_providers() -&gt; dict[str, type]\n</code></pre> <p>List all registered providers.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping provider IDs to client classes</p> Example <p>providers = ProviderRegistry.list_providers() print(f\"Available: {list(providers.keys())}\")</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef list_providers(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered providers.\n\n    Returns:\n        Dictionary mapping provider IDs to client classes\n\n    Example:\n        providers = ProviderRegistry.list_providers()\n        print(f\"Available: {list(providers.keys())}\")\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return cls._providers.copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(provider_id: str) -&gt; bool\n</code></pre> <p>Check if provider is registered.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, provider_id: str) -&gt; bool:\n    \"\"\"\n    Check if provider is registered.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return provider_id in cls._providers\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(provider_id: str) -&gt; None\n</code></pre> <p>Unregister a provider (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, provider_id: str) -&gt; None:\n    \"\"\"\n    Unregister a provider (mainly for testing).\n\n    Args:\n        provider_id: Provider identifier\n\n    Raises:\n        ValueError: If provider not found\n    \"\"\"\n    if provider_id not in cls._providers:\n        raise ValueError(f\"Provider '{provider_id}' not registered\")\n\n    del cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_data_reader","title":"create_data_reader","text":"<pre><code>create_data_reader(source_type: DataSourceType, source_path: Path | None = None, dataframe: DataFrame | None = None, **kwargs: any) -&gt; DataReader\n</code></pre> <p>Factory function to create appropriate data reader.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>DataSourceType</code> <p>Type of data source</p> required <code>source_path</code> <code>Path | None</code> <p>Path to file (for file sources)</p> <code>None</code> <code>dataframe</code> <code>DataFrame | None</code> <p>DataFrame (for DataFrame source)</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Additional reader-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataReader</code> <p>Configured DataReader</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source type not supported or parameters invalid</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_reader(\n    source_type: DataSourceType,\n    source_path: Path | None = None,\n    dataframe: pd.DataFrame | None = None,\n    **kwargs: any,\n) -&gt; DataReader:\n    \"\"\"\n    Factory function to create appropriate data reader.\n\n    Args:\n        source_type: Type of data source\n        source_path: Path to file (for file sources)\n        dataframe: DataFrame (for DataFrame source)\n        **kwargs: Additional reader-specific parameters\n\n    Returns:\n        Configured DataReader\n\n    Raises:\n        ValueError: If source type not supported or parameters invalid\n    \"\"\"\n    if source_type == DataSourceType.CSV:\n        if not source_path:\n            raise ValueError(\"source_path required for CSV\")\n        return CSVReader(\n            source_path,\n            delimiter=kwargs.get(\"delimiter\", \",\"),\n            encoding=kwargs.get(\"encoding\", \"utf-8\"),\n        )\n    if source_type == DataSourceType.EXCEL:\n        if not source_path:\n            raise ValueError(\"source_path required for Excel\")\n        return ExcelReader(source_path, sheet_name=kwargs.get(\"sheet_name\", 0))\n    if source_type == DataSourceType.PARQUET:\n        if not source_path:\n            raise ValueError(\"source_path required for Parquet\")\n        return ParquetReader(source_path)\n    if source_type == DataSourceType.DATAFRAME:\n        if dataframe is None:\n            raise ValueError(\"dataframe required for DataFrame source\")\n        return DataFrameReader(dataframe)\n    raise ValueError(f\"Unsupported source type: {source_type}\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_data_writer","title":"create_data_writer","text":"<pre><code>create_data_writer(destination_type: DataSourceType) -&gt; DataWriter\n</code></pre> <p>Factory function to create appropriate data writer.</p> <p>Parameters:</p> Name Type Description Default <code>destination_type</code> <code>DataSourceType</code> <p>Type of destination</p> required <p>Returns:</p> Type Description <code>DataWriter</code> <p>Configured DataWriter</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If destination type not supported</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_writer(destination_type: DataSourceType) -&gt; DataWriter:\n    \"\"\"\n    Factory function to create appropriate data writer.\n\n    Args:\n        destination_type: Type of destination\n\n    Returns:\n        Configured DataWriter\n\n    Raises:\n        ValueError: If destination type not supported\n    \"\"\"\n    if destination_type == DataSourceType.CSV:\n        return CSVWriter()\n    if destination_type == DataSourceType.EXCEL:\n        return ExcelWriter()\n    if destination_type == DataSourceType.PARQUET:\n        return ParquetWriter()\n    raise ValueError(f\"Unsupported destination: {destination_type}\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_llm_client","title":"create_llm_client","text":"<pre><code>create_llm_client(spec: LLMSpec) -&gt; LLMClient\n</code></pre> <p>Factory function to create appropriate LLM client using ProviderRegistry.</p> <p>Supports both built-in providers (via LLMProvider enum) and custom providers (registered via ProviderRegistry).</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required <p>Returns:</p> Type Description <code>LLMClient</code> <p>Configured LLM client</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not supported</p> Example Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def create_llm_client(spec: LLMSpec) -&gt; LLMClient:\n    \"\"\"\n    Factory function to create appropriate LLM client using ProviderRegistry.\n\n    Supports both built-in providers (via LLMProvider enum) and custom\n    providers (registered via ProviderRegistry).\n\n    Args:\n        spec: LLM specification\n\n    Returns:\n        Configured LLM client\n\n    Raises:\n        ValueError: If provider not supported\n\n    Example:\n        # Built-in provider\n        spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\")\n        client = create_llm_client(spec)\n\n        # Custom provider (registered via @provider decorator)\n        spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\")\n        client = create_llm_client(spec)\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Check if custom provider ID is specified (from PipelineBuilder.with_llm)\n    custom_provider_id = getattr(spec, \"_custom_provider_id\", None)\n    if custom_provider_id:\n        provider_id = custom_provider_id\n    else:\n        # Convert enum to string for registry lookup\n        provider_id = (\n            spec.provider.value\n            if isinstance(spec.provider, LLMProvider)\n            else spec.provider\n        )\n\n    # Get provider class from registry\n    provider_class = ProviderRegistry.get(provider_id)\n\n    # Instantiate and return\n    return provider_class(spec)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_llm_client--built-in-provider","title":"Built-in provider","text":"<p>spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/#ondine.adapters.create_llm_client--custom-provider-registered-via-provider-decorator","title":"Custom provider (registered via @provider decorator)","text":"<p>spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/#ondine.adapters.provider","title":"provider","text":"<pre><code>provider(provider_id: str)\n</code></pre> <p>Decorator to register a custom LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@provider(\"replicate\") class ReplicateClient(LLMClient):     def init(self, spec: LLMSpec):         super().init(spec)         import replicate         self.client = replicate.Client(api_token=spec.api_key)</p> <pre><code>def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n    output = self.client.run(self.model, input={\"prompt\": prompt})\n    return LLMResponse(\n        text=output,\n        tokens_in=self.estimate_tokens(prompt),\n        tokens_out=self.estimate_tokens(output),\n        model=self.model,\n        cost=self.calculate_cost(...),\n        latency_ms=...\n    )\n\ndef estimate_tokens(self, text: str) -&gt; int:\n    return len(text.split())\n</code></pre> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>def provider(provider_id: str):\n    \"\"\"\n    Decorator to register a custom LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @provider(\"replicate\")\n        class ReplicateClient(LLMClient):\n            def __init__(self, spec: LLMSpec):\n                super().__init__(spec)\n                import replicate\n                self.client = replicate.Client(api_token=spec.api_key)\n\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                output = self.client.run(self.model, input={\"prompt\": prompt})\n                return LLMResponse(\n                    text=output,\n                    tokens_in=self.estimate_tokens(prompt),\n                    tokens_out=self.estimate_tokens(output),\n                    model=self.model,\n                    cost=self.calculate_cost(...),\n                    latency_ms=...\n                )\n\n            def estimate_tokens(self, text: str) -&gt; int:\n                return len(text.split())\n    \"\"\"\n\n    def decorator(cls):\n        ProviderRegistry.register(provider_id, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/","title":"checkpoint_storage","text":""},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage","title":"checkpoint_storage","text":"<p>Checkpoint storage for fault tolerance.</p> <p>Provides persistent storage of execution state to enable resume after failures.</p>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage","title":"CheckpointStorage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for checkpoint storage implementations.</p> <p>Follows Strategy pattern for pluggable storage backends.</p>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Unique session identifier</p> required <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Save checkpoint data.\n\n    Args:\n        session_id: Unique session identifier\n        data: Checkpoint data to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load latest checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Checkpoint data or None if not found</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Load latest checkpoint data.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Checkpoint data or None if not found\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.list_checkpoints","title":"list_checkpoints  <code>abstractmethod</code>","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if exists</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if checkpoint exists.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if exists\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage","title":"LocalFileCheckpointStorage","text":"<pre><code>LocalFileCheckpointStorage(checkpoint_dir: Path = Path('.checkpoints'), use_json: bool = True)\n</code></pre> <p>               Bases: <code>CheckpointStorage</code></p> <p>Local filesystem checkpoint storage implementation.</p> <p>Stores checkpoints as JSON files for human readability and debugging.</p> <p>Initialize local file checkpoint storage.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory for checkpoints</p> <code>Path('.checkpoints')</code> <code>use_json</code> <code>bool</code> <p>Use JSON format (True) or pickle (False)</p> <code>True</code> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path = Path(\".checkpoints\"),\n    use_json: bool = True,\n):\n    \"\"\"\n    Initialize local file checkpoint storage.\n\n    Args:\n        checkpoint_dir: Directory for checkpoints\n        use_json: Use JSON format (True) or pickle (False)\n    \"\"\"\n    self.checkpoint_dir = checkpoint_dir\n    self.use_json = use_json\n\n    # Create directory if doesn't exist\n    self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.save","title":"save","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint to local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"Save checkpoint to local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    # Add metadata\n    checkpoint_data = {\n        \"version\": \"1.0\",\n        \"session_id\": str(session_id),\n        \"timestamp\": datetime.now().isoformat(),\n        \"data\": data,\n    }\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path, \"w\") as f:\n                json.dump(\n                    checkpoint_data,\n                    f,\n                    indent=2,\n                    default=str,  # Handle non-serializable types\n                )\n        else:\n            with open(checkpoint_path, \"wb\") as f:\n                pickle.dump(checkpoint_data, f)\n\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.load","title":"load","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load checkpoint from local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"Load checkpoint from local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if not checkpoint_path.exists():\n        return None\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path) as f:\n                checkpoint_data = json.load(f)\n        else:\n            with open(checkpoint_path, \"rb\") as f:\n                checkpoint_data = pickle.load(f)\n\n        return checkpoint_data.get(\"data\")\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all checkpoints in directory.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"List all checkpoints in directory.\"\"\"\n    checkpoints = []\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        try:\n            # Extract session ID from filename\n            session_id_str = checkpoint_file.stem.replace(\"checkpoint_\", \"\")\n            session_id = UUID(session_id_str)\n\n            # Get file stats\n            stat = checkpoint_file.stat()\n\n            # Try to load checkpoint for additional info\n            data = self.load(session_id)\n            row_index = data.get(\"last_processed_row\", 0) if data else 0\n            stage_index = data.get(\"current_stage_index\", 0) if data else 0\n\n            checkpoints.append(\n                CheckpointInfo(\n                    session_id=session_id,\n                    checkpoint_path=str(checkpoint_file),\n                    row_index=row_index,\n                    stage_index=stage_index,\n                    timestamp=datetime.fromtimestamp(stat.st_mtime),\n                    size_bytes=stat.st_size,\n                )\n            )\n        except Exception:  # nosec B112\n            # Skip invalid checkpoint files\n            continue\n\n    return sorted(checkpoints, key=lambda x: x.timestamp, reverse=True)\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.delete","title":"delete","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"Delete checkpoint file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if checkpoint_path.exists():\n        try:\n            checkpoint_path.unlink()\n            return True\n        except Exception:\n            return False\n    return False\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.exists","title":"exists","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"Check if checkpoint exists.\"\"\"\n    return self._get_checkpoint_path(session_id).exists()\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.cleanup_old_checkpoints","title":"cleanup_old_checkpoints","text":"<pre><code>cleanup_old_checkpoints(days: int = 7) -&gt; int\n</code></pre> <p>Delete checkpoints older than specified days.</p> <p>Parameters:</p> Name Type Description Default <code>days</code> <code>int</code> <p>Age threshold in days</p> <code>7</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of checkpoints deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def cleanup_old_checkpoints(self, days: int = 7) -&gt; int:\n    \"\"\"\n    Delete checkpoints older than specified days.\n\n    Args:\n        days: Age threshold in days\n\n    Returns:\n        Number of checkpoints deleted\n    \"\"\"\n    deleted = 0\n    cutoff = datetime.now().timestamp() - (days * 86400)\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        if checkpoint_file.stat().st_mtime &lt; cutoff:\n            try:\n                checkpoint_file.unlink()\n                deleted += 1\n            except Exception:  # nosec B112\n                continue\n\n    return deleted\n</code></pre>"},{"location":"api/adapters/data_io/","title":"data_io","text":""},{"location":"api/adapters/data_io/#ondine.adapters.data_io","title":"data_io","text":"<p>Data I/O adapters for reading and writing tabular data.</p> <p>Provides unified interface for multiple data formats following the Adapter pattern.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataReader","title":"DataReader","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data readers.</p> <p>Follows Open/Closed principle: open for extension via new readers, closed for modification.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataReader.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with all data</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Read entire dataset.\n\n    Returns:\n        DataFrame with all data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataReader.read_chunked","title":"read_chunked  <code>abstractmethod</code>","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read data in chunks for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrame chunks</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read data in chunks for memory efficiency.\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Yields:\n        DataFrame chunks\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVReader","title":"CSVReader","text":"<pre><code>CSVReader(file_path: Path, delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>CSV file reader implementation.</p> <p>Initialize CSV reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to CSV file</p> required <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(\n    self,\n    file_path: Path,\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n):\n    \"\"\"\n    Initialize CSV reader.\n\n    Args:\n        file_path: Path to CSV file\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.file_path = file_path\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire CSV file.\"\"\"\n    return pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read CSV in chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Read CSV in chunks.\"\"\"\n    yield from pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n        chunksize=chunk_size,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelReader","title":"ExcelReader","text":"<pre><code>ExcelReader(file_path: Path, sheet_name: str | int = 0)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Excel file reader implementation.</p> <p>Initialize Excel reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Excel file</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path, sheet_name: str | int = 0):\n    \"\"\"\n    Initialize Excel reader.\n\n    Args:\n        file_path: Path to Excel file\n        sheet_name: Sheet name or index\n    \"\"\"\n    self.file_path = file_path\n    self.sheet_name = sheet_name\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Excel file.\"\"\"\n    return pd.read_excel(self.file_path, sheet_name=self.sheet_name)\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Excel in chunks.</p> <p>Note: Excel doesn't support native chunking, so we load all and yield chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Excel in chunks.\n\n    Note: Excel doesn't support native chunking, so we load all\n    and yield chunks.\n    \"\"\"\n    df = self.read()\n    for i in range(0, len(df), chunk_size):\n        yield df.iloc[i : i + chunk_size]\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetReader","title":"ParquetReader","text":"<pre><code>ParquetReader(file_path: Path)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Parquet file reader implementation.</p> <p>Initialize Parquet reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Parquet file</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path):\n    \"\"\"\n    Initialize Parquet reader.\n\n    Args:\n        file_path: Path to Parquet file\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Parquet file.\"\"\"\n    return pd.read_parquet(self.file_path)\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Parquet in chunks using Polars for efficiency.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Parquet in chunks using Polars for efficiency.\n    \"\"\"\n    # Use Polars for efficient chunked reading\n    lf = pl.scan_parquet(self.file_path)\n\n    # Read in batches\n    total_rows = lf.select(pl.len()).collect().item()\n\n    for i in range(0, total_rows, chunk_size):\n        chunk = lf.slice(i, chunk_size).collect().to_pandas()\n        yield chunk\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(dataframe: DataFrame)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>In-memory DataFrame reader (pass-through).</p> <p>Initialize DataFrame reader.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame):\n    \"\"\"\n    Initialize DataFrame reader.\n\n    Args:\n        dataframe: Pandas DataFrame\n    \"\"\"\n    self.dataframe = dataframe.copy()\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataFrameReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Return DataFrame copy.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame copy.\"\"\"\n    return self.dataframe.copy()\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataFrameReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Yield DataFrame chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Yield DataFrame chunks.\"\"\"\n    for i in range(0, len(self.dataframe), chunk_size):\n        yield self.dataframe.iloc[i : i + chunk_size].copy()\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataWriter","title":"DataWriter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data writers.</p> <p>Follows Single Responsibility: only handles data persistence.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataWriter.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data to destination.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data to destination.\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataWriter.atomic_write","title":"atomic_write  <code>abstractmethod</code>","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data atomically (with rollback on failure).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data atomically (with rollback on failure).\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVWriter","title":"CSVWriter","text":"<pre><code>CSVWriter(delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataWriter</code></p> <p>CSV file writer implementation.</p> <p>Initialize CSV writer.</p> <p>Parameters:</p> Name Type Description Default <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, delimiter: str = \",\", encoding: str = \"utf-8\"):\n    \"\"\"\n    Initialize CSV writer.\n\n    Args:\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV file.\"\"\"\n    data.to_csv(\n        path,\n        sep=self.delimiter,\n        encoding=self.encoding,\n        index=False,\n    )\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        # Write to temp file\n        data.to_csv(\n            temp_path,\n            sep=self.delimiter,\n            encoding=self.encoding,\n            index=False,\n        )\n\n        # Atomic rename\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        # Cleanup on failure\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelWriter","title":"ExcelWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Excel file writer implementation.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel file.\"\"\"\n    data.to_excel(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_excel(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetWriter","title":"ParquetWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Parquet file writer implementation.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet file.\"\"\"\n    data.to_parquet(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_parquet(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.create_data_reader","title":"create_data_reader","text":"<pre><code>create_data_reader(source_type: DataSourceType, source_path: Path | None = None, dataframe: DataFrame | None = None, **kwargs: any) -&gt; DataReader\n</code></pre> <p>Factory function to create appropriate data reader.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>DataSourceType</code> <p>Type of data source</p> required <code>source_path</code> <code>Path | None</code> <p>Path to file (for file sources)</p> <code>None</code> <code>dataframe</code> <code>DataFrame | None</code> <p>DataFrame (for DataFrame source)</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Additional reader-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataReader</code> <p>Configured DataReader</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source type not supported or parameters invalid</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_reader(\n    source_type: DataSourceType,\n    source_path: Path | None = None,\n    dataframe: pd.DataFrame | None = None,\n    **kwargs: any,\n) -&gt; DataReader:\n    \"\"\"\n    Factory function to create appropriate data reader.\n\n    Args:\n        source_type: Type of data source\n        source_path: Path to file (for file sources)\n        dataframe: DataFrame (for DataFrame source)\n        **kwargs: Additional reader-specific parameters\n\n    Returns:\n        Configured DataReader\n\n    Raises:\n        ValueError: If source type not supported or parameters invalid\n    \"\"\"\n    if source_type == DataSourceType.CSV:\n        if not source_path:\n            raise ValueError(\"source_path required for CSV\")\n        return CSVReader(\n            source_path,\n            delimiter=kwargs.get(\"delimiter\", \",\"),\n            encoding=kwargs.get(\"encoding\", \"utf-8\"),\n        )\n    if source_type == DataSourceType.EXCEL:\n        if not source_path:\n            raise ValueError(\"source_path required for Excel\")\n        return ExcelReader(source_path, sheet_name=kwargs.get(\"sheet_name\", 0))\n    if source_type == DataSourceType.PARQUET:\n        if not source_path:\n            raise ValueError(\"source_path required for Parquet\")\n        return ParquetReader(source_path)\n    if source_type == DataSourceType.DATAFRAME:\n        if dataframe is None:\n            raise ValueError(\"dataframe required for DataFrame source\")\n        return DataFrameReader(dataframe)\n    raise ValueError(f\"Unsupported source type: {source_type}\")\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.create_data_writer","title":"create_data_writer","text":"<pre><code>create_data_writer(destination_type: DataSourceType) -&gt; DataWriter\n</code></pre> <p>Factory function to create appropriate data writer.</p> <p>Parameters:</p> Name Type Description Default <code>destination_type</code> <code>DataSourceType</code> <p>Type of destination</p> required <p>Returns:</p> Type Description <code>DataWriter</code> <p>Configured DataWriter</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If destination type not supported</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_writer(destination_type: DataSourceType) -&gt; DataWriter:\n    \"\"\"\n    Factory function to create appropriate data writer.\n\n    Args:\n        destination_type: Type of destination\n\n    Returns:\n        Configured DataWriter\n\n    Raises:\n        ValueError: If destination type not supported\n    \"\"\"\n    if destination_type == DataSourceType.CSV:\n        return CSVWriter()\n    if destination_type == DataSourceType.EXCEL:\n        return ExcelWriter()\n    if destination_type == DataSourceType.PARQUET:\n        return ParquetWriter()\n    raise ValueError(f\"Unsupported destination: {destination_type}\")\n</code></pre>"},{"location":"api/adapters/llm_client/","title":"llm_client","text":""},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client","title":"llm_client","text":"<p>LLM client abstractions and implementations.</p> <p>Provides unified interface for multiple LLM providers following the Adapter pattern and Dependency Inversion principle.</p>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient","title":"LLMClient","text":"<pre><code>LLMClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for LLM clients.</p> <p>Defines the contract that all LLM provider implementations must follow, enabling easy swapping of providers (Strategy pattern).</p> <p>Initialize LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"\n    Initialize LLM client.\n\n    Args:\n        spec: LLM specification\n    \"\"\"\n    self.spec = spec\n    self.model = spec.model\n    self.temperature = spec.temperature\n    self.max_tokens = spec.max_tokens\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.invoke","title":"invoke  <code>abstractmethod</code>","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke LLM with a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke LLM with a single prompt.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional model parameters\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.structured_invoke","title":"structured_invoke","text":"<pre><code>structured_invoke(prompt: str, output_cls: type[BaseModel], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke LLM with structured output enforcement (LlamaIndex native).</p> <p>Leverages LlamaIndex's structured_predict to guarantee schema compliance via JSON mode or function calling. Handles prompt formatting, parsing, validation, and retries automatically.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>output_cls</code> <code>type[BaseModel]</code> <p>Pydantic model class for output validation</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters (e.g., system_message)</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with validated structured result (serialized JSON)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If structured prediction fails after retries</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def structured_invoke(\n    self,\n    prompt: str,\n    output_cls: type[BaseModel],\n    **kwargs: Any,\n) -&gt; LLMResponse:\n    \"\"\"\n    Invoke LLM with structured output enforcement (LlamaIndex native).\n\n    Leverages LlamaIndex's structured_predict to guarantee schema compliance\n    via JSON mode or function calling. Handles prompt formatting, parsing,\n    validation, and retries automatically.\n\n    Args:\n        prompt: Text prompt\n        output_cls: Pydantic model class for output validation\n        **kwargs: Additional model parameters (e.g., system_message)\n\n    Returns:\n        LLMResponse with validated structured result (serialized JSON)\n\n    Raises:\n        ValueError: If structured prediction fails after retries\n    \"\"\"\n    start_time = time.time()\n\n    # Extract system_message from kwargs for chat-based models\n    system_message = kwargs.pop(\"system_message\", None)\n\n    # Use LlamaIndex's native structured prediction\n    try:\n        if hasattr(self.client, \"structured_predict\"):\n            # Modern LlamaIndex (v0.10+) - direct method\n            # Ensure prompt is wrapped in PromptTemplate for validation\n            if isinstance(prompt, str):\n                prompt_tmpl = PromptTemplate(prompt)\n            else:\n                prompt_tmpl = prompt\n\n            # Build messages array if system_message is provided\n            if system_message:\n                messages = [\n                    ChatMessage(role=\"system\", content=system_message),\n                    ChatMessage(role=\"user\", content=prompt),\n                ]\n                result_obj = self.client.structured_predict(\n                    output_cls,\n                    messages=messages,\n                )\n            else:\n                result_obj = self.client.structured_predict(\n                    output_cls,\n                    prompt=prompt_tmpl,\n                )\n        else:\n            # Fallback: Use LLMTextCompletionProgram for older versions\n            from llama_index.core.program import LLMTextCompletionProgram\n\n            program = LLMTextCompletionProgram.from_defaults(\n                output_cls=output_cls,\n                llm=self.client,\n                prompt_template_str=\"{prompt}\",\n            )\n            result_obj = program(prompt=prompt)\n\n        latency_ms = (time.time() - start_time) * 1000\n\n        # FIX: LlamaIndex bug - Anthropic returns validation error as string\n        # See: https://github.com/run-llama/llama_index/issues/16604\n        if isinstance(result_obj, str):\n            # Validation failed, LlamaIndex returned error message as string\n            raise ValueError(\n                f\"Model returned validation error instead of structured object: {result_obj[:200]}\"\n            )\n\n        # Serialize result to JSON for pipeline consistency\n        response_text = result_obj.model_dump_json()\n\n        # Estimate tokens (structured_predict doesn't expose usage directly)\n        tokens_in = self.estimate_tokens(prompt)\n        tokens_out = self.estimate_tokens(response_text)\n        cost = self.calculate_cost(tokens_in, tokens_out)\n\n        return LLMResponse(\n            text=response_text,\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            model=self.model,\n            cost=cost,\n            latency_ms=latency_ms,\n        )\n\n    except Exception as e:\n        # Re-raise for RetryHandler to manage\n        raise ValueError(f\"Structured prediction failed: {e}\") from e\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.estimate_tokens","title":"estimate_tokens  <code>abstractmethod</code>","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.batch_invoke","title":"batch_invoke","text":"<pre><code>batch_invoke(prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]\n</code></pre> <p>Invoke LLM with multiple prompts.</p> <p>Default implementation: sequential invocation. Subclasses can override for provider-optimized batch processing.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of text prompts</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[LLMResponse]</code> <p>List of LLMResponse objects</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def batch_invoke(self, prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]:\n    \"\"\"\n    Invoke LLM with multiple prompts.\n\n    Default implementation: sequential invocation.\n    Subclasses can override for provider-optimized batch processing.\n\n    Args:\n        prompts: List of text prompts\n        **kwargs: Additional model parameters\n\n    Returns:\n        List of LLMResponse objects\n    \"\"\"\n    return [self.invoke(prompt, **kwargs) for prompt in prompts]\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for token usage.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost in USD</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for token usage.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost in USD\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.spec.input_cost_per_1k_tokens or Decimal(\"0.0\"),\n        output_cost_per_1k=self.spec.output_cost_per_1k_tokens or Decimal(\"0.0\"),\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAIClient","title":"OpenAIClient","text":"<pre><code>OpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>OpenAI LLM client implementation.</p> <p>Initialize OpenAI client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize OpenAI client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY not found in spec or environment\")\n\n    self.client = OpenAI(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke OpenAI API with optional prompt caching.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke OpenAI API with optional prompt caching.\"\"\"\n    start_time = time.time()\n\n    # Build messages array (OpenAI auto-caches system messages)\n    messages = []\n\n    # Extract system message from kwargs\n    system_message = kwargs.get(\"system_message\")\n    if system_message and self.spec.enable_prefix_caching:\n        messages.append(ChatMessage(role=\"system\", content=system_message))\n\n    # User message (dynamic, not cached)\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n\n    response = self.client.chat(messages)\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage from API response (OpenAI returns actual counts)\n    tokens_in = 0\n    tokens_out = 0\n    cached_tokens = 0\n\n    if hasattr(response, \"raw\") and response.raw and hasattr(response.raw, \"usage\"):\n        usage = response.raw.usage\n        tokens_in = getattr(usage, \"prompt_tokens\", 0)\n        tokens_out = getattr(usage, \"completion_tokens\", 0)\n\n        # Extract cached tokens (OpenAI format: nested in prompt_tokens_details)\n        if hasattr(usage, \"prompt_tokens_details\") and usage.prompt_tokens_details:\n            cached_tokens = getattr(usage.prompt_tokens_details, \"cached_tokens\", 0)\n\n        # Debug: Log first response to see what OpenAI returns\n        if not hasattr(self, \"_debug_logged\"):\n            self._debug_logged = True\n            from ondine.utils import get_logger\n\n            logger = get_logger(f\"{__name__}.OpenAIClient\")\n            logger.debug(\n                f\"First API response: {tokens_in} input + {tokens_out} output tokens \"\n                f\"({cached_tokens} cached)\"\n            )\n\n        # Log if caching is detected\n        if cached_tokens &gt; 0:\n            from ondine.utils import get_logger\n\n            logger = get_logger(f\"{__name__}.OpenAIClient\")\n            cache_pct = (cached_tokens / tokens_in * 100) if tokens_in &gt; 0 else 0\n            logger.info(\n                f\"\u2705 Cache hit! {cached_tokens}/{tokens_in} tokens cached ({cache_pct:.0f}%)\"\n            )\n\n    # Fallback to tiktoken estimation if API doesn't provide counts\n    if tokens_in == 0:\n        total_prompt = prompt\n        if system_message and self.spec.enable_prefix_caching:\n            total_prompt = system_message + \"\\n\" + prompt\n        tokens_in = len(self.tokenizer.encode(total_prompt))\n    if tokens_out == 0:\n        tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AzureOpenAIClient","title":"AzureOpenAIClient","text":"<pre><code>AzureOpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Azure OpenAI LLM client implementation.</p> <p>Initialize Azure OpenAI client with API key or Managed Identity.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Azure OpenAI client with API key or Managed Identity.\"\"\"\n    super().__init__(spec)\n\n    if not spec.azure_endpoint:\n        raise ValueError(\"azure_endpoint required for Azure OpenAI\")\n\n    if not spec.azure_deployment:\n        raise ValueError(\"azure_deployment required for Azure OpenAI\")\n\n    # Authentication: Three options in priority order\n    # 1. Managed Identity (preferred for Azure deployments)\n    # 2. Pre-fetched Azure AD token\n    # 3. API key (backward compatible)\n\n    if spec.use_managed_identity:\n        # Use Azure Managed Identity\n        try:\n            from azure.identity import DefaultAzureCredential\n        except ImportError:\n            raise ImportError(\n                \"Azure Managed Identity requires azure-identity. \"\n                \"Install with: pip install ondine[azure]\"\n            )\n\n        try:\n            credential = DefaultAzureCredential()\n            token = credential.get_token(\n                \"https://cognitiveservices.azure.com/.default\"\n            )\n\n            self.client = AzureOpenAI(\n                model=spec.model,\n                deployment_name=spec.azure_deployment,\n                azure_ad_token=token.token,\n                azure_endpoint=spec.azure_endpoint,\n                api_version=spec.api_version or \"2024-02-15-preview\",\n                temperature=spec.temperature,\n                max_tokens=spec.max_tokens,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to authenticate with Azure Managed Identity: {e}. \"\n                \"Ensure the resource has a Managed Identity assigned with \"\n                \"'Cognitive Services OpenAI User' role.\"\n            ) from e\n\n    elif spec.azure_ad_token:\n        # Use pre-fetched token\n        self.client = AzureOpenAI(\n            model=spec.model,\n            deployment_name=spec.azure_deployment,\n            azure_ad_token=spec.azure_ad_token,\n            azure_endpoint=spec.azure_endpoint,\n            api_version=spec.api_version or \"2024-02-15-preview\",\n            temperature=spec.temperature,\n            max_tokens=spec.max_tokens,\n        )\n\n    else:\n        # Use API key (existing behavior - backward compatible)\n        api_key = spec.api_key or os.getenv(\"AZURE_OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"Azure OpenAI requires either:\\n\"\n                \"  1. use_managed_identity=True (for keyless auth), or\\n\"\n                \"  2. api_key parameter, or\\n\"\n                \"  3. AZURE_OPENAI_API_KEY environment variable\"\n            )\n\n        self.client = AzureOpenAI(\n            model=spec.model,\n            deployment_name=spec.azure_deployment,\n            api_key=api_key,\n            azure_endpoint=spec.azure_endpoint,\n            api_version=spec.api_version or \"2024-02-15-preview\",\n            temperature=spec.temperature,\n            max_tokens=spec.max_tokens,\n        )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AzureOpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Azure OpenAI API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Azure OpenAI API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AzureOpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AnthropicClient","title":"AnthropicClient","text":"<pre><code>AnthropicClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Anthropic Claude LLM client implementation.</p> <p>Initialize Anthropic client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Anthropic client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        raise ValueError(\"ANTHROPIC_API_KEY not found in spec or environment\")\n\n    # Support custom base_url for Anthropic-compatible endpoints (e.g., Z.AI)\n    client_kwargs = {\n        \"model\": spec.model,\n        \"api_key\": api_key,\n        \"temperature\": spec.temperature,\n        \"max_tokens\": spec.max_tokens or 1024,\n    }\n\n    if spec.base_url:\n        client_kwargs[\"base_url\"] = spec.base_url\n\n    self.client = Anthropic(**client_kwargs)\n\n    # Anthropic uses approximate token counting\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AnthropicClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Anthropic API with prompt caching.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Anthropic API with prompt caching.\"\"\"\n    start_time = time.time()\n\n    # Build messages array with optional system message\n    messages = []\n\n    # Extract system message from kwargs\n    system_message = kwargs.get(\"system_message\")\n\n    # Anthropic uses a separate system parameter with cache_control\n    # Until explicit cache_control is wired up, send system message to avoid dropping it\n    if system_message:\n        if self.spec.enable_prefix_caching:\n            # TODO: Wire up explicit cache_control system param when LlamaIndex supports it\n            # For now, send as system message (Anthropic caches automatically)\n            messages.append(ChatMessage(role=\"system\", content=system_message))\n        else:\n            # Fallback: prepend to user message if caching disabled\n            prompt = f\"{system_message}\\n\\n{prompt}\"\n\n    # User message (dynamic, not cached)\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n\n    response = self.client.chat(messages)\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Approximate token usage (include system message if present)\n    total_prompt = prompt\n    if system_message:\n        total_prompt = system_message + \"\\n\" + prompt\n    tokens_in = len(self.tokenizer.encode(total_prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AnthropicClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens (approximate for Anthropic).</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens (approximate for Anthropic).\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.GroqClient","title":"GroqClient","text":"<pre><code>GroqClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Groq LLM client implementation.</p> <p>Initialize Groq client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Groq client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"GROQ_API_KEY\")\n    if not api_key:\n        raise ValueError(\"GROQ_API_KEY not found in spec or environment\")\n\n    self.client = Groq(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Use tiktoken for token estimation\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\n    # Initialize logger for debug logging\n    from ondine.utils import get_logger\n\n    self.logger = get_logger(f\"{__name__}.GroqClient\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.GroqClient.structured_invoke","title":"structured_invoke","text":"<pre><code>structured_invoke(prompt: str, output_cls: type[BaseModel], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Groq-specific override: Use LLMTextCompletionProgram instead of tool calling.</p> <p>REASON: After extensive testing, LlamaIndex's structured_predict with Groq produces XML-wrapped tool calls () that Groq's API rejects with 400 tool_use_failed, regardless of client configuration. <p>SOLUTION: Use prompt-based extraction with JSON mode enforcement. This is reliable and produces valid structured output.</p> <p>See: https://github.com/run-llama/llama_index/issues/17082</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def structured_invoke(\n    self,\n    prompt: str,\n    output_cls: type[BaseModel],\n    **kwargs: Any,\n) -&gt; LLMResponse:\n    \"\"\"\n    Groq-specific override: Use LLMTextCompletionProgram instead of tool calling.\n\n    REASON: After extensive testing, LlamaIndex's structured_predict with Groq\n    produces XML-wrapped tool calls (&lt;function=...&gt;) that Groq's API rejects\n    with 400 tool_use_failed, regardless of client configuration.\n\n    SOLUTION: Use prompt-based extraction with JSON mode enforcement.\n    This is reliable and produces valid structured output.\n\n    See: https://github.com/run-llama/llama_index/issues/17082\n    \"\"\"\n    start_time = time.time()\n\n    try:\n        from llama_index.core.program import LLMTextCompletionProgram\n\n        if isinstance(prompt, str):\n            prompt_tmpl = PromptTemplate(prompt)\n        else:\n            prompt_tmpl = prompt\n\n        # Use text completion with JSON mode (bypasses tool calling XML bug)\n        program = LLMTextCompletionProgram.from_defaults(\n            output_cls=output_cls,\n            llm=self.client,\n            prompt_template_str=\"{prompt}\",\n        )\n\n        # Enforce JSON mode to prevent trailing text/malformed output\n        result_obj = program(\n            prompt=prompt_tmpl,\n            llm_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n        )\n\n        latency_ms = (time.time() - start_time) * 1000\n        response_text = result_obj.model_dump_json()\n\n        tokens_in = self.estimate_tokens(prompt)\n        tokens_out = self.estimate_tokens(response_text)\n        cost = self.calculate_cost(tokens_in, tokens_out)\n\n        return LLMResponse(\n            text=response_text,\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            model=self.model,\n            cost=cost,\n            latency_ms=latency_ms,\n        )\n\n    except Exception as e:\n        raise ValueError(f\"Groq structured prediction failed: {e}\") from e\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.GroqClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Groq API with optional system message support.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Groq API with optional system message support.\"\"\"\n    start_time = time.time()\n\n    # Build messages array (support system message for caching)\n    messages = []\n\n    system_message = kwargs.get(\"system_message\")\n    if system_message and self.spec.enable_prefix_caching:\n        messages.append(ChatMessage(role=\"system\", content=system_message))\n\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n\n    # Call Groq API\n    response = self.client.chat(messages)\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract text from response\n    if hasattr(response, \"message\") and hasattr(response.message, \"content\"):\n        response_text = response.message.content or \"\"\n    elif hasattr(response, \"content\"):\n        response_text = response.content or \"\"\n    else:\n        response_text = str(response) if response else \"\"\n\n    # Extract token usage from LlamaIndex response.raw (ChatCompletion object)\n    tokens_in = 0\n    tokens_out = 0\n    cached_tokens = 0\n\n    # LlamaIndex provides actual token counts in response.raw.usage\n    if hasattr(response, \"raw\") and response.raw and hasattr(response.raw, \"usage\"):\n        usage = response.raw.usage\n        tokens_in = getattr(usage, \"prompt_tokens\", 0)\n        tokens_out = getattr(usage, \"completion_tokens\", 0)\n\n        # Extract cached tokens (OpenAI/Groq format: nested in prompt_tokens_details)\n        cached_tokens = 0\n        if hasattr(usage, \"prompt_tokens_details\") and usage.prompt_tokens_details:\n            cached_tokens = getattr(usage.prompt_tokens_details, \"cached_tokens\", 0)\n\n        # Debug: Log first response to see what Groq returns\n        if not hasattr(self, \"_debug_logged\"):\n            self._debug_logged = True\n            self.logger.debug(\n                f\"First API response: {tokens_in} input + {tokens_out} output tokens \"\n                f\"({cached_tokens} cached)\"\n            )\n\n        # Log if caching is detected\n        # Track cache hits (use DEBUG level to avoid spam in production)\n        if cached_tokens &gt; 0:\n            cache_pct = (cached_tokens / tokens_in * 100) if tokens_in &gt; 0 else 0\n            self.logger.info(\n                f\"\u2705 Cache hit! {cached_tokens}/{tokens_in} tokens cached ({cache_pct:.0f}%)\"\n            )\n\n    # Fallback to tiktoken estimation if API doesn't provide counts\n    if tokens_in == 0:\n        full_prompt = (system_message or \"\") + prompt\n        tokens_in = len(self.tokenizer.encode(full_prompt))\n    if tokens_out == 0:\n        tokens_out = len(self.tokenizer.encode(response_text))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.GroqClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAICompatibleClient","title":"OpenAICompatibleClient","text":"<pre><code>OpenAICompatibleClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Client for OpenAI-compatible API endpoints.</p> <p>Supports custom providers like Ollama, vLLM, Together.ai, Anyscale, and any other API that implements the OpenAI chat completions format.</p> <p>Initialize OpenAI-compatible client.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification with base_url required</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If base_url not provided</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"\n    Initialize OpenAI-compatible client.\n\n    Args:\n        spec: LLM specification with base_url required\n\n    Raises:\n        ValueError: If base_url not provided\n    \"\"\"\n    super().__init__(spec)\n\n    if not spec.base_url:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n\n    # Get API key (optional for local APIs like Ollama)\n    api_key = spec.api_key or os.getenv(\"OPENAI_COMPATIBLE_API_KEY\") or \"dummy\"\n\n    # Use OpenAILike for custom providers (doesn't validate model names)\n    self.client = OpenAILike(\n        model=spec.model,\n        api_key=api_key,\n        api_base=spec.base_url,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n        is_chat_model=True,  # Assume chat model for OpenAI-compatible APIs\n    )\n\n    # Use provider_name for logging/metrics, or default\n    self.provider_name = spec.provider_name or \"OpenAI-Compatible\"\n\n    # Initialize tokenizer (use default encoding for custom providers)\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\n    # Initialize logger for debug logging\n    from ondine.utils import get_logger\n\n    self.logger = get_logger(f\"{__name__}.{self.provider_name}\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAICompatibleClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke OpenAI-compatible API with optional system message support.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters (including system_message)</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke OpenAI-compatible API with optional system message support.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional model parameters (including system_message)\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    start_time = time.time()\n\n    # Build messages array (support system message for caching)\n    messages = []\n\n    system_message = kwargs.get(\"system_message\")\n    if system_message and self.spec.enable_prefix_caching:\n        messages.append(ChatMessage(role=\"system\", content=system_message))\n\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n\n    # Call API\n    response = self.client.chat(messages)\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract text from response\n    response_text = str(response) if response else \"\"\n\n    # Extract token usage from API response (if available)\n    tokens_in = 0\n    tokens_out = 0\n    cached_tokens = 0\n\n    if hasattr(response, \"raw\") and response.raw and hasattr(response.raw, \"usage\"):\n        usage = response.raw.usage\n        tokens_in = getattr(usage, \"prompt_tokens\", 0)\n        tokens_out = getattr(usage, \"completion_tokens\", 0)\n\n        # Extract cached tokens (OpenAI/Moonshot format: nested in prompt_tokens_details)\n        if hasattr(usage, \"prompt_tokens_details\") and usage.prompt_tokens_details:\n            cached_tokens = getattr(usage.prompt_tokens_details, \"cached_tokens\", 0)\n\n        # Debug: Log first response\n        if not hasattr(self, \"_debug_logged\"):\n            self._debug_logged = True\n            self.logger.debug(\n                f\"First API response: {tokens_in} input + {tokens_out} output tokens \"\n                f\"({cached_tokens} cached)\"\n            )\n\n        # Log if caching is detected\n        # Track cache hits (use DEBUG level to avoid spam in production)\n        if cached_tokens &gt; 0:\n            cache_pct = (cached_tokens / tokens_in * 100) if tokens_in &gt; 0 else 0\n            self.logger.info(\n                f\"\u2705 Cache hit! {cached_tokens}/{tokens_in} tokens cached ({cache_pct:.0f}%)\"\n            )\n\n    # Fallback to tiktoken estimation if API doesn't provide counts\n    if tokens_in == 0:\n        full_prompt = (system_message or \"\") + prompt\n        tokens_in = len(self.tokenizer.encode(full_prompt))\n    if tokens_out == 0:\n        tokens_out = len(self.tokenizer.encode(response_text))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=f\"{self.provider_name}/{self.model}\",  # Show provider in metrics\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAICompatibleClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> <p>Note: This is approximate for custom providers.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate tokens using tiktoken.\n\n    Note: This is approximate for custom providers.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.MLXClient","title":"MLXClient","text":"<pre><code>MLXClient(spec: LLMSpec, _mlx_lm_module=None)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>MLX client for Apple Silicon local inference.</p> <p>MLX is Apple's optimized ML framework for M-series chips. This client enables fast, local LLM inference without API costs.</p> <p>Requires: pip install ondine[mlx] Platform: macOS with Apple Silicon only</p> <p>Initialize MLX client and load model.</p> <p>Model is loaded once and cached for fast subsequent calls.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification with model name</p> required <code>_mlx_lm_module</code> <p>MLX module (internal/testing only)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If MLX not installed</p> <code>Exception</code> <p>If model loading fails</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec, _mlx_lm_module=None):\n    \"\"\"\n    Initialize MLX client and load model.\n\n    Model is loaded once and cached for fast subsequent calls.\n\n    Args:\n        spec: LLM specification with model name\n        _mlx_lm_module: MLX module (internal/testing only)\n\n    Raises:\n        ImportError: If MLX not installed\n        Exception: If model loading fails\n    \"\"\"\n    super().__init__(spec)\n\n    # Load mlx_lm module (or use injected module for testing)\n    if _mlx_lm_module is None:\n        try:\n            import mlx_lm as _mlx_lm_module\n        except ImportError as e:\n            raise ImportError(\n                \"MLX not installed. Install with:\\n\"\n                \"  pip install ondine[mlx]\\n\"\n                \"or:\\n\"\n                \"  pip install mlx mlx-lm\\n\\n\"\n                \"Note: MLX only works on Apple Silicon (M1/M2/M3/M4 chips)\"\n            ) from e\n\n    # Store mlx_lm module for later use\n    self.mlx_lm = _mlx_lm_module\n\n    # Load model once (expensive operation, ~1-2 seconds)\n    print(f\"\ud83d\udd04 Loading MLX model: {spec.model}...\")\n    try:\n        self.mlx_model, self.mlx_tokenizer = self.mlx_lm.load(spec.model)\n        print(\"\u2705 Model loaded successfully\")\n    except Exception as e:\n        raise Exception(\n            f\"Failed to load MLX model '{spec.model}'. \"\n            f\"Ensure the model exists on HuggingFace and you have access. \"\n            f\"Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.MLXClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke MLX model for inference.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke MLX model for inference.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional generation parameters\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    start_time = time.time()\n\n    # Generate response using cached model\n    max_tokens = kwargs.get(\"max_tokens\", self.max_tokens)\n\n    response_text = self.mlx_lm.generate(\n        self.mlx_model,\n        self.mlx_tokenizer,\n        prompt=prompt,\n        max_tokens=max_tokens,\n        verbose=False,\n    )\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Estimate token usage using MLX tokenizer\n    try:\n        tokens_in = len(self.mlx_tokenizer.encode(prompt))\n        tokens_out = len(self.mlx_tokenizer.encode(response_text))\n    except Exception:\n        # Fallback to simple estimation if encoding fails\n        tokens_in = len(prompt.split())\n        tokens_out = len(response_text.split())\n\n    # Calculate cost (typically $0 for local models)\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=f\"MLX/{self.model}\",\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.MLXClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count using MLX tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count using MLX tokenizer.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    try:\n        return len(self.mlx_tokenizer.encode(text))\n    except Exception:\n        # Fallback to simple word count\n        return len(text.split())\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.create_llm_client","title":"create_llm_client","text":"<pre><code>create_llm_client(spec: LLMSpec) -&gt; LLMClient\n</code></pre> <p>Factory function to create appropriate LLM client using ProviderRegistry.</p> <p>Supports both built-in providers (via LLMProvider enum) and custom providers (registered via ProviderRegistry).</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required <p>Returns:</p> Type Description <code>LLMClient</code> <p>Configured LLM client</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not supported</p> Example Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def create_llm_client(spec: LLMSpec) -&gt; LLMClient:\n    \"\"\"\n    Factory function to create appropriate LLM client using ProviderRegistry.\n\n    Supports both built-in providers (via LLMProvider enum) and custom\n    providers (registered via ProviderRegistry).\n\n    Args:\n        spec: LLM specification\n\n    Returns:\n        Configured LLM client\n\n    Raises:\n        ValueError: If provider not supported\n\n    Example:\n        # Built-in provider\n        spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\")\n        client = create_llm_client(spec)\n\n        # Custom provider (registered via @provider decorator)\n        spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\")\n        client = create_llm_client(spec)\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Check if custom provider ID is specified (from PipelineBuilder.with_llm)\n    custom_provider_id = getattr(spec, \"_custom_provider_id\", None)\n    if custom_provider_id:\n        provider_id = custom_provider_id\n    else:\n        # Convert enum to string for registry lookup\n        provider_id = (\n            spec.provider.value\n            if isinstance(spec.provider, LLMProvider)\n            else spec.provider\n        )\n\n    # Get provider class from registry\n    provider_class = ProviderRegistry.get(provider_id)\n\n    # Instantiate and return\n    return provider_class(spec)\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.create_llm_client--built-in-provider","title":"Built-in provider","text":"<p>spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.create_llm_client--custom-provider-registered-via-provider-decorator","title":"Custom provider (registered via @provider decorator)","text":"<p>spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/provider_registry/","title":"provider_registry","text":""},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry","title":"provider_registry","text":"<p>Provider registry for extensible LLM client plugins.</p> <p>Enables custom LLM providers to be registered and discovered without modifying core code.</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.LLMClient","title":"LLMClient","text":"<p>Protocol for LLM client implementations (imported to avoid circular dependency).</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry","title":"ProviderRegistry","text":"<p>Global registry for LLM provider plugins.</p> <p>Enables registration and discovery of custom LLM providers without modifying core code. Uses lazy initialization for built-in providers.</p> Example"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry--register-custom-provider","title":"Register custom provider","text":"<p>@ProviderRegistry.register(\"my_llm\") class MyLLMClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         ...</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline.with_llm(provider=\"my_llm\", model=\"my-model\")</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(provider_id: str, client_class: type) -&gt; type\n</code></pre> <p>Register an LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")</p> required <code>client_class</code> <code>type</code> <p>LLM client class implementing LLMClient interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered client class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider_id already registered</p> Example <p>@ProviderRegistry.register(\"replicate\") class ReplicateClient(LLMClient):     ...</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef register(cls, provider_id: str, client_class: type) -&gt; type:\n    \"\"\"\n    Register an LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")\n        client_class: LLM client class implementing LLMClient interface\n\n    Returns:\n        The registered client class (enables use as decorator)\n\n    Raises:\n        ValueError: If provider_id already registered\n\n    Example:\n        @ProviderRegistry.register(\"replicate\")\n        class ReplicateClient(LLMClient):\n            ...\n    \"\"\"\n    if provider_id in cls._providers:\n        raise ValueError(\n            f\"Provider '{provider_id}' already registered. \"\n            f\"Use a different provider_id or unregister first.\"\n        )\n\n    cls._providers[provider_id] = client_class\n    return client_class\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(provider_id: str) -&gt; type\n</code></pre> <p>Get provider class by ID.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>LLM client class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Example <p>client_class = ProviderRegistry.get(\"openai\") client = client_class(spec)</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef get(cls, provider_id: str) -&gt; type:\n    \"\"\"\n    Get provider class by ID.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        LLM client class\n\n    Raises:\n        ValueError: If provider not found\n\n    Example:\n        client_class = ProviderRegistry.get(\"openai\")\n        client = client_class(spec)\n    \"\"\"\n    cls._ensure_builtins_registered()\n\n    if provider_id not in cls._providers:\n        available = \", \".join(sorted(cls._providers.keys()))\n        raise ValueError(\n            f\"Unknown provider: '{provider_id}'. Available providers: {available}\"\n        )\n\n    return cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.list_providers","title":"list_providers  <code>classmethod</code>","text":"<pre><code>list_providers() -&gt; dict[str, type]\n</code></pre> <p>List all registered providers.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping provider IDs to client classes</p> Example <p>providers = ProviderRegistry.list_providers() print(f\"Available: {list(providers.keys())}\")</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef list_providers(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered providers.\n\n    Returns:\n        Dictionary mapping provider IDs to client classes\n\n    Example:\n        providers = ProviderRegistry.list_providers()\n        print(f\"Available: {list(providers.keys())}\")\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return cls._providers.copy()\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(provider_id: str) -&gt; bool\n</code></pre> <p>Check if provider is registered.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, provider_id: str) -&gt; bool:\n    \"\"\"\n    Check if provider is registered.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return provider_id in cls._providers\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(provider_id: str) -&gt; None\n</code></pre> <p>Unregister a provider (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, provider_id: str) -&gt; None:\n    \"\"\"\n    Unregister a provider (mainly for testing).\n\n    Args:\n        provider_id: Provider identifier\n\n    Raises:\n        ValueError: If provider not found\n    \"\"\"\n    if provider_id not in cls._providers:\n        raise ValueError(f\"Provider '{provider_id}' not registered\")\n\n    del cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.provider","title":"provider","text":"<pre><code>provider(provider_id: str)\n</code></pre> <p>Decorator to register a custom LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@provider(\"replicate\") class ReplicateClient(LLMClient):     def init(self, spec: LLMSpec):         super().init(spec)         import replicate         self.client = replicate.Client(api_token=spec.api_key)</p> <pre><code>def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n    output = self.client.run(self.model, input={\"prompt\": prompt})\n    return LLMResponse(\n        text=output,\n        tokens_in=self.estimate_tokens(prompt),\n        tokens_out=self.estimate_tokens(output),\n        model=self.model,\n        cost=self.calculate_cost(...),\n        latency_ms=...\n    )\n\ndef estimate_tokens(self, text: str) -&gt; int:\n    return len(text.split())\n</code></pre> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>def provider(provider_id: str):\n    \"\"\"\n    Decorator to register a custom LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @provider(\"replicate\")\n        class ReplicateClient(LLMClient):\n            def __init__(self, spec: LLMSpec):\n                super().__init__(spec)\n                import replicate\n                self.client = replicate.Client(api_token=spec.api_key)\n\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                output = self.client.run(self.model, input={\"prompt\": prompt})\n                return LLMResponse(\n                    text=output,\n                    tokens_in=self.estimate_tokens(prompt),\n                    tokens_out=self.estimate_tokens(output),\n                    model=self.model,\n                    cost=self.calculate_cost(...),\n                    latency_ms=...\n                )\n\n            def estimate_tokens(self, text: str) -&gt; int:\n                return len(text.split())\n    \"\"\"\n\n    def decorator(cls):\n        ProviderRegistry.register(provider_id, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/api/","title":"api","text":""},{"location":"api/api/#ondine.api","title":"api","text":"<p>High-level API for pipeline construction and execution.</p>"},{"location":"api/api/#ondine.api.DatasetProcessor","title":"DatasetProcessor","text":"<pre><code>DatasetProcessor(data: str | DataFrame, input_column: str, output_column: str, prompt: str, llm_config: dict[str, any])\n</code></pre> <p>Simplified API for single-prompt, single-column use cases.</p> <p>This is a convenience wrapper around PipelineBuilder for users who don't need fine-grained control.</p> Example <p>processor = DatasetProcessor(     data=\"data.csv\",     input_column=\"description\",     output_column=\"cleaned\",     prompt=\"Clean this text: {description}\",     llm_config={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\"} ) result = processor.run()</p> <p>Initialize dataset processor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>CSV file path or DataFrame</p> required <code>input_column</code> <code>str</code> <p>Input column name</p> required <code>output_column</code> <code>str</code> <p>Output column name</p> required <code>prompt</code> <code>str</code> <p>Prompt template</p> required <code>llm_config</code> <code>dict[str, any]</code> <p>LLM configuration dict</p> required Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    input_column: str,\n    output_column: str,\n    prompt: str,\n    llm_config: dict[str, any],\n):\n    \"\"\"\n    Initialize dataset processor.\n\n    Args:\n        data: CSV file path or DataFrame\n        input_column: Input column name\n        output_column: Output column name\n        prompt: Prompt template\n        llm_config: LLM configuration dict\n    \"\"\"\n    self.data = data\n    self.input_column = input_column\n    self.output_column = output_column\n    self.prompt = prompt\n    self.llm_config = llm_config\n\n    # Build pipeline internally\n    builder = PipelineBuilder.create()\n\n    # Configure data source\n    if isinstance(data, str):\n        builder.from_csv(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    elif isinstance(data, pd.DataFrame):\n        builder.from_dataframe(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    else:\n        raise ValueError(\"data must be file path or DataFrame\")\n\n    # Configure prompt\n    builder.with_prompt(prompt)\n\n    # Configure LLM\n    provider = llm_config.get(\"provider\", \"openai\")\n    model = llm_config.get(\"model\", \"gpt-4o-mini\")\n    api_key = llm_config.get(\"api_key\")\n    temperature = llm_config.get(\"temperature\", 0.0)\n    max_tokens = llm_config.get(\"max_tokens\")\n\n    builder.with_llm(\n        provider=provider,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    # Build pipeline\n    self.pipeline = builder.build()\n</code></pre>"},{"location":"api/api/#ondine.api.DatasetProcessor.run","title":"run","text":"<pre><code>run() -&gt; pd.DataFrame\n</code></pre> <p>Execute processing and return results.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute processing and return results.\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    result = self.pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/#ondine.api.DatasetProcessor.run_sample","title":"run_sample","text":"<pre><code>run_sample(n: int = 10) -&gt; pd.DataFrame\n</code></pre> <p>Test on first N rows.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of rows to process</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with sample results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run_sample(self, n: int = 10) -&gt; pd.DataFrame:\n    \"\"\"\n    Test on first N rows.\n\n    Args:\n        n: Number of rows to process\n\n    Returns:\n        DataFrame with sample results\n    \"\"\"\n    # Create sample pipeline\n    if isinstance(self.data, str):\n        df = pd.read_csv(self.data).head(n)\n    else:\n        df = self.data.head(n)\n\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df,\n            input_columns=[self.input_column],\n            output_columns=[self.output_column],\n        )\n        .with_prompt(self.prompt)\n        .with_llm(\n            provider=self.llm_config.get(\"provider\", \"openai\"),\n            model=self.llm_config.get(\"model\", \"gpt-4o-mini\"),\n            api_key=self.llm_config.get(\"api_key\"),\n            temperature=self.llm_config.get(\"temperature\", 0.0),\n        )\n    )\n\n    sample_pipeline = builder.build()\n    result = sample_pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/#ondine.api.DatasetProcessor.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; float\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost in USD</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def estimate_cost(self) -&gt; float:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Estimated cost in USD\n    \"\"\"\n    estimate = self.pipeline.estimate_cost()\n    return float(estimate.total_cost)\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck","title":"HealthCheck","text":"<pre><code>HealthCheck(pipeline: Pipeline)\n</code></pre> <p>Health check API for monitoring pipeline status.</p> <p>Provides information about pipeline health and readiness.</p> <p>Initialize health check.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>Pipeline instance to monitor</p> required Source code in <code>ondine/api/health_check.py</code> <pre><code>def __init__(self, pipeline: Pipeline):\n    \"\"\"\n    Initialize health check.\n\n    Args:\n        pipeline: Pipeline instance to monitor\n    \"\"\"\n    self.pipeline = pipeline\n    self.last_check: datetime | None = None\n    self.last_status: dict[str, Any] = {}\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck.check","title":"check","text":"<pre><code>check() -&gt; dict[str, Any]\n</code></pre> <p>Perform health check.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Health status dictionary</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def check(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Perform health check.\n\n    Returns:\n        Health status dictionary\n    \"\"\"\n    self.last_check = datetime.now()\n\n    status = {\n        \"status\": \"healthy\",\n        \"timestamp\": self.last_check.isoformat(),\n        \"pipeline_id\": str(self.pipeline.id),\n        \"checks\": {},\n    }\n\n    # Check LLM provider configuration\n    try:\n        llm_spec = self.pipeline.specifications.llm\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"ok\",\n            \"provider\": llm_spec.provider.value,\n            \"model\": llm_spec.model,\n        }\n    except Exception as e:\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check data source configuration\n    try:\n        dataset_spec = self.pipeline.specifications.dataset\n        source_exists = True\n\n        if dataset_spec.source_path:\n            source_exists = dataset_spec.source_path.exists()\n\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"ok\" if source_exists else \"warning\",\n            \"source_type\": dataset_spec.source_type.value,\n            \"exists\": source_exists,\n        }\n    except Exception as e:\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check checkpoint storage\n    try:\n        checkpoint_dir = self.pipeline.specifications.processing.checkpoint_dir\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"ok\",\n            \"directory\": str(checkpoint_dir),\n            \"exists\": checkpoint_dir.exists(),\n        }\n    except Exception as e:\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"warning\",\n            \"error\": str(e),\n        }\n\n    # Store last status\n    self.last_status = status\n\n    return status\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck.is_healthy","title":"is_healthy","text":"<pre><code>is_healthy() -&gt; bool\n</code></pre> <p>Check if pipeline is healthy.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if healthy</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def is_healthy(self) -&gt; bool:\n    \"\"\"\n    Check if pipeline is healthy.\n\n    Returns:\n        True if healthy\n    \"\"\"\n    status = self.check()\n    return status[\"status\"] == \"healthy\"\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck.get_readiness","title":"get_readiness","text":"<pre><code>get_readiness() -&gt; dict[str, Any]\n</code></pre> <p>Get readiness status.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Readiness information</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def get_readiness(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get readiness status.\n\n    Returns:\n        Readiness information\n    \"\"\"\n    validation = self.pipeline.validate()\n\n    return {\n        \"ready\": validation.is_valid,\n        \"errors\": validation.errors,\n        \"warnings\": validation.warnings,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(specifications: PipelineSpecifications, dataframe: DataFrame | None = None, executor: ExecutionStrategy | None = None)\n</code></pre> <p>Main pipeline class - Facade for dataset processing.</p> <p>Provides high-level interface for building and executing LLM-powered data transformations. Handles orchestration, state management, cost tracking, checkpointing, and error handling.</p> <p>This is typically created via PipelineBuilder or QuickPipeline, not directly.</p> Example <pre><code>from ondine import PipelineBuilder\n\n# Create via builder (recommended)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Execute\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.total_rows} rows\")\nprint(f\"Cost: ${result.costs.total_cost}\")\n</code></pre> Note <p>Use PipelineBuilder for construction, not direct instantiation.</p> <p>Initialize pipeline with specifications.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Complete pipeline configuration</p> required <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded DataFrame</p> <code>None</code> <code>executor</code> <code>ExecutionStrategy | None</code> <p>Optional execution strategy (default: SyncExecutor)</p> <code>None</code> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def __init__(\n    self,\n    specifications: PipelineSpecifications,\n    dataframe: pd.DataFrame | None = None,\n    executor: ExecutionStrategy | None = None,\n):\n    \"\"\"\n    Initialize pipeline with specifications.\n\n    Args:\n        specifications: Complete pipeline configuration\n        dataframe: Optional pre-loaded DataFrame\n        executor: Optional execution strategy (default: SyncExecutor)\n    \"\"\"\n    self.id = uuid4()\n    self.specifications = specifications\n    self.dataframe = dataframe\n    self.executor = executor or SyncExecutor()\n    self.observers: list[ExecutionObserver] = []\n    self.logger = get_logger(f\"{__name__}.{self.id}\")\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; Pipeline\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"Pipeline\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.validate","title":"validate","text":"<pre><code>validate() -&gt; ValidationResult\n</code></pre> <p>Validate pipeline configuration.</p> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with any errors/warnings</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def validate(self) -&gt; ValidationResult:\n    \"\"\"\n    Validate pipeline configuration.\n\n    Returns:\n        ValidationResult with any errors/warnings\n    \"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Validate dataset spec\n    if not self.specifications.dataset.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    if not self.specifications.dataset.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    # Validate that input columns exist in dataframe (if dataframe is provided)\n    if self.dataframe is not None and self.specifications.dataset.input_columns:\n        df_cols = set(self.dataframe.columns)\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_cols = input_cols - df_cols\n        if missing_cols:\n            result.add_error(\n                f\"Input columns not found in dataframe: {missing_cols}\"\n            )\n\n    # Validate prompt spec\n    if not self.specifications.prompt.template:\n        result.add_error(\"No prompt template specified\")\n    else:\n        # Check that template variables match input columns\n        import re\n\n        template_vars = set(\n            re.findall(r\"\\{(\\w+)\\}\", self.specifications.prompt.template)\n        )\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_vars = template_vars - input_cols\n        if missing_vars:\n            result.add_error(\n                f\"Template variables not in input columns: {missing_vars}\"\n            )\n\n    # Validate LLM spec\n    if not self.specifications.llm.model:\n        result.add_error(\"No LLM model specified\")\n\n    return result\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; CostEstimate\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def estimate_cost(self) -&gt; CostEstimate:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    # Create stages\n    loader = DataLoaderStage(self.dataframe)\n\n    # Load first few rows for estimation\n    df = loader.process(self.specifications.dataset, ExecutionContext())\n    sample_size = min(10, len(df))\n    sample_df = df.head(sample_size)\n\n    # Create formatter and get prompts\n    formatter = PromptFormatterStage(\n        self.specifications.processing.batch_size,\n        use_jinja2=self.specifications.processing.use_jinja2,\n    )\n    batches = formatter.process(\n        (sample_df, self.specifications.prompt), ExecutionContext()\n    )\n\n    # Create LLM client and estimate\n    llm_client = create_llm_client(self.specifications.llm)\n    llm_stage = LLMInvocationStage(llm_client)\n\n    sample_estimate = llm_stage.estimate_cost(batches)\n\n    # Scale to full dataset\n    scale_factor = Decimal(len(df)) / Decimal(sample_size)\n\n    return CostEstimate(\n        total_cost=sample_estimate.total_cost * scale_factor,\n        total_tokens=int(sample_estimate.total_tokens * float(scale_factor)),\n        input_tokens=int(sample_estimate.input_tokens * float(scale_factor)),\n        output_tokens=int(sample_estimate.output_tokens * float(scale_factor)),\n        rows=len(df),\n        confidence=\"sample-based\",\n    )\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.execute","title":"execute","text":"<pre><code>execute(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Runs all stages: data loading, prompt formatting, LLM invocation, response parsing, and result writing. Handles checkpointing, cost tracking, and error recovery.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint (for fault tolerance)</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult containing: - data: DataFrame with results - metrics: Processing statistics (total_rows, success_count, etc.) - costs: Cost breakdown (total_cost, input_tokens, output_tokens) - duration: Execution time in seconds - errors: List of any errors encountered</p> Example <pre><code># Execute pipeline\nresult = pipeline.execute()\n\n# Access results\nprint(f\"Processed: {result.metrics.total_rows} rows\")\nprint(f\"Successful: {result.metrics.success_count} rows\")\nprint(f\"Cost: ${result.costs.total_cost}\")\nprint(f\"Time: {result.duration:.2f}s\")\n\n# Access output data\nresult.data.to_csv(\"output.csv\", index=False)\n\n# Resume from checkpoint (if pipeline was interrupted)\nresult = pipeline.execute(resume_from=previous_session_id)\n</code></pre> Note <p>Progress is automatically saved via checkpoints. If execution fails, use resume_from to continue from the last checkpoint.</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Runs all stages: data loading, prompt formatting, LLM invocation, response parsing,\n    and result writing. Handles checkpointing, cost tracking, and error recovery.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint (for fault tolerance)\n\n    Returns:\n        ExecutionResult containing:\n            - data: DataFrame with results\n            - metrics: Processing statistics (total_rows, success_count, etc.)\n            - costs: Cost breakdown (total_cost, input_tokens, output_tokens)\n            - duration: Execution time in seconds\n            - errors: List of any errors encountered\n\n    Example:\n        ```python\n        # Execute pipeline\n        result = pipeline.execute()\n\n        # Access results\n        print(f\"Processed: {result.metrics.total_rows} rows\")\n        print(f\"Successful: {result.metrics.success_count} rows\")\n        print(f\"Cost: ${result.costs.total_cost}\")\n        print(f\"Time: {result.duration:.2f}s\")\n\n        # Access output data\n        result.data.to_csv(\"output.csv\", index=False)\n\n        # Resume from checkpoint (if pipeline was interrupted)\n        result = pipeline.execute(resume_from=previous_session_id)\n        ```\n\n    Note:\n        Progress is automatically saved via checkpoints. If execution fails,\n        use resume_from to continue from the last checkpoint.\n    \"\"\"\n    # Validate first\n    validation = self.validate()\n    if not validation.is_valid:\n        raise ValueError(f\"Pipeline validation failed: {validation.errors}\")\n\n    # Create or restore execution context\n    state_manager = StateManager(\n        storage=LocalFileCheckpointStorage(\n            self.specifications.processing.checkpoint_dir\n        ),\n        checkpoint_interval=self.specifications.processing.checkpoint_interval,\n    )\n\n    if resume_from:\n        # Resume from checkpoint\n        context = state_manager.load_checkpoint(resume_from)\n        if not context:\n            raise ValueError(f\"No checkpoint found for session {resume_from}\")\n        self.logger.info(\n            f\"Resuming from checkpoint at row {context.last_processed_row}\"\n        )\n    else:\n        # Create new context\n        context = ExecutionContext(pipeline_id=self.id)\n\n    # Add default observers if none specified\n    if not self.observers:\n        self.observers = [\n            ProgressBarObserver(),\n            LoggingObserver(),\n            CostTrackingObserver(),\n        ]\n\n    # Attach observers to context for progress notifications\n    context.observers = self.observers\n\n    # Initialize new observability system if observers configured\n    observer_configs = self.specifications.metadata.get(\"observers\", [])\n    if observer_configs:\n        from ondine.observability.dispatcher import ObserverDispatcher\n        from ondine.observability.registry import ObserverRegistry\n\n        # Instantiate observers from configuration\n        new_observers = []\n        for observer_name, observer_config in observer_configs:\n            try:\n                observer_class = ObserverRegistry.get(observer_name)\n                observer_instance = observer_class(config=observer_config)\n                new_observers.append(observer_instance)\n                self.logger.info(f\"Initialized observer: {observer_name}\")\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to initialize observer '{observer_name}': {e}\"\n                )\n\n        # Create dispatcher and attach to context\n        if new_observers:\n            context.observer_dispatcher = ObserverDispatcher(new_observers)\n\n            # Emit pipeline start event\n            from ondine.observability.events import PipelineStartEvent\n\n            start_event = PipelineStartEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                config={},\n                metadata=self.specifications.metadata,\n                total_rows=0,  # Will be updated after data loading\n            )\n            context.observer_dispatcher.dispatch(\"pipeline_start\", start_event)\n\n    # Notify legacy observers of start\n    for observer in self.observers:\n        observer.on_pipeline_start(self, context)\n\n    try:\n        # Execute stages (preprocessing happens inside if enabled)\n        result_df = self._execute_stages(context, state_manager)\n\n        # Mark completion\n        context.end_time = datetime.now()\n\n        # Create execution result\n        # Extract token tracking from intermediate_data (populated by LLMInvocationStage)\n        token_tracking = context.intermediate_data.get(\"token_tracking\", {})\n        input_tokens = token_tracking.get(\"input_tokens\", 0)\n        output_tokens = token_tracking.get(\"output_tokens\", 0)\n\n        result = ExecutionResult(\n            data=result_df,\n            metrics=context.get_stats(),\n            costs=CostEstimate(\n                total_cost=context.accumulated_cost,\n                total_tokens=context.accumulated_tokens,\n                input_tokens=input_tokens,\n                output_tokens=output_tokens,\n                rows=context.total_rows,\n                confidence=\"actual\",\n            ),\n            execution_id=context.session_id,\n            start_time=context.start_time,\n            end_time=context.end_time,\n            success=True,\n        )\n\n        # Optional: Auto-retry failed rows\n        if self.specifications.processing.auto_retry_failed:\n            # Get preprocessed data from context (or loaded data if no preprocessing)\n            retry_source_df = context.intermediate_data.get(\"preprocessed_data\")\n            if retry_source_df is None:\n                retry_source_df = context.intermediate_data.get(\"loaded_data\")\n            result = self._auto_retry_failed_rows(result, retry_source_df)\n\n        # Cleanup checkpoints on success\n        state_manager.cleanup_checkpoints(context.session_id)\n\n        # Notify legacy observers of completion\n        for observer in self.observers:\n            observer.on_pipeline_complete(context, result)\n\n        # Emit pipeline end event for new observability system\n        if context.observer_dispatcher:\n            from ondine.observability.events import PipelineEndEvent\n\n            end_event = PipelineEndEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                success=True,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                total_duration_ms=(\n                    (context.end_time - context.start_time).total_seconds() * 1000\n                    if context.end_time\n                    else 0\n                ),\n                rows_processed=result.metrics.processed_rows,\n                rows_succeeded=result.metrics.processed_rows\n                - result.metrics.failed_rows,\n                rows_failed=result.metrics.failed_rows,\n                rows_skipped=result.metrics.skipped_rows,\n                total_cost=result.costs.total_cost,\n                total_tokens=result.costs.total_tokens,\n                input_tokens=result.costs.input_tokens,\n                output_tokens=result.costs.output_tokens,\n            )\n            context.observer_dispatcher.dispatch(\"pipeline_end\", end_event)\n\n            # Flush and close observers\n            context.observer_dispatcher.flush_all()\n            context.observer_dispatcher.close_all()\n\n        return result\n\n    except Exception as e:\n        # Save checkpoint on error\n        state_manager.save_checkpoint(context)\n        self.logger.error(\n            f\"Pipeline failed. Checkpoint saved. \"\n            f\"Resume with: pipeline.execute(resume_from=UUID('{context.session_id}'))\"\n        )\n\n        # Notify legacy observers of error\n        for observer in self.observers:\n            observer.on_pipeline_error(context, e)\n\n        # Emit error event for new observability system\n        if context.observer_dispatcher:\n            from ondine.observability.events import ErrorEvent\n\n            error_event = ErrorEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                error=e,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                stack_trace=\"\",  # Could add full traceback if needed\n            )\n            context.observer_dispatcher.dispatch(\"error\", error_event)\n\n            # Flush and close observers even on error\n            context.observer_dispatcher.flush_all()\n            context.observer_dispatcher.close_all()\n\n        raise\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline asynchronously.</p> <p>Uses AsyncExecutor for non-blocking execution. Ideal for integration with FastAPI, aiohttp, and other async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support async</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>async def execute_async(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline asynchronously.\n\n    Uses AsyncExecutor for non-blocking execution. Ideal for integration\n    with FastAPI, aiohttp, and other async frameworks.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        ValueError: If executor doesn't support async\n    \"\"\"\n    if not self.executor.supports_async():\n        raise ValueError(\n            \"Current executor doesn't support async. \"\n            \"Use AsyncExecutor: Pipeline(specs, executor=AsyncExecutor())\"\n        )\n\n    # For now, wrap synchronous execution in async\n    # TODO: Implement fully async execution pipeline\n    import asyncio\n\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, self.execute, resume_from)\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(chunk_size: int | None = None) -&gt; Iterator[ExecutionResult]\n</code></pre> <p>Execute pipeline in streaming mode.</p> <p>Processes data in chunks for memory-efficient handling of large datasets. Ideal for datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int | None</code> <p>Number of rows per chunk (uses executor's chunk_size if None)</p> <code>None</code> <p>Yields:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult objects for each processed chunk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support streaming</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute_stream(\n    self, chunk_size: int | None = None\n) -&gt; Iterator[ExecutionResult]:\n    \"\"\"\n    Execute pipeline in streaming mode.\n\n    Processes data in chunks for memory-efficient handling of large datasets.\n    Ideal for datasets that don't fit in memory.\n\n    Args:\n        chunk_size: Number of rows per chunk (uses executor's chunk_size if None)\n\n    Yields:\n        ExecutionResult objects for each processed chunk\n\n    Raises:\n        ValueError: If executor doesn't support streaming\n    \"\"\"\n    if not self.executor.supports_streaming():\n        raise ValueError(\n            \"Current executor doesn't support streaming. \"\n            \"Use StreamingExecutor: Pipeline(specs, executor=StreamingExecutor())\"\n        )\n\n    # Use executor's chunk_size if not provided\n    if chunk_size is None and isinstance(self.executor, StreamingExecutor):\n        chunk_size = self.executor.chunk_size\n    elif chunk_size is None:\n        chunk_size = 1000  # Default fallback\n\n    # For now, execute the full pipeline and split result into chunks\n    # TODO: Implement proper streaming execution that processes chunks independently\n    result = self.execute()\n\n    # Split the result data into chunks and yield as separate ExecutionResults\n    total_rows = len(result.data)\n    for start_idx in range(0, total_rows, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_data = result.data.iloc[start_idx:end_idx].copy()\n\n        # Create a chunk result with proportional metrics\n        chunk_rows = len(chunk_data)\n        chunk_result = ExecutionResult(\n            data=chunk_data,\n            metrics=ProcessingStats(\n                total_rows=chunk_rows,\n                processed_rows=chunk_rows,\n                failed_rows=0,\n                skipped_rows=0,\n                rows_per_second=result.metrics.rows_per_second,\n                total_duration_seconds=result.metrics.total_duration_seconds\n                * (chunk_rows / total_rows),\n                stage_durations=result.metrics.stage_durations,\n            ),\n            costs=CostEstimate(\n                total_cost=result.costs.total_cost\n                * Decimal(chunk_rows / total_rows),\n                total_tokens=int(\n                    result.costs.total_tokens * (chunk_rows / total_rows)\n                ),\n                input_tokens=int(\n                    result.costs.input_tokens * (chunk_rows / total_rows)\n                ),\n                output_tokens=int(\n                    result.costs.output_tokens * (chunk_rows / total_rows)\n                ),\n                rows=chunk_rows,\n                confidence=result.costs.confidence,\n            ),\n            execution_id=result.execution_id,\n            start_time=result.start_time,\n            end_time=result.end_time,\n            success=True,\n        )\n        yield chunk_result\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder","title":"PipelineBuilder","text":"<pre><code>PipelineBuilder()\n</code></pre> <p>Fluent builder for constructing pipelines.</p> <p>Provides an intuitive, chainable API for common use cases.</p> Example <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p> <p>Initialize builder with None values.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with None values.\"\"\"\n    self._dataset_spec: DatasetSpec | None = None\n    self._prompt_spec: PromptSpec | None = None\n    self._llm_spec: LLMSpec | None = None\n    self._processing_spec: ProcessingSpec = ProcessingSpec()\n    self._output_spec: OutputSpec | None = None\n    self._dataframe: pd.DataFrame | None = None\n    self._executor: ExecutionStrategy | None = None\n    self._custom_parser: any | None = None\n    self._custom_llm_client: any | None = None\n    self._custom_stages: list[dict] = []  # For custom stage injection\n    self._observers: list[tuple[str, dict]] = []  # For observability\n    self._custom_metadata: dict[str, Any] = {}  # For arbitrary metadata\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create() -&gt; PipelineBuilder\n</code></pre> <p>Start builder chain.</p> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>New PipelineBuilder instance</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef create() -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Start builder chain.\n\n    Returns:\n        New PipelineBuilder instance\n    \"\"\"\n    return PipelineBuilder()\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_specifications","title":"from_specifications  <code>staticmethod</code>","text":"<pre><code>from_specifications(specs: PipelineSpecifications) -&gt; PipelineBuilder\n</code></pre> <p>Create builder from existing specifications.</p> <p>Useful for loading from YAML and modifying programmatically.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>PipelineSpecifications</code> <p>Complete pipeline specifications</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>PipelineBuilder pre-configured with specs</p> Example <p>specs = load_pipeline_config(\"config.yaml\") builder = PipelineBuilder.from_specifications(specs) pipeline = builder.build()</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef from_specifications(specs: PipelineSpecifications) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Create builder from existing specifications.\n\n    Useful for loading from YAML and modifying programmatically.\n\n    Args:\n        specs: Complete pipeline specifications\n\n    Returns:\n        PipelineBuilder pre-configured with specs\n\n    Example:\n        specs = load_pipeline_config(\"config.yaml\")\n        builder = PipelineBuilder.from_specifications(specs)\n        pipeline = builder.build()\n    \"\"\"\n    builder = PipelineBuilder()\n    builder._dataset_spec = specs.dataset\n    builder._prompt_spec = specs.prompt\n    builder._llm_spec = specs.llm\n    builder._processing_spec = specs.processing\n    builder._output_spec = specs.output\n    return builder\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_csv","title":"from_csv","text":"<pre><code>from_csv(path: str, input_columns: list[str], output_columns: list[str], delimiter: str = ',', encoding: str = 'utf-8') -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names to use in prompts</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names to generate</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding (default: utf-8)</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>builder = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"description\"],\n        output_columns=[\"category\"]\n    )\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_csv(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV data source.\n\n    Args:\n        path: Path to CSV file\n        input_columns: Input column names to use in prompts\n        output_columns: Output column names to generate\n        delimiter: CSV delimiter (default: comma)\n        encoding: File encoding (default: utf-8)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        builder = (\n            PipelineBuilder.create()\n            .from_csv(\n                \"products.csv\",\n                input_columns=[\"description\"],\n                output_columns=[\"category\"]\n            )\n        )\n        ```\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.CSV,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        delimiter=delimiter,\n        encoding=encoding,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_excel","title":"from_excel","text":"<pre><code>from_excel(path: str, input_columns: list[str], output_columns: list[str], sheet_name: str | int = 0) -&gt; PipelineBuilder\n</code></pre> <p>Configure Excel data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Excel file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_excel(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    sheet_name: str | int = 0,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Excel data source.\n\n    Args:\n        path: Path to Excel file\n        input_columns: Input column names\n        output_columns: Output column names\n        sheet_name: Sheet name or index\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.EXCEL,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        sheet_name=sheet_name,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_parquet","title":"from_parquet","text":"<pre><code>from_parquet(path: str, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure Parquet data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_parquet(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Parquet data source.\n\n    Args:\n        path: Path to Parquet file\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.PARQUET,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_dataframe","title":"from_dataframe","text":"<pre><code>from_dataframe(df: DataFrame, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure DataFrame source.</p> <p>Useful for processing in-memory data or chaining pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame with input data</p> required <code>input_columns</code> <code>list[str]</code> <p>Column names to use in prompts</p> required <code>output_columns</code> <code>list[str]</code> <p>Column names to generate</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\"text\": [\"Hello\", \"World\"]})\nbuilder = (\n    PipelineBuilder.create()\n    .from_dataframe(df, input_columns=[\"text\"], output_columns=[\"result\"])\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_dataframe(\n    self,\n    df: pd.DataFrame,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure DataFrame source.\n\n    Useful for processing in-memory data or chaining pipelines.\n\n    Args:\n        df: Pandas DataFrame with input data\n        input_columns: Column names to use in prompts\n        output_columns: Column names to generate\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        import pandas as pd\n\n        df = pd.DataFrame({\"text\": [\"Hello\", \"World\"]})\n        builder = (\n            PipelineBuilder.create()\n            .from_dataframe(df, input_columns=[\"text\"], output_columns=[\"result\"])\n        )\n        ```\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.DATAFRAME,\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    self._dataframe = df\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_prompt","title":"with_prompt","text":"<pre><code>with_prompt(template: str, system_message: str | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Configure prompt template.</p> <p>Use {variable} syntax to reference input columns. The LLM will process each row using this template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {variable} placeholders matching input_columns</p> required <code>system_message</code> <code>str | None</code> <p>Optional system message for chat models</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>builder.with_prompt(\n    \"Categorize this product: {title}\\n\\nCategory:\",\n    system_message=\"You are a product categorization expert.\"\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_prompt(\n    self,\n    template: str,\n    system_message: str | None = None,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure prompt template.\n\n    Use {variable} syntax to reference input columns. The LLM will process\n    each row using this template.\n\n    Args:\n        template: Prompt template with {variable} placeholders matching input_columns\n        system_message: Optional system message for chat models\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        builder.with_prompt(\n            \"Categorize this product: {title}\\\\n\\\\nCategory:\",\n            system_message=\"You are a product categorization expert.\"\n        )\n        ```\n    \"\"\"\n    self._prompt_spec = PromptSpec(\n        template=template,\n        system_message=system_message,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_system_prompt","title":"with_system_prompt","text":"<pre><code>with_system_prompt(system_prompt: str) -&gt; PipelineBuilder\n</code></pre> <p>Set system prompt for caching optimization.</p> <p>System prompts are cached by providers (OpenAI, Anthropic) and reused across all rows, reducing costs by 50-90% for the cached portion.</p> <p>This method should be called after with_prompt() to set or update the system message separately from the user prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>Static instructions/context (will be cached)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Review: {text}\")  # Dynamic per row\n    .with_system_prompt('''\n        You are a sentiment classifier.\n        Classify reviews as: positive, negative, or neutral.\n        Return only the label, nothing else.\n    ''')  # Cached across all rows\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <p>Can also be set via with_prompt(template, system_message=...). This method provides a more explicit API for caching optimization.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_system_prompt(self, system_prompt: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set system prompt for caching optimization.\n\n    System prompts are cached by providers (OpenAI, Anthropic) and reused\n    across all rows, reducing costs by 50-90% for the cached portion.\n\n    This method should be called after with_prompt() to set or update the\n    system message separately from the user prompt template.\n\n    Args:\n        system_prompt: Static instructions/context (will be cached)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"])\n            .with_prompt(\"Review: {text}\")  # Dynamic per row\n            .with_system_prompt('''\n                You are a sentiment classifier.\n                Classify reviews as: positive, negative, or neutral.\n                Return only the label, nothing else.\n            ''')  # Cached across all rows\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        Can also be set via with_prompt(template, system_message=...).\n        This method provides a more explicit API for caching optimization.\n    \"\"\"\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_system_prompt()\")\n\n    self._prompt_spec.system_message = system_prompt\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_batch_size","title":"with_batch_size","text":"<pre><code>with_batch_size(batch_size: int) -&gt; PipelineBuilder\n</code></pre> <p>Set batch size for multi-row processing.</p> <p>Process multiple rows in a single API call to reduce costs and latency by up to 100\u00d7. Batch size of 1 (default) processes rows individually.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of rows to process per API call (1-500)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt; 1 or prompt not set</p> Example <pre><code># Process 100 rows per API call (100\u00d7 fewer calls)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Classify: {text}\")\n    .with_batch_size(100)  # 5M rows = 50K API calls!\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <ul> <li>Batch size is limited by model context window</li> <li>Larger batches = fewer API calls but higher risk of partial failures</li> <li>Recommended: Start with 10-50, increase based on results</li> </ul> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_size(self, batch_size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set batch size for multi-row processing.\n\n    Process multiple rows in a single API call to reduce costs and latency\n    by up to 100\u00d7. Batch size of 1 (default) processes rows individually.\n\n    Args:\n        batch_size: Number of rows to process per API call (1-500)\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If batch_size &lt; 1 or prompt not set\n\n    Example:\n        ```python\n        # Process 100 rows per API call (100\u00d7 fewer calls)\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"])\n            .with_prompt(\"Classify: {text}\")\n            .with_batch_size(100)  # 5M rows = 50K API calls!\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        - Batch size is limited by model context window\n        - Larger batches = fewer API calls but higher risk of partial failures\n        - Recommended: Start with 10-50, increase based on results\n    \"\"\"\n    if batch_size &lt; 1:\n        raise ValueError(f\"batch_size must be &gt;= 1, got {batch_size}\")\n\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_batch_size()\")\n\n    # Update batch_size directly (consistent with with_system_prompt)\n    self._prompt_spec.batch_size = batch_size\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_batch_strategy","title":"with_batch_strategy","text":"<pre><code>with_batch_strategy(strategy: str) -&gt; PipelineBuilder\n</code></pre> <p>Set batch formatting strategy.</p> <p>Choose how multiple rows are formatted in a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Strategy name (\"json\" or \"csv\")</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy is not supported or prompt not set</p> Example <pre><code># Use JSON array format (default, most reliable)\npipeline.with_batch_size(100).with_batch_strategy(\"json\")\n\n# Use CSV format (more compact, experimental)\npipeline.with_batch_size(100).with_batch_strategy(\"csv\")\n</code></pre> Supported Strategies <ul> <li>\"json\": JSON array format (default, most reliable)</li> <li>\"csv\": CSV format (more compact, experimental)</li> </ul> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_strategy(self, strategy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set batch formatting strategy.\n\n    Choose how multiple rows are formatted in a single prompt.\n\n    Args:\n        strategy: Strategy name (\"json\" or \"csv\")\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If strategy is not supported or prompt not set\n\n    Example:\n        ```python\n        # Use JSON array format (default, most reliable)\n        pipeline.with_batch_size(100).with_batch_strategy(\"json\")\n\n        # Use CSV format (more compact, experimental)\n        pipeline.with_batch_size(100).with_batch_strategy(\"csv\")\n        ```\n\n    Supported Strategies:\n        - \"json\": JSON array format (default, most reliable)\n        - \"csv\": CSV format (more compact, experimental)\n    \"\"\"\n    allowed = [\"json\", \"csv\"]\n    if strategy not in allowed:\n        raise ValueError(\n            f\"batch_strategy must be one of {allowed}, got '{strategy}'\"\n        )\n\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_batch_strategy()\")\n\n    # Update batch_strategy directly (consistent with with_system_prompt)\n    self._prompt_spec.batch_strategy = strategy\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_jinja2","title":"with_jinja2","text":"<pre><code>with_jinja2(enabled: bool = True) -&gt; PipelineBuilder\n</code></pre> <p>Enable Jinja2 template rendering for advanced prompt control.</p> <p>Jinja2 provides powerful features for dynamic prompts: - Conditionals: {% if condition %}...{% endif %} - Loops: {% for item in items %}...{% endfor %} - Filters: {{ text | upper | truncate(100) }} - Complex logic within templates</p> <p>By default, Ondine uses Python's .format() for simple {variable} substitution. Enable Jinja2 when you need programmatic control flow.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Enable (True) or disable (False) Jinja2 rendering</p> <code>True</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Conditional prompt based on data\ntemplate = '''Extract from: {{ description }}\n{% if category == \"beverage\" %}\nFocus on volume units (oz, ml, L)\n{% elif category == \"food\" %}\nFocus on weight units (lb, oz, g)\n{% endif %}'''\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"products.csv\", input_columns=[\"description\", \"category\"])\n    .with_prompt(template)\n    .with_jinja2(True)  # Enable Jinja2\n    .with_llm(\"openai\", \"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <p>Simple {variable} syntax works with both modes. Only enable Jinja2 if you need conditionals, loops, or filters.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_jinja2(self, enabled: bool = True) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Enable Jinja2 template rendering for advanced prompt control.\n\n    Jinja2 provides powerful features for dynamic prompts:\n    - Conditionals: {% if condition %}...{% endif %}\n    - Loops: {% for item in items %}...{% endfor %}\n    - Filters: {{ text | upper | truncate(100) }}\n    - Complex logic within templates\n\n    By default, Ondine uses Python's .format() for simple {variable}\n    substitution. Enable Jinja2 when you need programmatic control flow.\n\n    Args:\n        enabled: Enable (True) or disable (False) Jinja2 rendering\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Conditional prompt based on data\n        template = '''Extract from: {{ description }}\n        {% if category == \"beverage\" %}\n        Focus on volume units (oz, ml, L)\n        {% elif category == \"food\" %}\n        Focus on weight units (lb, oz, g)\n        {% endif %}'''\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"products.csv\", input_columns=[\"description\", \"category\"])\n            .with_prompt(template)\n            .with_jinja2(True)  # Enable Jinja2\n            .with_llm(\"openai\", \"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        Simple {variable} syntax works with both modes. Only enable\n        Jinja2 if you need conditionals, loops, or filters.\n    \"\"\"\n    self._processing_spec.use_jinja2 = enabled\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm","title":"with_llm","text":"<pre><code>with_llm(provider: str, model: str, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, **kwargs: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM provider.</p> <p>Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom providers. API keys can be provided explicitly or via environment variables.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (openai, azure_openai, anthropic, groq, mlx) or custom provider ID</p> required <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\")</p> required <code>api_key</code> <code>str | None</code> <p>API key (optional, reads from environment if not provided)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-1.0, default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum output tokens (optional, uses model default)</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Provider-specific parameters (e.g., azure_endpoint, azure_deployment)</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># OpenAI\nbuilder.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n# Groq (fast and affordable)\nbuilder.with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n\n# Azure OpenAI with Managed Identity\nbuilder.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    use_managed_identity=True\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm(\n    self,\n    provider: str,\n    model: str,\n    api_key: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    **kwargs: any,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM provider.\n\n    Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom providers.\n    API keys can be provided explicitly or via environment variables.\n\n    Args:\n        provider: Provider name (openai, azure_openai, anthropic, groq, mlx) or custom provider ID\n        model: Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\")\n        api_key: API key (optional, reads from environment if not provided)\n        temperature: Sampling temperature (0.0-1.0, default: 0.0 for deterministic)\n        max_tokens: Maximum output tokens (optional, uses model default)\n        **kwargs: Provider-specific parameters (e.g., azure_endpoint, azure_deployment)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # OpenAI\n        builder.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n        # Groq (fast and affordable)\n        builder.with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n\n        # Azure OpenAI with Managed Identity\n        builder.with_llm(\n            provider=\"azure_openai\",\n            model=\"gpt-4\",\n            azure_endpoint=\"https://your-resource.openai.azure.com/\",\n            azure_deployment=\"gpt-4-deployment\",\n            use_managed_identity=True\n        )\n        ```\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Try to convert to enum for built-in providers\n    try:\n        provider_enum = LLMProvider(provider.lower())\n    except ValueError:\n        # Not a built-in provider - check if it's a custom provider\n        if ProviderRegistry.is_registered(provider):\n            # Use a dummy enum value for validation, but store the actual provider string\n            provider_enum = LLMProvider.OPENAI  # Dummy for Pydantic validation\n            kwargs[\"_custom_provider_id\"] = provider\n        else:\n            raise ValueError(\n                f\"Unknown provider: {provider}. \"\n                f\"Available providers: {', '.join(ProviderRegistry.list_providers())}\"\n            )\n\n    self._llm_spec = LLMSpec(\n        provider=provider_enum,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec","title":"with_llm_spec","text":"<pre><code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM using a pre-built LLMSpec object.</p> <p>This method allows using LLMSpec objects directly, enabling: - Reusable provider configurations - Use of LLMProviderPresets for common providers - Custom LLMSpec instances for advanced use cases</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification object</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If spec is not an LLMSpec instance</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm_spec(self, spec: LLMSpec) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM using a pre-built LLMSpec object.\n\n    This method allows using LLMSpec objects directly, enabling:\n    - Reusable provider configurations\n    - Use of LLMProviderPresets for common providers\n    - Custom LLMSpec instances for advanced use cases\n\n    Args:\n        spec: LLM specification object\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        TypeError: If spec is not an LLMSpec instance\n\n    Example:\n        # Use preset\n        from ondine.core.specifications import LLMProviderPresets\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Process: {text}\")\n            .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n            .build()\n        )\n\n        # Custom spec\n        custom = LLMSpec(\n            provider=LLMProvider.OPENAI,\n            model=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n        pipeline.with_llm_spec(custom)\n\n        # Override preset\n        spec = LLMProviderPresets.GPT4O_MINI.model_copy(\n            update={\"temperature\": 0.9}\n        )\n        pipeline.with_llm_spec(spec)\n    \"\"\"\n    if not isinstance(spec, LLMSpec):\n        raise TypeError(\n            f\"Expected LLMSpec, got {type(spec).__name__}. \"\n            f\"Use with_llm() for parameter-based configuration.\"\n        )\n\n    self._llm_spec = spec\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec--use-preset","title":"Use preset","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec--custom-spec","title":"Custom spec","text":"<p>custom = LLMSpec(     provider=LLMProvider.OPENAI,     model=\"gpt-4o-mini\",     temperature=0.7 ) pipeline.with_llm_spec(custom)</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec--override-preset","title":"Override preset","text":"<p>spec = LLMProviderPresets.GPT4O_MINI.model_copy(     update={\"temperature\": 0.9} ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_custom_llm_client","title":"with_custom_llm_client","text":"<pre><code>with_custom_llm_client(client: any) -&gt; PipelineBuilder\n</code></pre> <p>Provide a custom LLM client instance directly.</p> <p>This allows advanced users to create their own LLM client implementations by extending the LLMClient base class. The custom client will be used instead of the factory-created client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>any</code> <p>Custom LLM client instance (must inherit from LLMClient)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <p>class MyCustomClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         # Custom implementation         ...</p> <p>pipeline = (     PipelineBuilder.create()     .from_dataframe(df, ...)     .with_prompt(\"...\")     .with_custom_llm_client(MyCustomClient(spec))     .build() )</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_custom_llm_client(self, client: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Provide a custom LLM client instance directly.\n\n    This allows advanced users to create their own LLM client implementations\n    by extending the LLMClient base class. The custom client will be used\n    instead of the factory-created client.\n\n    Args:\n        client: Custom LLM client instance (must inherit from LLMClient)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        class MyCustomClient(LLMClient):\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                # Custom implementation\n                ...\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_dataframe(df, ...)\n            .with_prompt(\"...\")\n            .with_custom_llm_client(MyCustomClient(spec))\n            .build()\n        )\n    \"\"\"\n    from ondine.adapters.llm_client import LLMClient\n\n    if not isinstance(client, LLMClient):\n        raise TypeError(\n            f\"Custom client must inherit from LLMClient, got {type(client).__name__}\"\n        )\n\n    self._custom_llm_client = client\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_processing_batch_size","title":"with_processing_batch_size","text":"<pre><code>with_processing_batch_size(size: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure internal batch size for PromptFormatterStage.</p> <p>This is different from with_batch_size() which enables multi-row batching. This method controls how many prompts are grouped together internally for processing efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Rows per internal batch</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Note <p>This is an internal optimization parameter. Most users should use with_batch_size() for multi-row batching instead.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_processing_batch_size(self, size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure internal batch size for PromptFormatterStage.\n\n    This is different from with_batch_size() which enables multi-row batching.\n    This method controls how many prompts are grouped together internally\n    for processing efficiency.\n\n    Args:\n        size: Rows per internal batch\n\n    Returns:\n        Self for chaining\n\n    Note:\n        This is an internal optimization parameter. Most users should use\n        with_batch_size() for multi-row batching instead.\n    \"\"\"\n    self._processing_spec.batch_size = size\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_concurrency","title":"with_concurrency","text":"<pre><code>with_concurrency(threads: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure concurrent requests.</p> <p>Higher concurrency = faster processing but more API load. Adjust based on your provider's rate limits.</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int</code> <p>Number of concurrent threads (1-100, typical: 5-20)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Conservative (free tier)\nbuilder.with_concurrency(5)\n\n# Aggressive (paid tier)\nbuilder.with_concurrency(20)\n\n# Maximum (enterprise)\nbuilder.with_concurrency(50)\n</code></pre> Note <p>Groq supports high concurrency (~100), while OpenAI free tier is limited to ~5.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_concurrency(self, threads: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure concurrent requests.\n\n    Higher concurrency = faster processing but more API load. Adjust based on\n    your provider's rate limits.\n\n    Args:\n        threads: Number of concurrent threads (1-100, typical: 5-20)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Conservative (free tier)\n        builder.with_concurrency(5)\n\n        # Aggressive (paid tier)\n        builder.with_concurrency(20)\n\n        # Maximum (enterprise)\n        builder.with_concurrency(50)\n        ```\n\n    Note:\n        Groq supports high concurrency (~100), while OpenAI free tier is limited to ~5.\n    \"\"\"\n    self._processing_spec.concurrency = threads\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_checkpoint_interval","title":"with_checkpoint_interval","text":"<pre><code>with_checkpoint_interval(rows: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint frequency.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Rows between checkpoints</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_interval(self, rows: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint frequency.\n\n    Args:\n        rows: Rows between checkpoints\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_interval = rows\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_rate_limit","title":"with_rate_limit","text":"<pre><code>with_rate_limit(rpm: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure rate limiting.</p> <p>Prevents hitting API rate limits by throttling requests using token bucket algorithm. Set this below your provider's actual limit for safety.</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute (typical: 20-60 for free tiers, 100+ for paid)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># OpenAI free tier (60 RPM limit)\nbuilder.with_rate_limit(50)\n\n# Groq free tier (30 RPM limit)\nbuilder.with_rate_limit(25)\n\n# Paid tier with high limits\nbuilder.with_rate_limit(100)\n</code></pre> Note <p>Rate limiting is applied per pipeline execution, not globally.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_rate_limit(self, rpm: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure rate limiting.\n\n    Prevents hitting API rate limits by throttling requests using token bucket algorithm.\n    Set this below your provider's actual limit for safety.\n\n    Args:\n        rpm: Requests per minute (typical: 20-60 for free tiers, 100+ for paid)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # OpenAI free tier (60 RPM limit)\n        builder.with_rate_limit(50)\n\n        # Groq free tier (30 RPM limit)\n        builder.with_rate_limit(25)\n\n        # Paid tier with high limits\n        builder.with_rate_limit(100)\n        ```\n\n    Note:\n        Rate limiting is applied per pipeline execution, not globally.\n    \"\"\"\n    self._processing_spec.rate_limit_rpm = rpm\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_max_retries","title":"with_max_retries","text":"<pre><code>with_max_retries(retries: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>Maximum number of retry attempts</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_retries(self, retries: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum retry attempts.\n\n    Args:\n        retries: Maximum number of retry attempts\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_retries = retries\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_max_budget","title":"with_max_budget","text":"<pre><code>with_max_budget(budget: float) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum budget.</p> <p>Pipeline will halt execution if cost exceeds this limit. Warnings are shown at 75% and 90% of budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget</code> <code>float</code> <p>Maximum budget in USD (e.g., 5.0 for $5)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Set $5 budget limit\nbuilder.with_max_budget(5.0)\n\n# For testing with small budget\nbuilder.with_max_budget(0.50)\n\n# For production runs\nbuilder.with_max_budget(100.0)\n</code></pre> Note <p>Budget enforcement uses Decimal precision to avoid floating-point errors.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_budget(self, budget: float) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum budget.\n\n    Pipeline will halt execution if cost exceeds this limit. Warnings are shown\n    at 75% and 90% of budget.\n\n    Args:\n        budget: Maximum budget in USD (e.g., 5.0 for $5)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Set $5 budget limit\n        builder.with_max_budget(5.0)\n\n        # For testing with small budget\n        builder.with_max_budget(0.50)\n\n        # For production runs\n        builder.with_max_budget(100.0)\n        ```\n\n    Note:\n        Budget enforcement uses Decimal precision to avoid floating-point errors.\n    \"\"\"\n    self._processing_spec.max_budget = Decimal(str(budget))\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_error_policy","title":"with_error_policy","text":"<pre><code>with_error_policy(policy: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure error handling policy.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>Error policy ('skip', 'fail', 'retry', 'use_default')</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_error_policy(self, policy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure error handling policy.\n\n    Args:\n        policy: Error policy ('skip', 'fail', 'retry', 'use_default')\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.core.specifications import ErrorPolicy\n\n    self._processing_spec.error_policy = ErrorPolicy(policy.lower())\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_checkpoint_dir","title":"with_checkpoint_dir","text":"<pre><code>with_checkpoint_dir(directory: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to checkpoint directory</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_dir(self, directory: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint directory.\n\n    Args:\n        directory: Path to checkpoint directory\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_dir = Path(directory)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_parser","title":"with_parser","text":"<pre><code>with_parser(parser: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure response parser.</p> <p>This method allows setting a custom parser. The parser type determines the response_format in the prompt spec.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>any</code> <p>Parser instance (JSONParser, RegexParser, PydanticParser, etc.)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_parser(self, parser: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure response parser.\n\n    This method allows setting a custom parser. The parser type\n    determines the response_format in the prompt spec.\n\n    Args:\n        parser: Parser instance (JSONParser, RegexParser, PydanticParser, etc.)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    # Store the parser for later use in the pipeline\n    # We'll configure response_format based on parser type\n    if hasattr(parser, \"__class__\"):\n        parser_name = parser.__class__.__name__\n        if \"JSON\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            # Update the existing prompt spec's response_format\n            self._prompt_spec.response_format = \"json\"\n        elif \"Regex\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            self._prompt_spec.response_format = \"regex\"\n            if hasattr(parser, \"patterns\"):\n                self._prompt_spec.regex_patterns = parser.patterns\n\n    # Store the parser instance in metadata for the pipeline to use\n    if not hasattr(self, \"_custom_parser\"):\n        self._custom_parser = parser\n\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.to_csv","title":"to_csv","text":"<pre><code>to_csv(path: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV output destination.</p> <p>Alias for with_output(path, format='csv').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output CSV file path</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def to_csv(self, path: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV output destination.\n\n    Alias for with_output(path, format='csv').\n\n    Args:\n        path: Output CSV file path\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    return self.with_output(path, format=\"csv\")\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_output","title":"with_output","text":"<pre><code>with_output(path: str, format: str = 'csv', merge_strategy: str = 'replace') -&gt; PipelineBuilder\n</code></pre> <p>Configure output destination.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path</p> required <code>format</code> <code>str</code> <p>Output format (csv, excel, parquet)</p> <code>'csv'</code> <code>merge_strategy</code> <code>str</code> <p>Merge strategy (replace, append, update)</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_output(\n    self,\n    path: str,\n    format: str = \"csv\",\n    merge_strategy: str = \"replace\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure output destination.\n\n    Args:\n        path: Output file path\n        format: Output format (csv, excel, parquet)\n        merge_strategy: Merge strategy (replace, append, update)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    format_map = {\n        \"csv\": DataSourceType.CSV,\n        \"excel\": DataSourceType.EXCEL,\n        \"parquet\": DataSourceType.PARQUET,\n    }\n\n    merge_map = {\n        \"replace\": MergeStrategy.REPLACE,\n        \"append\": MergeStrategy.APPEND,\n        \"update\": MergeStrategy.UPDATE,\n    }\n\n    self._output_spec = OutputSpec(\n        destination_type=format_map[format.lower()],\n        destination_path=Path(path),\n        merge_strategy=merge_map[merge_strategy.lower()],\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_executor","title":"with_executor","text":"<pre><code>with_executor(executor: ExecutionStrategy) -&gt; PipelineBuilder\n</code></pre> <p>Set custom execution strategy.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ExecutionStrategy</code> <p>ExecutionStrategy instance</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_executor(self, executor: ExecutionStrategy) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set custom execution strategy.\n\n    Args:\n        executor: ExecutionStrategy instance\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = executor\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_async_execution","title":"with_async_execution","text":"<pre><code>with_async_execution(max_concurrency: int = 10) -&gt; PipelineBuilder\n</code></pre> <p>Use async execution strategy.</p> <p>Enables async/await for non-blocking execution. Ideal for FastAPI, aiohttp, and async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_async_execution(self, max_concurrency: int = 10) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use async execution strategy.\n\n    Enables async/await for non-blocking execution.\n    Ideal for FastAPI, aiohttp, and async frameworks.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = AsyncExecutor(max_concurrency=max_concurrency)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_streaming","title":"with_streaming","text":"<pre><code>with_streaming(chunk_size: int = 1000) -&gt; PipelineBuilder\n</code></pre> <p>Use streaming execution strategy.</p> <p>Processes data in chunks for memory-efficient handling. Ideal for large datasets (100K+ rows).</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_streaming(self, chunk_size: int = 1000) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use streaming execution strategy.\n\n    Processes data in chunks for memory-efficient handling.\n    Ideal for large datasets (100K+ rows).\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = StreamingExecutor(chunk_size=chunk_size)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_progress_mode","title":"with_progress_mode","text":"<pre><code>with_progress_mode(mode: str = 'auto') -&gt; PipelineBuilder\n</code></pre> <p>Configure progress tracking mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Progress tracking mode - \"auto\": Auto-detect (rich if TTY, else logging) [default] - \"rich\": Beautiful progress bars with ETA - \"logging\": Simple log messages - \"none\": Disable progress tracking</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Use rich progress (beautiful UI)\nbuilder.with_progress_mode(\"rich\")\n\n# Disable progress (faster, cleaner logs)\nbuilder.with_progress_mode(\"none\")\n\n# Auto-detect (recommended)\nbuilder.with_progress_mode(\"auto\")\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_progress_mode(self, mode: str = \"auto\") -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure progress tracking mode.\n\n    Args:\n        mode: Progress tracking mode\n            - \"auto\": Auto-detect (rich if TTY, else logging) [default]\n            - \"rich\": Beautiful progress bars with ETA\n            - \"logging\": Simple log messages\n            - \"none\": Disable progress tracking\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Use rich progress (beautiful UI)\n        builder.with_progress_mode(\"rich\")\n\n        # Disable progress (faster, cleaner logs)\n        builder.with_progress_mode(\"none\")\n\n        # Auto-detect (recommended)\n        builder.with_progress_mode(\"auto\")\n        ```\n    \"\"\"\n    self._processing_spec.progress_mode = mode\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_stage","title":"with_stage","text":"<pre><code>with_stage(stage_name: str, position: str = 'before_prompt', **stage_kwargs) -&gt; PipelineBuilder\n</code></pre> <p>Add a custom pipeline stage by name.</p> <p>Enables injection of custom processing stages at specific points in the pipeline. Stages must be registered via StageRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Registered stage name (e.g., \"rag_retrieval\")</p> required <code>position</code> <code>str</code> <p>Where to inject the stage. Options: - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing - \"after_parser\": After response parsing</p> <code>'before_prompt'</code> <code>**stage_kwargs</code> <p>Arguments to pass to stage constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name not registered or position invalid</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_stage(\n    self,\n    stage_name: str,\n    position: str = \"before_prompt\",\n    **stage_kwargs,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add a custom pipeline stage by name.\n\n    Enables injection of custom processing stages at specific points\n    in the pipeline. Stages must be registered via StageRegistry.\n\n    Args:\n        stage_name: Registered stage name (e.g., \"rag_retrieval\")\n        position: Where to inject the stage. Options:\n            - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting\n            - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation\n            - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing\n            - \"after_parser\": After response parsing\n        **stage_kwargs: Arguments to pass to stage constructor\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If stage_name not registered or position invalid\n\n    Example:\n        # RAG retrieval example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])\n            .with_stage(\n                \"rag_retrieval\",\n                position=\"before_prompt\",\n                vector_store=\"pinecone\",\n                index_name=\"my-docs\",\n                top_k=5\n            )\n            .with_prompt(\"Context: {retrieved_context}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o\")\n            .build()\n        )\n\n        # Content moderation example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])\n            .with_stage(\n                \"content_moderation\",\n                position=\"before_llm\",\n                block_patterns=[\"spam\", \"offensive\"]\n            )\n            .with_prompt(\"Moderate: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n    \"\"\"\n    from ondine.stages.stage_registry import StageRegistry\n\n    # Validate position\n    valid_positions = [\n        \"after_loader\",\n        \"before_prompt\",\n        \"after_prompt\",\n        \"before_llm\",\n        \"after_llm\",\n        \"before_parser\",\n        \"after_parser\",\n    ]\n    if position not in valid_positions:\n        raise ValueError(\n            f\"Invalid position '{position}'. Must be one of: {', '.join(valid_positions)}\"\n        )\n\n    # Get stage class from registry (this will raise ValueError if not found)\n    stage_class = StageRegistry.get(stage_name)\n\n    # Store stage config for later instantiation\n    self._custom_stages.append(\n        {\n            \"name\": stage_name,\n            \"class\": stage_class,\n            \"position\": position,\n            \"kwargs\": stage_kwargs,\n        }\n    )\n\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_stage--rag-retrieval-example","title":"RAG retrieval example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])     .with_stage(         \"rag_retrieval\",         position=\"before_prompt\",         vector_store=\"pinecone\",         index_name=\"my-docs\",         top_k=5     )     .with_prompt(\"Context: {retrieved_context}\\n\\nQuestion: {question}\\n\\nAnswer:\")     .with_llm(provider=\"openai\", model=\"gpt-4o\")     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_stage--content-moderation-example","title":"Content moderation example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])     .with_stage(         \"content_moderation\",         position=\"before_llm\",         block_patterns=[\"spam\", \"offensive\"]     )     .with_prompt(\"Moderate: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_observer","title":"with_observer","text":"<pre><code>with_observer(name: str, config: dict[str, any] | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Add observability observer to the pipeline.</p> <p>Observers receive events during pipeline execution for monitoring, logging, and tracing.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier (e.g., \"langfuse\", \"opentelemetry\", \"logging\")</p> required <code>config</code> <code>dict[str, any] | None</code> <p>Observer-specific configuration dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If observer not registered</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_observer(\n    self, name: str, config: dict[str, any] | None = None\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add observability observer to the pipeline.\n\n    Observers receive events during pipeline execution for monitoring,\n    logging, and tracing.\n\n    Args:\n        name: Observer identifier (e.g., \"langfuse\", \"opentelemetry\", \"logging\")\n        config: Observer-specific configuration dictionary\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If observer not registered\n\n    Example:\n        # OpenTelemetry for infrastructure monitoring\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"opentelemetry\", config={\n                \"tracer_name\": \"my_pipeline\",\n                \"include_prompts\": False\n            })\n            .build()\n        )\n\n        # Langfuse for LLM-specific observability\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"langfuse\", config={\n                \"public_key\": \"pk-lf-...\",\n                \"secret_key\": \"sk-lf-...\"  # pragma: allowlist secret\n            })\n            .build()\n        )\n\n        # Multiple observers\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"langfuse\", config={...})\n            .with_observer(\"opentelemetry\", config={...})\n            .with_observer(\"logging\", config={\"log_level\": \"DEBUG\"})\n            .build()\n        )\n    \"\"\"\n    from ondine.observability.registry import ObserverRegistry\n\n    # Validate observer is registered\n    if not ObserverRegistry.is_registered(name):\n        available = \", \".join(ObserverRegistry.list_observers())\n        raise ValueError(\n            f\"Observer '{name}' not registered. \"\n            f\"Available observers: {available or 'none'}\"\n        )\n\n    # Store observer config for later instantiation\n    self._observers.append((name, config or {}))\n\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_observer--opentelemetry-for-infrastructure-monitoring","title":"OpenTelemetry for infrastructure monitoring","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"opentelemetry\", config={         \"tracer_name\": \"my_pipeline\",         \"include_prompts\": False     })     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_observer--langfuse-for-llm-specific-observability","title":"Langfuse for LLM-specific observability","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"langfuse\", config={         \"public_key\": \"pk-lf-...\",         \"secret_key\": \"sk-lf-...\"  # pragma: allowlist secret     })     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_observer--multiple-observers","title":"Multiple observers","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"langfuse\", config={...})     .with_observer(\"opentelemetry\", config={...})     .with_observer(\"logging\", config={\"log_level\": \"DEBUG\"})     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_structured_output","title":"with_structured_output","text":"<pre><code>with_structured_output(schema: Any) -&gt; PipelineBuilder\n</code></pre> <p>Configure structured output using a Pydantic model.</p> <p>This enables native schema enforcement, parsing, and auto-retry logic using LlamaIndex's structured_predict capabilities.</p> <p>Automatically configures JSONParser to handle the structured JSON output, unless a custom parser was already configured.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Any</code> <p>Pydantic model class defining the expected output structure</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_structured_output(self, schema: Any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure structured output using a Pydantic model.\n\n    This enables native schema enforcement, parsing, and auto-retry logic\n    using LlamaIndex's structured_predict capabilities.\n\n    Automatically configures JSONParser to handle the structured JSON output,\n    unless a custom parser was already configured.\n\n    Args:\n        schema: Pydantic model class defining the expected output structure\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    if not hasattr(self, \"_custom_metadata\"):\n        self._custom_metadata = {}\n    self._custom_metadata[\"structured_output_model\"] = schema\n\n    # Auto-inject JSONParser if no parser configured\n    # Structured output always returns JSON, so we need a JSON parser\n    if self._custom_parser is None:\n        from ondine.stages.response_parser_stage import JSONParser\n\n        self._custom_parser = JSONParser()\n\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.build","title":"build","text":"<pre><code>build() -&gt; Pipeline\n</code></pre> <p>Build final Pipeline.</p> <p>Validates all configurations and constructs the Pipeline object ready for execution. This is the final step in the builder chain.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required specifications missing (dataset, prompt, or LLM)</p> Example <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_concurrency(10)\n    .with_max_budget(5.0)\n    .build()  # Returns Pipeline object\n)\n\n# Execute the pipeline\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.total_rows} rows\")\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def build(self) -&gt; Pipeline:\n    \"\"\"\n    Build final Pipeline.\n\n    Validates all configurations and constructs the Pipeline object ready for execution.\n    This is the final step in the builder chain.\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If required specifications missing (dataset, prompt, or LLM)\n\n    Example:\n        ```python\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Summarize: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_concurrency(10)\n            .with_max_budget(5.0)\n            .build()  # Returns Pipeline object\n        )\n\n        # Execute the pipeline\n        result = pipeline.execute()\n        print(f\"Processed {result.metrics.total_rows} rows\")\n        ```\n    \"\"\"\n    # Validate required specs\n    if not self._dataset_spec:\n        raise ValueError(\"Dataset specification required\")\n    if not self._prompt_spec:\n        raise ValueError(\"Prompt specification required\")\n\n    # LLM spec is optional if custom client is provided\n    if not self._llm_spec and not self._custom_llm_client:\n        raise ValueError(\"Either LLM specification or custom LLM client required\")\n\n    # Prepare metadata with custom parser, custom client, custom stages, and observers\n    metadata = {}\n    if self._custom_metadata:\n        metadata.update(self._custom_metadata)\n    if self._custom_parser is not None:\n        metadata[\"custom_parser\"] = self._custom_parser\n    if self._custom_llm_client is not None:\n        metadata[\"custom_llm_client\"] = self._custom_llm_client\n    if self._custom_stages:\n        metadata[\"custom_stages\"] = self._custom_stages\n    if self._observers:\n        metadata[\"observers\"] = self._observers\n\n    # Create specifications bundle\n    # If custom client provided but no llm_spec, create a dummy spec\n    llm_spec = self._llm_spec\n    if llm_spec is None and self._custom_llm_client is not None:\n        # Create minimal spec using custom client's attributes\n        llm_spec = LLMSpec(\n            provider=LLMProvider.OPENAI,  # Dummy provider\n            model=self._custom_llm_client.model,\n            temperature=self._custom_llm_client.temperature,\n            max_tokens=self._custom_llm_client.max_tokens,\n        )\n\n    specifications = PipelineSpecifications(\n        dataset=self._dataset_spec,\n        prompt=self._prompt_spec,\n        llm=llm_spec,\n        processing=self._processing_spec,\n        output=self._output_spec,\n        metadata=metadata,\n    )\n\n    # Create and return pipeline\n    return Pipeline(\n        specifications,\n        dataframe=self._dataframe,\n        executor=self._executor,\n    )\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer","title":"PipelineComposer","text":"<pre><code>PipelineComposer(input_data: str | Path | DataFrame)\n</code></pre> <p>Composes multiple pipelines to process independent columns.</p> <p>Each pipeline processes one output column. Pipelines can depend on outputs from previous pipelines, enabling sequential processing.</p> <p>Design Philosophy: - Keep individual pipelines simple (single responsibility) - Compose complex workflows from simple building blocks - Make dependencies explicit (no hidden coupling) - Fail fast with clear error messages</p> <p>Initialize pipeline composer.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str | Path | DataFrame</code> <p>Either a file path (CSV/Excel) or DataFrame</p> required Design Note <p>We accept both paths and DataFrames to support different workflows: - Path: Lazy loading, memory efficient - DataFrame: Already loaded, faster iteration</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def __init__(self, input_data: str | Path | pd.DataFrame):\n    \"\"\"\n    Initialize pipeline composer.\n\n    Args:\n        input_data: Either a file path (CSV/Excel) or DataFrame\n\n    Design Note:\n        We accept both paths and DataFrames to support different workflows:\n        - Path: Lazy loading, memory efficient\n        - DataFrame: Already loaded, faster iteration\n    \"\"\"\n    if isinstance(input_data, str | Path):\n        self.input_path = str(input_data)\n        self.input_df = None  # Lazy load\n    elif isinstance(input_data, pd.DataFrame):\n        self.input_path = None\n        self.input_df = input_data.copy()\n    else:\n        raise TypeError(\n            f\"input_data must be str, Path, or DataFrame, got {type(input_data)}\"\n        )\n\n    # Storage for column pipelines\n    # Format: (column_name, pipeline, dependencies)\n    self.column_pipelines: list[tuple[str, Pipeline, list[str]]] = []\n\n    logger.info(\"PipelineComposer initialized\")\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer.add_column","title":"add_column","text":"<pre><code>add_column(column_name: str, pipeline: Pipeline, depends_on: list[str] | None = None) -&gt; PipelineComposer\n</code></pre> <p>Add a pipeline for processing one output column.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of output column</p> required <code>pipeline</code> <code>Pipeline</code> <p>Pipeline to generate this column</p> required <code>depends_on</code> <code>list[str] | None</code> <p>List of columns this depends on (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Self for method chaining (fluent API)</p> Example <p>composer.add_column(\"score\", score_pipeline) composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])</p> Design Note <p>Fluent API (returns self) enables readable chaining: composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def add_column(\n    self,\n    column_name: str,\n    pipeline: Pipeline,\n    depends_on: list[str] | None = None,\n) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Add a pipeline for processing one output column.\n\n    Args:\n        column_name: Name of output column\n        pipeline: Pipeline to generate this column\n        depends_on: List of columns this depends on (optional)\n\n    Returns:\n        Self for method chaining (fluent API)\n\n    Example:\n        composer.add_column(\"score\", score_pipeline)\n        composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])\n\n    Design Note:\n        Fluent API (returns self) enables readable chaining:\n        composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()\n    \"\"\"\n    dependencies = depends_on or []\n\n    # Validate column name unique\n    existing_cols = [col for col, _, _ in self.column_pipelines]\n    if column_name in existing_cols:\n        raise ValueError(f\"Column '{column_name}' already added\")\n\n    self.column_pipelines.append((column_name, pipeline, dependencies))\n\n    logger.info(\n        f\"Added column '{column_name}' with dependencies: {dependencies or 'none'}\"\n    )\n\n    return self  # Enable chaining\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer.execute","title":"execute","text":"<pre><code>execute() -&gt; ExecutionResult\n</code></pre> <p>Execute all column pipelines in dependency order.</p> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with all columns merged</p> Algorithm <ol> <li>Load input data (lazy if needed)</li> <li>Resolve execution order (topological sort)</li> <li>For each column:    a. Inject current DataFrame into pipeline    b. Execute pipeline    c. Merge result column into DataFrame</li> <li>Aggregate metrics and return final result</li> </ol> Design Note <p>Each pipeline operates on the accumulating DataFrame, so later pipelines can use earlier outputs as inputs.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def execute(self) -&gt; ExecutionResult:\n    \"\"\"\n    Execute all column pipelines in dependency order.\n\n    Returns:\n        ExecutionResult with all columns merged\n\n    Algorithm:\n        1. Load input data (lazy if needed)\n        2. Resolve execution order (topological sort)\n        3. For each column:\n           a. Inject current DataFrame into pipeline\n           b. Execute pipeline\n           c. Merge result column into DataFrame\n        4. Aggregate metrics and return final result\n\n    Design Note:\n        Each pipeline operates on the accumulating DataFrame,\n        so later pipelines can use earlier outputs as inputs.\n    \"\"\"\n    # Lazy load if needed\n    if self.input_df is None:\n        logger.info(f\"Loading input data from {self.input_path}\")\n        # Use DataReader adapter for consistent file loading\n        from ondine.adapters.data_io import create_data_reader\n        from ondine.core.specifications import DataSourceType\n\n        # Detect source type from file extension\n        if self.input_path.endswith(\".xlsx\") or self.input_path.endswith(\".xls\"):\n            source_type = DataSourceType.EXCEL\n        elif self.input_path.endswith(\".csv\"):\n            source_type = DataSourceType.CSV\n        elif self.input_path.endswith(\".parquet\"):\n            source_type = DataSourceType.PARQUET\n        else:\n            raise ValueError(f\"Unsupported file type: {self.input_path}\")\n\n        # Create reader and load data\n        reader = create_data_reader(\n            source_type=source_type, source_path=self.input_path\n        )\n        self.input_df = reader.read()\n\n    # Start with input data\n    df = self.input_df.copy()\n\n    # Resolve execution order\n    execution_order = self._get_execution_order()\n\n    logger.info(\n        f\"Executing {len(execution_order)} column pipelines in order: \"\n        f\"{[col for col, _, _ in execution_order]}\"\n    )\n\n    # Track metrics (keep Decimal precision for costs)\n    from decimal import Decimal\n\n    total_cost = Decimal(\"0.0\")\n    total_errors = []\n\n    # Execute each column pipeline\n    for col_name, pipeline, _deps in execution_order:\n        logger.info(f\"Processing column '{col_name}'...\")\n\n        # Inject current DataFrame (includes previous outputs)\n        pipeline.dataframe = df\n\n        # Execute pipeline\n        result = pipeline.execute()\n\n        # Merge new column\n        if col_name in result.data.columns:\n            df[col_name] = result.data[col_name]\n        else:\n            logger.warning(\n                f\"Pipeline for '{col_name}' didn't produce expected column\"\n            )\n\n        # Accumulate metrics (preserve Decimal precision)\n        cost_to_add = result.costs.total_cost\n        if not isinstance(cost_to_add, Decimal):\n            cost_to_add = Decimal(str(cost_to_add))\n        total_cost += cost_to_add\n        total_errors.extend(result.errors)\n\n        logger.info(\n            f\"Column '{col_name}' complete: \"\n            f\"{len(df)} rows, ${result.costs.total_cost:.4f}\"\n        )\n\n    # Create final result\n    final_result = ExecutionResult(\n        data=df,\n        metrics=ProcessingStats(\n            total_rows=len(df),\n            processed_rows=len(df),\n            failed_rows=len(total_errors),\n            skipped_rows=0,\n            rows_per_second=0.0,  # Aggregate metric not meaningful\n            total_duration_seconds=0.0,  # Would need timing\n        ),\n        costs=CostEstimate(\n            total_cost=total_cost,\n            total_tokens=0,  # Aggregate from individual pipelines\n            input_tokens=0,\n            output_tokens=0,\n            rows=len(df),\n        ),\n        errors=total_errors,\n    )\n\n    logger.info(\n        f\"Composition complete: {len(execution_order)} columns, \"\n        f\"${total_cost:.4f} total cost\"\n    )\n\n    return final_result\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(config_path: str) -&gt; PipelineComposer\n</code></pre> <p>Load composer configuration from YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to composition config file</p> required <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Configured PipelineComposer</p> Example YAML <p>composition:   input: \"data.xlsx\"   pipelines:     - column: col1       config: pipeline1.yaml     - column: col2       depends_on: [col1]       config: pipeline2.yaml</p> Design Note <p>This enables pure YAML workflows without Python code.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>@classmethod\ndef from_yaml(cls, config_path: str) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Load composer configuration from YAML.\n\n    Args:\n        config_path: Path to composition config file\n\n    Returns:\n        Configured PipelineComposer\n\n    Example YAML:\n        composition:\n          input: \"data.xlsx\"\n          pipelines:\n            - column: col1\n              config: pipeline1.yaml\n            - column: col2\n              depends_on: [col1]\n              config: pipeline2.yaml\n\n    Design Note:\n        This enables pure YAML workflows without Python code.\n    \"\"\"\n    import yaml\n\n    from ondine.config import ConfigLoader\n\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n\n    composition = config.get(\"composition\", {})\n    input_data = composition.get(\"input\")\n\n    if not input_data:\n        raise ValueError(\"composition.input is required\")\n\n    composer = cls(input_data=input_data)\n\n    # Load each pipeline config\n    for pipeline_config in composition.get(\"pipelines\", []):\n        col_name = pipeline_config[\"column\"]\n        config_file = pipeline_config[\"config\"]\n        depends_on = pipeline_config.get(\"depends_on\", [])\n\n        # Load pipeline from its config\n        pipeline_specs = ConfigLoader.from_yaml(config_file)\n        pipeline = Pipeline(pipeline_specs)\n\n        composer.add_column(col_name, pipeline, depends_on)\n\n    return composer\n</code></pre>"},{"location":"api/api/#ondine.api.QuickPipeline","title":"QuickPipeline","text":"<p>Simplified pipeline API with smart defaults.</p> <p>Designed for rapid prototyping and common use cases. Automatically detects: - Input columns from prompt template placeholders - Provider from model name (e.g., gpt-4 \u2192 openai, claude \u2192 anthropic) - Parser type (JSON for multi-column, text for single column) - Reasonable defaults for batch size, concurrency, retries</p> <p>Examples:</p> <p>Minimal usage:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"data.csv\",\n...     prompt=\"Categorize this text: {text}\"\n... )\n&gt;&gt;&gt; result = pipeline.execute()\n</code></pre> <p>With explicit outputs:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"products.csv\",\n...     prompt=\"Extract: {description}\",\n...     output_columns=[\"brand\", \"model\", \"price\"]\n... )\n</code></pre> <p>Override defaults:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=df,\n...     prompt=\"Summarize: {content}\",\n...     model=\"gpt-4o\",\n...     temperature=0.7,\n...     max_budget=Decimal(\"5.0\")\n... )\n</code></pre>"},{"location":"api/api/#ondine.api.QuickPipeline.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(data: str | Path | DataFrame, prompt: str, model: str = 'gpt-4o-mini', output_columns: list[str] | str | None = None, provider: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, max_budget: Decimal | float | str | None = None, batch_size: int | None = None, concurrency: int | None = None, **kwargs: Any) -&gt; Pipeline\n</code></pre> <p>Create a pipeline with smart defaults.</p> <p>The simplest way to process data with LLMs. Automatically detects input columns from prompt placeholders and configures optimal settings based on data size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path | DataFrame</code> <p>CSV/Excel/Parquet file path or DataFrame</p> required <code>prompt</code> <code>str</code> <p>Prompt template with {placeholders} matching column names</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o-mini). Provider auto-detected from model name.</p> <code>'gpt-4o-mini'</code> <code>output_columns</code> <code>list[str] | str | None</code> <p>Output column name(s). If None, uses [\"output\"]</p> <code>None</code> <code>provider</code> <code>str | None</code> <p>LLM provider. If None, auto-detected (gpt-4 \u2192 openai, claude \u2192 anthropic)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-1.0, default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens (optional, uses provider default)</p> <code>None</code> <code>max_budget</code> <code>Decimal | float | str | None</code> <p>Maximum cost budget in USD (optional, no limit if not set)</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Rows per batch (optional, auto-sized: 10-500 based on data size)</p> <code>None</code> <code>concurrency</code> <code>int | None</code> <p>Parallel requests (optional, auto-sized: 5-100 based on provider)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to PipelineBuilder</p> <code>{}</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data cannot be loaded or prompt is invalid</p> Example <pre><code>from ondine import QuickPipeline\n\n# Minimal - auto-detects everything\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Categorize: {description}\"\n)\nresult = pipeline.execute()\n\n# With budget control\npipeline = QuickPipeline.create(\n    data=\"reviews.csv\",\n    prompt=\"Sentiment: {review_text}\",\n    model=\"gpt-4o-mini\",\n    max_budget=5.0\n)\n\n# Multi-column output\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Extract from {title}: brand, price, category as JSON\",\n    output_columns=[\"brand\", \"price\", \"category\"]\n)\n\n# Custom provider\npipeline = QuickPipeline.create(\n    data=df,\n    prompt=\"Summarize: {text}\",\n    model=\"llama-3.3-70b-versatile\",\n    provider=\"groq\"\n)\n</code></pre> Note <p>Input columns are automatically detected from {placeholders} in the prompt. Provider is auto-detected from model name (gpt-4 \u2192 openai, claude \u2192 anthropic, llama \u2192 groq).</p> Source code in <code>ondine/api/quick.py</code> <pre><code>@staticmethod\ndef create(\n    data: str | Path | pd.DataFrame,\n    prompt: str,\n    model: str = \"gpt-4o-mini\",\n    output_columns: list[str] | str | None = None,\n    provider: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    max_budget: Decimal | float | str | None = None,\n    batch_size: int | None = None,\n    concurrency: int | None = None,\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"\n    Create a pipeline with smart defaults.\n\n    The simplest way to process data with LLMs. Automatically detects input columns\n    from prompt placeholders and configures optimal settings based on data size.\n\n    Args:\n        data: CSV/Excel/Parquet file path or DataFrame\n        prompt: Prompt template with {placeholders} matching column names\n        model: Model name (default: gpt-4o-mini). Provider auto-detected from model name.\n        output_columns: Output column name(s). If None, uses [\"output\"]\n        provider: LLM provider. If None, auto-detected (gpt-4 \u2192 openai, claude \u2192 anthropic)\n        temperature: Sampling temperature (0.0-1.0, default: 0.0 for deterministic)\n        max_tokens: Max output tokens (optional, uses provider default)\n        max_budget: Maximum cost budget in USD (optional, no limit if not set)\n        batch_size: Rows per batch (optional, auto-sized: 10-500 based on data size)\n        concurrency: Parallel requests (optional, auto-sized: 5-100 based on provider)\n        **kwargs: Additional arguments passed to PipelineBuilder\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If input data cannot be loaded or prompt is invalid\n\n    Example:\n        ```python\n        from ondine import QuickPipeline\n\n        # Minimal - auto-detects everything\n        pipeline = QuickPipeline.create(\n            data=\"products.csv\",\n            prompt=\"Categorize: {description}\"\n        )\n        result = pipeline.execute()\n\n        # With budget control\n        pipeline = QuickPipeline.create(\n            data=\"reviews.csv\",\n            prompt=\"Sentiment: {review_text}\",\n            model=\"gpt-4o-mini\",\n            max_budget=5.0\n        )\n\n        # Multi-column output\n        pipeline = QuickPipeline.create(\n            data=\"products.csv\",\n            prompt=\"Extract from {title}: brand, price, category as JSON\",\n            output_columns=[\"brand\", \"price\", \"category\"]\n        )\n\n        # Custom provider\n        pipeline = QuickPipeline.create(\n            data=df,\n            prompt=\"Summarize: {text}\",\n            model=\"llama-3.3-70b-versatile\",\n            provider=\"groq\"\n        )\n        ```\n\n    Note:\n        Input columns are automatically detected from {placeholders} in the prompt.\n        Provider is auto-detected from model name (gpt-4 \u2192 openai, claude \u2192 anthropic, llama \u2192 groq).\n    \"\"\"\n    # 1. Load data\n    df = QuickPipeline._load_data(data)\n\n    # 2. Auto-detect input columns from prompt template\n    input_columns = QuickPipeline._extract_placeholders(prompt)\n    if not input_columns:\n        raise ValueError(\n            f\"No placeholders found in prompt: {prompt}\\n\"\n            \"Expected format: 'Your prompt with {{column_name}} placeholders'\"\n        )\n\n    # Validate input columns exist in data\n    missing = [col for col in input_columns if col not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Input columns {missing} not found in data. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    # 3. Normalize output columns\n    if output_columns is None:\n        output_columns = [\"output\"]\n    elif isinstance(output_columns, str):\n        output_columns = [output_columns]\n\n    # 4. Auto-detect provider from model name\n    if provider is None:\n        provider = QuickPipeline._detect_provider(model)\n\n    # 5. Auto-select parser (JSON for multi-column, text for single)\n    parser = QuickPipeline._select_parser(output_columns)\n\n    # 6. Smart defaults for batch_size and concurrency\n    if batch_size is None:\n        batch_size = QuickPipeline._default_batch_size(len(df))\n    if concurrency is None:\n        concurrency = QuickPipeline._default_concurrency(provider)\n\n    # 7. Build pipeline\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df, input_columns=input_columns, output_columns=output_columns\n        )\n        .with_prompt(template=prompt)\n        .with_llm(\n            provider=provider,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n    )\n\n    # Add optional parser if multi-column\n    if parser:\n        builder = builder.with_parser(parser)\n\n    # Add batch/concurrency settings\n    # Note: QuickPipeline uses processing_batch_size (internal batching)\n    # not with_batch_size (multi-row batching)\n    builder = builder.with_processing_batch_size(batch_size).with_concurrency(\n        concurrency\n    )\n\n    # Add budget if specified\n    if max_budget is not None:\n        # Convert to float for PipelineBuilder (it expects float)\n        if isinstance(max_budget, Decimal | str):\n            max_budget = float(max_budget)\n        builder = builder.with_max_budget(budget=max_budget)\n\n    # Add sensible retry defaults\n    builder = builder.with_max_retries(3)\n\n    return builder.build()\n</code></pre>"},{"location":"api/api/dataset_processor/","title":"dataset_processor","text":""},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor","title":"dataset_processor","text":"<p>DatasetProcessor - Simplified convenience wrapper.</p> <p>For users who just want to process data with minimal configuration.</p>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor","title":"DatasetProcessor","text":"<pre><code>DatasetProcessor(data: str | DataFrame, input_column: str, output_column: str, prompt: str, llm_config: dict[str, any])\n</code></pre> <p>Simplified API for single-prompt, single-column use cases.</p> <p>This is a convenience wrapper around PipelineBuilder for users who don't need fine-grained control.</p> Example <p>processor = DatasetProcessor(     data=\"data.csv\",     input_column=\"description\",     output_column=\"cleaned\",     prompt=\"Clean this text: {description}\",     llm_config={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\"} ) result = processor.run()</p> <p>Initialize dataset processor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>CSV file path or DataFrame</p> required <code>input_column</code> <code>str</code> <p>Input column name</p> required <code>output_column</code> <code>str</code> <p>Output column name</p> required <code>prompt</code> <code>str</code> <p>Prompt template</p> required <code>llm_config</code> <code>dict[str, any]</code> <p>LLM configuration dict</p> required Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    input_column: str,\n    output_column: str,\n    prompt: str,\n    llm_config: dict[str, any],\n):\n    \"\"\"\n    Initialize dataset processor.\n\n    Args:\n        data: CSV file path or DataFrame\n        input_column: Input column name\n        output_column: Output column name\n        prompt: Prompt template\n        llm_config: LLM configuration dict\n    \"\"\"\n    self.data = data\n    self.input_column = input_column\n    self.output_column = output_column\n    self.prompt = prompt\n    self.llm_config = llm_config\n\n    # Build pipeline internally\n    builder = PipelineBuilder.create()\n\n    # Configure data source\n    if isinstance(data, str):\n        builder.from_csv(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    elif isinstance(data, pd.DataFrame):\n        builder.from_dataframe(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    else:\n        raise ValueError(\"data must be file path or DataFrame\")\n\n    # Configure prompt\n    builder.with_prompt(prompt)\n\n    # Configure LLM\n    provider = llm_config.get(\"provider\", \"openai\")\n    model = llm_config.get(\"model\", \"gpt-4o-mini\")\n    api_key = llm_config.get(\"api_key\")\n    temperature = llm_config.get(\"temperature\", 0.0)\n    max_tokens = llm_config.get(\"max_tokens\")\n\n    builder.with_llm(\n        provider=provider,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    # Build pipeline\n    self.pipeline = builder.build()\n</code></pre>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor.run","title":"run","text":"<pre><code>run() -&gt; pd.DataFrame\n</code></pre> <p>Execute processing and return results.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute processing and return results.\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    result = self.pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor.run_sample","title":"run_sample","text":"<pre><code>run_sample(n: int = 10) -&gt; pd.DataFrame\n</code></pre> <p>Test on first N rows.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of rows to process</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with sample results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run_sample(self, n: int = 10) -&gt; pd.DataFrame:\n    \"\"\"\n    Test on first N rows.\n\n    Args:\n        n: Number of rows to process\n\n    Returns:\n        DataFrame with sample results\n    \"\"\"\n    # Create sample pipeline\n    if isinstance(self.data, str):\n        df = pd.read_csv(self.data).head(n)\n    else:\n        df = self.data.head(n)\n\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df,\n            input_columns=[self.input_column],\n            output_columns=[self.output_column],\n        )\n        .with_prompt(self.prompt)\n        .with_llm(\n            provider=self.llm_config.get(\"provider\", \"openai\"),\n            model=self.llm_config.get(\"model\", \"gpt-4o-mini\"),\n            api_key=self.llm_config.get(\"api_key\"),\n            temperature=self.llm_config.get(\"temperature\", 0.0),\n        )\n    )\n\n    sample_pipeline = builder.build()\n    result = sample_pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; float\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost in USD</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def estimate_cost(self) -&gt; float:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Estimated cost in USD\n    \"\"\"\n    estimate = self.pipeline.estimate_cost()\n    return float(estimate.total_cost)\n</code></pre>"},{"location":"api/api/health_check/","title":"health_check","text":""},{"location":"api/api/health_check/#ondine.api.health_check","title":"health_check","text":"<p>Health check API for pipeline monitoring.</p> <p>Provides status information for operational monitoring.</p>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck","title":"HealthCheck","text":"<pre><code>HealthCheck(pipeline: Pipeline)\n</code></pre> <p>Health check API for monitoring pipeline status.</p> <p>Provides information about pipeline health and readiness.</p> <p>Initialize health check.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>Pipeline instance to monitor</p> required Source code in <code>ondine/api/health_check.py</code> <pre><code>def __init__(self, pipeline: Pipeline):\n    \"\"\"\n    Initialize health check.\n\n    Args:\n        pipeline: Pipeline instance to monitor\n    \"\"\"\n    self.pipeline = pipeline\n    self.last_check: datetime | None = None\n    self.last_status: dict[str, Any] = {}\n</code></pre>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck.check","title":"check","text":"<pre><code>check() -&gt; dict[str, Any]\n</code></pre> <p>Perform health check.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Health status dictionary</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def check(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Perform health check.\n\n    Returns:\n        Health status dictionary\n    \"\"\"\n    self.last_check = datetime.now()\n\n    status = {\n        \"status\": \"healthy\",\n        \"timestamp\": self.last_check.isoformat(),\n        \"pipeline_id\": str(self.pipeline.id),\n        \"checks\": {},\n    }\n\n    # Check LLM provider configuration\n    try:\n        llm_spec = self.pipeline.specifications.llm\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"ok\",\n            \"provider\": llm_spec.provider.value,\n            \"model\": llm_spec.model,\n        }\n    except Exception as e:\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check data source configuration\n    try:\n        dataset_spec = self.pipeline.specifications.dataset\n        source_exists = True\n\n        if dataset_spec.source_path:\n            source_exists = dataset_spec.source_path.exists()\n\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"ok\" if source_exists else \"warning\",\n            \"source_type\": dataset_spec.source_type.value,\n            \"exists\": source_exists,\n        }\n    except Exception as e:\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check checkpoint storage\n    try:\n        checkpoint_dir = self.pipeline.specifications.processing.checkpoint_dir\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"ok\",\n            \"directory\": str(checkpoint_dir),\n            \"exists\": checkpoint_dir.exists(),\n        }\n    except Exception as e:\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"warning\",\n            \"error\": str(e),\n        }\n\n    # Store last status\n    self.last_status = status\n\n    return status\n</code></pre>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck.is_healthy","title":"is_healthy","text":"<pre><code>is_healthy() -&gt; bool\n</code></pre> <p>Check if pipeline is healthy.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if healthy</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def is_healthy(self) -&gt; bool:\n    \"\"\"\n    Check if pipeline is healthy.\n\n    Returns:\n        True if healthy\n    \"\"\"\n    status = self.check()\n    return status[\"status\"] == \"healthy\"\n</code></pre>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck.get_readiness","title":"get_readiness","text":"<pre><code>get_readiness() -&gt; dict[str, Any]\n</code></pre> <p>Get readiness status.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Readiness information</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def get_readiness(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get readiness status.\n\n    Returns:\n        Readiness information\n    \"\"\"\n    validation = self.pipeline.validate()\n\n    return {\n        \"ready\": validation.is_valid,\n        \"errors\": validation.errors,\n        \"warnings\": validation.warnings,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n</code></pre>"},{"location":"api/api/pipeline/","title":"pipeline","text":""},{"location":"api/api/pipeline/#ondine.api.pipeline","title":"pipeline","text":"<p>Main Pipeline class - the Facade for the entire system.</p> <p>This is the primary entry point that users interact with.</p>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(specifications: PipelineSpecifications, dataframe: DataFrame | None = None, executor: ExecutionStrategy | None = None)\n</code></pre> <p>Main pipeline class - Facade for dataset processing.</p> <p>Provides high-level interface for building and executing LLM-powered data transformations. Handles orchestration, state management, cost tracking, checkpointing, and error handling.</p> <p>This is typically created via PipelineBuilder or QuickPipeline, not directly.</p> Example <pre><code>from ondine import PipelineBuilder\n\n# Create via builder (recommended)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Execute\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.total_rows} rows\")\nprint(f\"Cost: ${result.costs.total_cost}\")\n</code></pre> Note <p>Use PipelineBuilder for construction, not direct instantiation.</p> <p>Initialize pipeline with specifications.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Complete pipeline configuration</p> required <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded DataFrame</p> <code>None</code> <code>executor</code> <code>ExecutionStrategy | None</code> <p>Optional execution strategy (default: SyncExecutor)</p> <code>None</code> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def __init__(\n    self,\n    specifications: PipelineSpecifications,\n    dataframe: pd.DataFrame | None = None,\n    executor: ExecutionStrategy | None = None,\n):\n    \"\"\"\n    Initialize pipeline with specifications.\n\n    Args:\n        specifications: Complete pipeline configuration\n        dataframe: Optional pre-loaded DataFrame\n        executor: Optional execution strategy (default: SyncExecutor)\n    \"\"\"\n    self.id = uuid4()\n    self.specifications = specifications\n    self.dataframe = dataframe\n    self.executor = executor or SyncExecutor()\n    self.observers: list[ExecutionObserver] = []\n    self.logger = get_logger(f\"{__name__}.{self.id}\")\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; Pipeline\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"Pipeline\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.validate","title":"validate","text":"<pre><code>validate() -&gt; ValidationResult\n</code></pre> <p>Validate pipeline configuration.</p> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with any errors/warnings</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def validate(self) -&gt; ValidationResult:\n    \"\"\"\n    Validate pipeline configuration.\n\n    Returns:\n        ValidationResult with any errors/warnings\n    \"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Validate dataset spec\n    if not self.specifications.dataset.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    if not self.specifications.dataset.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    # Validate that input columns exist in dataframe (if dataframe is provided)\n    if self.dataframe is not None and self.specifications.dataset.input_columns:\n        df_cols = set(self.dataframe.columns)\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_cols = input_cols - df_cols\n        if missing_cols:\n            result.add_error(\n                f\"Input columns not found in dataframe: {missing_cols}\"\n            )\n\n    # Validate prompt spec\n    if not self.specifications.prompt.template:\n        result.add_error(\"No prompt template specified\")\n    else:\n        # Check that template variables match input columns\n        import re\n\n        template_vars = set(\n            re.findall(r\"\\{(\\w+)\\}\", self.specifications.prompt.template)\n        )\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_vars = template_vars - input_cols\n        if missing_vars:\n            result.add_error(\n                f\"Template variables not in input columns: {missing_vars}\"\n            )\n\n    # Validate LLM spec\n    if not self.specifications.llm.model:\n        result.add_error(\"No LLM model specified\")\n\n    return result\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; CostEstimate\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def estimate_cost(self) -&gt; CostEstimate:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    # Create stages\n    loader = DataLoaderStage(self.dataframe)\n\n    # Load first few rows for estimation\n    df = loader.process(self.specifications.dataset, ExecutionContext())\n    sample_size = min(10, len(df))\n    sample_df = df.head(sample_size)\n\n    # Create formatter and get prompts\n    formatter = PromptFormatterStage(\n        self.specifications.processing.batch_size,\n        use_jinja2=self.specifications.processing.use_jinja2,\n    )\n    batches = formatter.process(\n        (sample_df, self.specifications.prompt), ExecutionContext()\n    )\n\n    # Create LLM client and estimate\n    llm_client = create_llm_client(self.specifications.llm)\n    llm_stage = LLMInvocationStage(llm_client)\n\n    sample_estimate = llm_stage.estimate_cost(batches)\n\n    # Scale to full dataset\n    scale_factor = Decimal(len(df)) / Decimal(sample_size)\n\n    return CostEstimate(\n        total_cost=sample_estimate.total_cost * scale_factor,\n        total_tokens=int(sample_estimate.total_tokens * float(scale_factor)),\n        input_tokens=int(sample_estimate.input_tokens * float(scale_factor)),\n        output_tokens=int(sample_estimate.output_tokens * float(scale_factor)),\n        rows=len(df),\n        confidence=\"sample-based\",\n    )\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.execute","title":"execute","text":"<pre><code>execute(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Runs all stages: data loading, prompt formatting, LLM invocation, response parsing, and result writing. Handles checkpointing, cost tracking, and error recovery.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint (for fault tolerance)</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult containing: - data: DataFrame with results - metrics: Processing statistics (total_rows, success_count, etc.) - costs: Cost breakdown (total_cost, input_tokens, output_tokens) - duration: Execution time in seconds - errors: List of any errors encountered</p> Example <pre><code># Execute pipeline\nresult = pipeline.execute()\n\n# Access results\nprint(f\"Processed: {result.metrics.total_rows} rows\")\nprint(f\"Successful: {result.metrics.success_count} rows\")\nprint(f\"Cost: ${result.costs.total_cost}\")\nprint(f\"Time: {result.duration:.2f}s\")\n\n# Access output data\nresult.data.to_csv(\"output.csv\", index=False)\n\n# Resume from checkpoint (if pipeline was interrupted)\nresult = pipeline.execute(resume_from=previous_session_id)\n</code></pre> Note <p>Progress is automatically saved via checkpoints. If execution fails, use resume_from to continue from the last checkpoint.</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Runs all stages: data loading, prompt formatting, LLM invocation, response parsing,\n    and result writing. Handles checkpointing, cost tracking, and error recovery.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint (for fault tolerance)\n\n    Returns:\n        ExecutionResult containing:\n            - data: DataFrame with results\n            - metrics: Processing statistics (total_rows, success_count, etc.)\n            - costs: Cost breakdown (total_cost, input_tokens, output_tokens)\n            - duration: Execution time in seconds\n            - errors: List of any errors encountered\n\n    Example:\n        ```python\n        # Execute pipeline\n        result = pipeline.execute()\n\n        # Access results\n        print(f\"Processed: {result.metrics.total_rows} rows\")\n        print(f\"Successful: {result.metrics.success_count} rows\")\n        print(f\"Cost: ${result.costs.total_cost}\")\n        print(f\"Time: {result.duration:.2f}s\")\n\n        # Access output data\n        result.data.to_csv(\"output.csv\", index=False)\n\n        # Resume from checkpoint (if pipeline was interrupted)\n        result = pipeline.execute(resume_from=previous_session_id)\n        ```\n\n    Note:\n        Progress is automatically saved via checkpoints. If execution fails,\n        use resume_from to continue from the last checkpoint.\n    \"\"\"\n    # Validate first\n    validation = self.validate()\n    if not validation.is_valid:\n        raise ValueError(f\"Pipeline validation failed: {validation.errors}\")\n\n    # Create or restore execution context\n    state_manager = StateManager(\n        storage=LocalFileCheckpointStorage(\n            self.specifications.processing.checkpoint_dir\n        ),\n        checkpoint_interval=self.specifications.processing.checkpoint_interval,\n    )\n\n    if resume_from:\n        # Resume from checkpoint\n        context = state_manager.load_checkpoint(resume_from)\n        if not context:\n            raise ValueError(f\"No checkpoint found for session {resume_from}\")\n        self.logger.info(\n            f\"Resuming from checkpoint at row {context.last_processed_row}\"\n        )\n    else:\n        # Create new context\n        context = ExecutionContext(pipeline_id=self.id)\n\n    # Add default observers if none specified\n    if not self.observers:\n        self.observers = [\n            ProgressBarObserver(),\n            LoggingObserver(),\n            CostTrackingObserver(),\n        ]\n\n    # Attach observers to context for progress notifications\n    context.observers = self.observers\n\n    # Initialize new observability system if observers configured\n    observer_configs = self.specifications.metadata.get(\"observers\", [])\n    if observer_configs:\n        from ondine.observability.dispatcher import ObserverDispatcher\n        from ondine.observability.registry import ObserverRegistry\n\n        # Instantiate observers from configuration\n        new_observers = []\n        for observer_name, observer_config in observer_configs:\n            try:\n                observer_class = ObserverRegistry.get(observer_name)\n                observer_instance = observer_class(config=observer_config)\n                new_observers.append(observer_instance)\n                self.logger.info(f\"Initialized observer: {observer_name}\")\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to initialize observer '{observer_name}': {e}\"\n                )\n\n        # Create dispatcher and attach to context\n        if new_observers:\n            context.observer_dispatcher = ObserverDispatcher(new_observers)\n\n            # Emit pipeline start event\n            from ondine.observability.events import PipelineStartEvent\n\n            start_event = PipelineStartEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                config={},\n                metadata=self.specifications.metadata,\n                total_rows=0,  # Will be updated after data loading\n            )\n            context.observer_dispatcher.dispatch(\"pipeline_start\", start_event)\n\n    # Notify legacy observers of start\n    for observer in self.observers:\n        observer.on_pipeline_start(self, context)\n\n    try:\n        # Execute stages (preprocessing happens inside if enabled)\n        result_df = self._execute_stages(context, state_manager)\n\n        # Mark completion\n        context.end_time = datetime.now()\n\n        # Create execution result\n        # Extract token tracking from intermediate_data (populated by LLMInvocationStage)\n        token_tracking = context.intermediate_data.get(\"token_tracking\", {})\n        input_tokens = token_tracking.get(\"input_tokens\", 0)\n        output_tokens = token_tracking.get(\"output_tokens\", 0)\n\n        result = ExecutionResult(\n            data=result_df,\n            metrics=context.get_stats(),\n            costs=CostEstimate(\n                total_cost=context.accumulated_cost,\n                total_tokens=context.accumulated_tokens,\n                input_tokens=input_tokens,\n                output_tokens=output_tokens,\n                rows=context.total_rows,\n                confidence=\"actual\",\n            ),\n            execution_id=context.session_id,\n            start_time=context.start_time,\n            end_time=context.end_time,\n            success=True,\n        )\n\n        # Optional: Auto-retry failed rows\n        if self.specifications.processing.auto_retry_failed:\n            # Get preprocessed data from context (or loaded data if no preprocessing)\n            retry_source_df = context.intermediate_data.get(\"preprocessed_data\")\n            if retry_source_df is None:\n                retry_source_df = context.intermediate_data.get(\"loaded_data\")\n            result = self._auto_retry_failed_rows(result, retry_source_df)\n\n        # Cleanup checkpoints on success\n        state_manager.cleanup_checkpoints(context.session_id)\n\n        # Notify legacy observers of completion\n        for observer in self.observers:\n            observer.on_pipeline_complete(context, result)\n\n        # Emit pipeline end event for new observability system\n        if context.observer_dispatcher:\n            from ondine.observability.events import PipelineEndEvent\n\n            end_event = PipelineEndEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                success=True,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                total_duration_ms=(\n                    (context.end_time - context.start_time).total_seconds() * 1000\n                    if context.end_time\n                    else 0\n                ),\n                rows_processed=result.metrics.processed_rows,\n                rows_succeeded=result.metrics.processed_rows\n                - result.metrics.failed_rows,\n                rows_failed=result.metrics.failed_rows,\n                rows_skipped=result.metrics.skipped_rows,\n                total_cost=result.costs.total_cost,\n                total_tokens=result.costs.total_tokens,\n                input_tokens=result.costs.input_tokens,\n                output_tokens=result.costs.output_tokens,\n            )\n            context.observer_dispatcher.dispatch(\"pipeline_end\", end_event)\n\n            # Flush and close observers\n            context.observer_dispatcher.flush_all()\n            context.observer_dispatcher.close_all()\n\n        return result\n\n    except Exception as e:\n        # Save checkpoint on error\n        state_manager.save_checkpoint(context)\n        self.logger.error(\n            f\"Pipeline failed. Checkpoint saved. \"\n            f\"Resume with: pipeline.execute(resume_from=UUID('{context.session_id}'))\"\n        )\n\n        # Notify legacy observers of error\n        for observer in self.observers:\n            observer.on_pipeline_error(context, e)\n\n        # Emit error event for new observability system\n        if context.observer_dispatcher:\n            from ondine.observability.events import ErrorEvent\n\n            error_event = ErrorEvent(\n                pipeline_id=self.id,\n                run_id=context.session_id,\n                timestamp=datetime.now(),\n                trace_id=context.trace_id,\n                span_id=context.span_id,\n                error=e,\n                error_type=type(e).__name__,\n                error_message=str(e),\n                stack_trace=\"\",  # Could add full traceback if needed\n            )\n            context.observer_dispatcher.dispatch(\"error\", error_event)\n\n            # Flush and close observers even on error\n            context.observer_dispatcher.flush_all()\n            context.observer_dispatcher.close_all()\n\n        raise\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline asynchronously.</p> <p>Uses AsyncExecutor for non-blocking execution. Ideal for integration with FastAPI, aiohttp, and other async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support async</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>async def execute_async(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline asynchronously.\n\n    Uses AsyncExecutor for non-blocking execution. Ideal for integration\n    with FastAPI, aiohttp, and other async frameworks.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        ValueError: If executor doesn't support async\n    \"\"\"\n    if not self.executor.supports_async():\n        raise ValueError(\n            \"Current executor doesn't support async. \"\n            \"Use AsyncExecutor: Pipeline(specs, executor=AsyncExecutor())\"\n        )\n\n    # For now, wrap synchronous execution in async\n    # TODO: Implement fully async execution pipeline\n    import asyncio\n\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, self.execute, resume_from)\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(chunk_size: int | None = None) -&gt; Iterator[ExecutionResult]\n</code></pre> <p>Execute pipeline in streaming mode.</p> <p>Processes data in chunks for memory-efficient handling of large datasets. Ideal for datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int | None</code> <p>Number of rows per chunk (uses executor's chunk_size if None)</p> <code>None</code> <p>Yields:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult objects for each processed chunk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support streaming</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute_stream(\n    self, chunk_size: int | None = None\n) -&gt; Iterator[ExecutionResult]:\n    \"\"\"\n    Execute pipeline in streaming mode.\n\n    Processes data in chunks for memory-efficient handling of large datasets.\n    Ideal for datasets that don't fit in memory.\n\n    Args:\n        chunk_size: Number of rows per chunk (uses executor's chunk_size if None)\n\n    Yields:\n        ExecutionResult objects for each processed chunk\n\n    Raises:\n        ValueError: If executor doesn't support streaming\n    \"\"\"\n    if not self.executor.supports_streaming():\n        raise ValueError(\n            \"Current executor doesn't support streaming. \"\n            \"Use StreamingExecutor: Pipeline(specs, executor=StreamingExecutor())\"\n        )\n\n    # Use executor's chunk_size if not provided\n    if chunk_size is None and isinstance(self.executor, StreamingExecutor):\n        chunk_size = self.executor.chunk_size\n    elif chunk_size is None:\n        chunk_size = 1000  # Default fallback\n\n    # For now, execute the full pipeline and split result into chunks\n    # TODO: Implement proper streaming execution that processes chunks independently\n    result = self.execute()\n\n    # Split the result data into chunks and yield as separate ExecutionResults\n    total_rows = len(result.data)\n    for start_idx in range(0, total_rows, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_data = result.data.iloc[start_idx:end_idx].copy()\n\n        # Create a chunk result with proportional metrics\n        chunk_rows = len(chunk_data)\n        chunk_result = ExecutionResult(\n            data=chunk_data,\n            metrics=ProcessingStats(\n                total_rows=chunk_rows,\n                processed_rows=chunk_rows,\n                failed_rows=0,\n                skipped_rows=0,\n                rows_per_second=result.metrics.rows_per_second,\n                total_duration_seconds=result.metrics.total_duration_seconds\n                * (chunk_rows / total_rows),\n                stage_durations=result.metrics.stage_durations,\n            ),\n            costs=CostEstimate(\n                total_cost=result.costs.total_cost\n                * Decimal(chunk_rows / total_rows),\n                total_tokens=int(\n                    result.costs.total_tokens * (chunk_rows / total_rows)\n                ),\n                input_tokens=int(\n                    result.costs.input_tokens * (chunk_rows / total_rows)\n                ),\n                output_tokens=int(\n                    result.costs.output_tokens * (chunk_rows / total_rows)\n                ),\n                rows=chunk_rows,\n                confidence=result.costs.confidence,\n            ),\n            execution_id=result.execution_id,\n            start_time=result.start_time,\n            end_time=result.end_time,\n            success=True,\n        )\n        yield chunk_result\n</code></pre>"},{"location":"api/api/pipeline_builder/","title":"pipeline_builder","text":""},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder","title":"pipeline_builder","text":"<p>Pipeline Builder - Fluent API for constructing pipelines.</p> <p>Implements Builder pattern for intuitive pipeline creation.</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder","title":"PipelineBuilder","text":"<pre><code>PipelineBuilder()\n</code></pre> <p>Fluent builder for constructing pipelines.</p> <p>Provides an intuitive, chainable API for common use cases.</p> Example <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p> <p>Initialize builder with None values.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with None values.\"\"\"\n    self._dataset_spec: DatasetSpec | None = None\n    self._prompt_spec: PromptSpec | None = None\n    self._llm_spec: LLMSpec | None = None\n    self._processing_spec: ProcessingSpec = ProcessingSpec()\n    self._output_spec: OutputSpec | None = None\n    self._dataframe: pd.DataFrame | None = None\n    self._executor: ExecutionStrategy | None = None\n    self._custom_parser: any | None = None\n    self._custom_llm_client: any | None = None\n    self._custom_stages: list[dict] = []  # For custom stage injection\n    self._observers: list[tuple[str, dict]] = []  # For observability\n    self._custom_metadata: dict[str, Any] = {}  # For arbitrary metadata\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create() -&gt; PipelineBuilder\n</code></pre> <p>Start builder chain.</p> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>New PipelineBuilder instance</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef create() -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Start builder chain.\n\n    Returns:\n        New PipelineBuilder instance\n    \"\"\"\n    return PipelineBuilder()\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_specifications","title":"from_specifications  <code>staticmethod</code>","text":"<pre><code>from_specifications(specs: PipelineSpecifications) -&gt; PipelineBuilder\n</code></pre> <p>Create builder from existing specifications.</p> <p>Useful for loading from YAML and modifying programmatically.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>PipelineSpecifications</code> <p>Complete pipeline specifications</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>PipelineBuilder pre-configured with specs</p> Example <p>specs = load_pipeline_config(\"config.yaml\") builder = PipelineBuilder.from_specifications(specs) pipeline = builder.build()</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef from_specifications(specs: PipelineSpecifications) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Create builder from existing specifications.\n\n    Useful for loading from YAML and modifying programmatically.\n\n    Args:\n        specs: Complete pipeline specifications\n\n    Returns:\n        PipelineBuilder pre-configured with specs\n\n    Example:\n        specs = load_pipeline_config(\"config.yaml\")\n        builder = PipelineBuilder.from_specifications(specs)\n        pipeline = builder.build()\n    \"\"\"\n    builder = PipelineBuilder()\n    builder._dataset_spec = specs.dataset\n    builder._prompt_spec = specs.prompt\n    builder._llm_spec = specs.llm\n    builder._processing_spec = specs.processing\n    builder._output_spec = specs.output\n    return builder\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_csv","title":"from_csv","text":"<pre><code>from_csv(path: str, input_columns: list[str], output_columns: list[str], delimiter: str = ',', encoding: str = 'utf-8') -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names to use in prompts</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names to generate</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding (default: utf-8)</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>builder = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"description\"],\n        output_columns=[\"category\"]\n    )\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_csv(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV data source.\n\n    Args:\n        path: Path to CSV file\n        input_columns: Input column names to use in prompts\n        output_columns: Output column names to generate\n        delimiter: CSV delimiter (default: comma)\n        encoding: File encoding (default: utf-8)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        builder = (\n            PipelineBuilder.create()\n            .from_csv(\n                \"products.csv\",\n                input_columns=[\"description\"],\n                output_columns=[\"category\"]\n            )\n        )\n        ```\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.CSV,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        delimiter=delimiter,\n        encoding=encoding,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_excel","title":"from_excel","text":"<pre><code>from_excel(path: str, input_columns: list[str], output_columns: list[str], sheet_name: str | int = 0) -&gt; PipelineBuilder\n</code></pre> <p>Configure Excel data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Excel file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_excel(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    sheet_name: str | int = 0,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Excel data source.\n\n    Args:\n        path: Path to Excel file\n        input_columns: Input column names\n        output_columns: Output column names\n        sheet_name: Sheet name or index\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.EXCEL,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        sheet_name=sheet_name,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_parquet","title":"from_parquet","text":"<pre><code>from_parquet(path: str, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure Parquet data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_parquet(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Parquet data source.\n\n    Args:\n        path: Path to Parquet file\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.PARQUET,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_dataframe","title":"from_dataframe","text":"<pre><code>from_dataframe(df: DataFrame, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure DataFrame source.</p> <p>Useful for processing in-memory data or chaining pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame with input data</p> required <code>input_columns</code> <code>list[str]</code> <p>Column names to use in prompts</p> required <code>output_columns</code> <code>list[str]</code> <p>Column names to generate</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\"text\": [\"Hello\", \"World\"]})\nbuilder = (\n    PipelineBuilder.create()\n    .from_dataframe(df, input_columns=[\"text\"], output_columns=[\"result\"])\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_dataframe(\n    self,\n    df: pd.DataFrame,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure DataFrame source.\n\n    Useful for processing in-memory data or chaining pipelines.\n\n    Args:\n        df: Pandas DataFrame with input data\n        input_columns: Column names to use in prompts\n        output_columns: Column names to generate\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        import pandas as pd\n\n        df = pd.DataFrame({\"text\": [\"Hello\", \"World\"]})\n        builder = (\n            PipelineBuilder.create()\n            .from_dataframe(df, input_columns=[\"text\"], output_columns=[\"result\"])\n        )\n        ```\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.DATAFRAME,\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    self._dataframe = df\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_prompt","title":"with_prompt","text":"<pre><code>with_prompt(template: str, system_message: str | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Configure prompt template.</p> <p>Use {variable} syntax to reference input columns. The LLM will process each row using this template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {variable} placeholders matching input_columns</p> required <code>system_message</code> <code>str | None</code> <p>Optional system message for chat models</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>builder.with_prompt(\n    \"Categorize this product: {title}\\n\\nCategory:\",\n    system_message=\"You are a product categorization expert.\"\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_prompt(\n    self,\n    template: str,\n    system_message: str | None = None,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure prompt template.\n\n    Use {variable} syntax to reference input columns. The LLM will process\n    each row using this template.\n\n    Args:\n        template: Prompt template with {variable} placeholders matching input_columns\n        system_message: Optional system message for chat models\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        builder.with_prompt(\n            \"Categorize this product: {title}\\\\n\\\\nCategory:\",\n            system_message=\"You are a product categorization expert.\"\n        )\n        ```\n    \"\"\"\n    self._prompt_spec = PromptSpec(\n        template=template,\n        system_message=system_message,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_system_prompt","title":"with_system_prompt","text":"<pre><code>with_system_prompt(system_prompt: str) -&gt; PipelineBuilder\n</code></pre> <p>Set system prompt for caching optimization.</p> <p>System prompts are cached by providers (OpenAI, Anthropic) and reused across all rows, reducing costs by 50-90% for the cached portion.</p> <p>This method should be called after with_prompt() to set or update the system message separately from the user prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>Static instructions/context (will be cached)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Review: {text}\")  # Dynamic per row\n    .with_system_prompt('''\n        You are a sentiment classifier.\n        Classify reviews as: positive, negative, or neutral.\n        Return only the label, nothing else.\n    ''')  # Cached across all rows\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <p>Can also be set via with_prompt(template, system_message=...). This method provides a more explicit API for caching optimization.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_system_prompt(self, system_prompt: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set system prompt for caching optimization.\n\n    System prompts are cached by providers (OpenAI, Anthropic) and reused\n    across all rows, reducing costs by 50-90% for the cached portion.\n\n    This method should be called after with_prompt() to set or update the\n    system message separately from the user prompt template.\n\n    Args:\n        system_prompt: Static instructions/context (will be cached)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"])\n            .with_prompt(\"Review: {text}\")  # Dynamic per row\n            .with_system_prompt('''\n                You are a sentiment classifier.\n                Classify reviews as: positive, negative, or neutral.\n                Return only the label, nothing else.\n            ''')  # Cached across all rows\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        Can also be set via with_prompt(template, system_message=...).\n        This method provides a more explicit API for caching optimization.\n    \"\"\"\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_system_prompt()\")\n\n    self._prompt_spec.system_message = system_prompt\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_batch_size","title":"with_batch_size","text":"<pre><code>with_batch_size(batch_size: int) -&gt; PipelineBuilder\n</code></pre> <p>Set batch size for multi-row processing.</p> <p>Process multiple rows in a single API call to reduce costs and latency by up to 100\u00d7. Batch size of 1 (default) processes rows individually.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of rows to process per API call (1-500)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt; 1 or prompt not set</p> Example <pre><code># Process 100 rows per API call (100\u00d7 fewer calls)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Classify: {text}\")\n    .with_batch_size(100)  # 5M rows = 50K API calls!\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <ul> <li>Batch size is limited by model context window</li> <li>Larger batches = fewer API calls but higher risk of partial failures</li> <li>Recommended: Start with 10-50, increase based on results</li> </ul> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_size(self, batch_size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set batch size for multi-row processing.\n\n    Process multiple rows in a single API call to reduce costs and latency\n    by up to 100\u00d7. Batch size of 1 (default) processes rows individually.\n\n    Args:\n        batch_size: Number of rows to process per API call (1-500)\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If batch_size &lt; 1 or prompt not set\n\n    Example:\n        ```python\n        # Process 100 rows per API call (100\u00d7 fewer calls)\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"])\n            .with_prompt(\"Classify: {text}\")\n            .with_batch_size(100)  # 5M rows = 50K API calls!\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        - Batch size is limited by model context window\n        - Larger batches = fewer API calls but higher risk of partial failures\n        - Recommended: Start with 10-50, increase based on results\n    \"\"\"\n    if batch_size &lt; 1:\n        raise ValueError(f\"batch_size must be &gt;= 1, got {batch_size}\")\n\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_batch_size()\")\n\n    # Update batch_size directly (consistent with with_system_prompt)\n    self._prompt_spec.batch_size = batch_size\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_batch_strategy","title":"with_batch_strategy","text":"<pre><code>with_batch_strategy(strategy: str) -&gt; PipelineBuilder\n</code></pre> <p>Set batch formatting strategy.</p> <p>Choose how multiple rows are formatted in a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Strategy name (\"json\" or \"csv\")</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy is not supported or prompt not set</p> Example <pre><code># Use JSON array format (default, most reliable)\npipeline.with_batch_size(100).with_batch_strategy(\"json\")\n\n# Use CSV format (more compact, experimental)\npipeline.with_batch_size(100).with_batch_strategy(\"csv\")\n</code></pre> Supported Strategies <ul> <li>\"json\": JSON array format (default, most reliable)</li> <li>\"csv\": CSV format (more compact, experimental)</li> </ul> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_strategy(self, strategy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set batch formatting strategy.\n\n    Choose how multiple rows are formatted in a single prompt.\n\n    Args:\n        strategy: Strategy name (\"json\" or \"csv\")\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If strategy is not supported or prompt not set\n\n    Example:\n        ```python\n        # Use JSON array format (default, most reliable)\n        pipeline.with_batch_size(100).with_batch_strategy(\"json\")\n\n        # Use CSV format (more compact, experimental)\n        pipeline.with_batch_size(100).with_batch_strategy(\"csv\")\n        ```\n\n    Supported Strategies:\n        - \"json\": JSON array format (default, most reliable)\n        - \"csv\": CSV format (more compact, experimental)\n    \"\"\"\n    allowed = [\"json\", \"csv\"]\n    if strategy not in allowed:\n        raise ValueError(\n            f\"batch_strategy must be one of {allowed}, got '{strategy}'\"\n        )\n\n    if not self._prompt_spec:\n        raise ValueError(\"Call with_prompt() before with_batch_strategy()\")\n\n    # Update batch_strategy directly (consistent with with_system_prompt)\n    self._prompt_spec.batch_strategy = strategy\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_jinja2","title":"with_jinja2","text":"<pre><code>with_jinja2(enabled: bool = True) -&gt; PipelineBuilder\n</code></pre> <p>Enable Jinja2 template rendering for advanced prompt control.</p> <p>Jinja2 provides powerful features for dynamic prompts: - Conditionals: {% if condition %}...{% endif %} - Loops: {% for item in items %}...{% endfor %} - Filters: {{ text | upper | truncate(100) }} - Complex logic within templates</p> <p>By default, Ondine uses Python's .format() for simple {variable} substitution. Enable Jinja2 when you need programmatic control flow.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Enable (True) or disable (False) Jinja2 rendering</p> <code>True</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Conditional prompt based on data\ntemplate = '''Extract from: {{ description }}\n{% if category == \"beverage\" %}\nFocus on volume units (oz, ml, L)\n{% elif category == \"food\" %}\nFocus on weight units (lb, oz, g)\n{% endif %}'''\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"products.csv\", input_columns=[\"description\", \"category\"])\n    .with_prompt(template)\n    .with_jinja2(True)  # Enable Jinja2\n    .with_llm(\"openai\", \"gpt-4o-mini\")\n    .build()\n)\n</code></pre> Note <p>Simple {variable} syntax works with both modes. Only enable Jinja2 if you need conditionals, loops, or filters.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_jinja2(self, enabled: bool = True) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Enable Jinja2 template rendering for advanced prompt control.\n\n    Jinja2 provides powerful features for dynamic prompts:\n    - Conditionals: {% if condition %}...{% endif %}\n    - Loops: {% for item in items %}...{% endfor %}\n    - Filters: {{ text | upper | truncate(100) }}\n    - Complex logic within templates\n\n    By default, Ondine uses Python's .format() for simple {variable}\n    substitution. Enable Jinja2 when you need programmatic control flow.\n\n    Args:\n        enabled: Enable (True) or disable (False) Jinja2 rendering\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Conditional prompt based on data\n        template = '''Extract from: {{ description }}\n        {% if category == \"beverage\" %}\n        Focus on volume units (oz, ml, L)\n        {% elif category == \"food\" %}\n        Focus on weight units (lb, oz, g)\n        {% endif %}'''\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"products.csv\", input_columns=[\"description\", \"category\"])\n            .with_prompt(template)\n            .with_jinja2(True)  # Enable Jinja2\n            .with_llm(\"openai\", \"gpt-4o-mini\")\n            .build()\n        )\n        ```\n\n    Note:\n        Simple {variable} syntax works with both modes. Only enable\n        Jinja2 if you need conditionals, loops, or filters.\n    \"\"\"\n    self._processing_spec.use_jinja2 = enabled\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm","title":"with_llm","text":"<pre><code>with_llm(provider: str, model: str, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, **kwargs: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM provider.</p> <p>Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom providers. API keys can be provided explicitly or via environment variables.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (openai, azure_openai, anthropic, groq, mlx) or custom provider ID</p> required <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\")</p> required <code>api_key</code> <code>str | None</code> <p>API key (optional, reads from environment if not provided)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-1.0, default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum output tokens (optional, uses model default)</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Provider-specific parameters (e.g., azure_endpoint, azure_deployment)</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># OpenAI\nbuilder.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n# Groq (fast and affordable)\nbuilder.with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n\n# Azure OpenAI with Managed Identity\nbuilder.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    use_managed_identity=True\n)\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm(\n    self,\n    provider: str,\n    model: str,\n    api_key: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    **kwargs: any,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM provider.\n\n    Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom providers.\n    API keys can be provided explicitly or via environment variables.\n\n    Args:\n        provider: Provider name (openai, azure_openai, anthropic, groq, mlx) or custom provider ID\n        model: Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\")\n        api_key: API key (optional, reads from environment if not provided)\n        temperature: Sampling temperature (0.0-1.0, default: 0.0 for deterministic)\n        max_tokens: Maximum output tokens (optional, uses model default)\n        **kwargs: Provider-specific parameters (e.g., azure_endpoint, azure_deployment)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # OpenAI\n        builder.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n        # Groq (fast and affordable)\n        builder.with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n\n        # Azure OpenAI with Managed Identity\n        builder.with_llm(\n            provider=\"azure_openai\",\n            model=\"gpt-4\",\n            azure_endpoint=\"https://your-resource.openai.azure.com/\",\n            azure_deployment=\"gpt-4-deployment\",\n            use_managed_identity=True\n        )\n        ```\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Try to convert to enum for built-in providers\n    try:\n        provider_enum = LLMProvider(provider.lower())\n    except ValueError:\n        # Not a built-in provider - check if it's a custom provider\n        if ProviderRegistry.is_registered(provider):\n            # Use a dummy enum value for validation, but store the actual provider string\n            provider_enum = LLMProvider.OPENAI  # Dummy for Pydantic validation\n            kwargs[\"_custom_provider_id\"] = provider\n        else:\n            raise ValueError(\n                f\"Unknown provider: {provider}. \"\n                f\"Available providers: {', '.join(ProviderRegistry.list_providers())}\"\n            )\n\n    self._llm_spec = LLMSpec(\n        provider=provider_enum,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec","title":"with_llm_spec","text":"<pre><code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM using a pre-built LLMSpec object.</p> <p>This method allows using LLMSpec objects directly, enabling: - Reusable provider configurations - Use of LLMProviderPresets for common providers - Custom LLMSpec instances for advanced use cases</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification object</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If spec is not an LLMSpec instance</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm_spec(self, spec: LLMSpec) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM using a pre-built LLMSpec object.\n\n    This method allows using LLMSpec objects directly, enabling:\n    - Reusable provider configurations\n    - Use of LLMProviderPresets for common providers\n    - Custom LLMSpec instances for advanced use cases\n\n    Args:\n        spec: LLM specification object\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        TypeError: If spec is not an LLMSpec instance\n\n    Example:\n        # Use preset\n        from ondine.core.specifications import LLMProviderPresets\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Process: {text}\")\n            .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n            .build()\n        )\n\n        # Custom spec\n        custom = LLMSpec(\n            provider=LLMProvider.OPENAI,\n            model=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n        pipeline.with_llm_spec(custom)\n\n        # Override preset\n        spec = LLMProviderPresets.GPT4O_MINI.model_copy(\n            update={\"temperature\": 0.9}\n        )\n        pipeline.with_llm_spec(spec)\n    \"\"\"\n    if not isinstance(spec, LLMSpec):\n        raise TypeError(\n            f\"Expected LLMSpec, got {type(spec).__name__}. \"\n            f\"Use with_llm() for parameter-based configuration.\"\n        )\n\n    self._llm_spec = spec\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec--use-preset","title":"Use preset","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec--custom-spec","title":"Custom spec","text":"<p>custom = LLMSpec(     provider=LLMProvider.OPENAI,     model=\"gpt-4o-mini\",     temperature=0.7 ) pipeline.with_llm_spec(custom)</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec--override-preset","title":"Override preset","text":"<p>spec = LLMProviderPresets.GPT4O_MINI.model_copy(     update={\"temperature\": 0.9} ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_custom_llm_client","title":"with_custom_llm_client","text":"<pre><code>with_custom_llm_client(client: any) -&gt; PipelineBuilder\n</code></pre> <p>Provide a custom LLM client instance directly.</p> <p>This allows advanced users to create their own LLM client implementations by extending the LLMClient base class. The custom client will be used instead of the factory-created client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>any</code> <p>Custom LLM client instance (must inherit from LLMClient)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <p>class MyCustomClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         # Custom implementation         ...</p> <p>pipeline = (     PipelineBuilder.create()     .from_dataframe(df, ...)     .with_prompt(\"...\")     .with_custom_llm_client(MyCustomClient(spec))     .build() )</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_custom_llm_client(self, client: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Provide a custom LLM client instance directly.\n\n    This allows advanced users to create their own LLM client implementations\n    by extending the LLMClient base class. The custom client will be used\n    instead of the factory-created client.\n\n    Args:\n        client: Custom LLM client instance (must inherit from LLMClient)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        class MyCustomClient(LLMClient):\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                # Custom implementation\n                ...\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_dataframe(df, ...)\n            .with_prompt(\"...\")\n            .with_custom_llm_client(MyCustomClient(spec))\n            .build()\n        )\n    \"\"\"\n    from ondine.adapters.llm_client import LLMClient\n\n    if not isinstance(client, LLMClient):\n        raise TypeError(\n            f\"Custom client must inherit from LLMClient, got {type(client).__name__}\"\n        )\n\n    self._custom_llm_client = client\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_processing_batch_size","title":"with_processing_batch_size","text":"<pre><code>with_processing_batch_size(size: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure internal batch size for PromptFormatterStage.</p> <p>This is different from with_batch_size() which enables multi-row batching. This method controls how many prompts are grouped together internally for processing efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Rows per internal batch</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Note <p>This is an internal optimization parameter. Most users should use with_batch_size() for multi-row batching instead.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_processing_batch_size(self, size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure internal batch size for PromptFormatterStage.\n\n    This is different from with_batch_size() which enables multi-row batching.\n    This method controls how many prompts are grouped together internally\n    for processing efficiency.\n\n    Args:\n        size: Rows per internal batch\n\n    Returns:\n        Self for chaining\n\n    Note:\n        This is an internal optimization parameter. Most users should use\n        with_batch_size() for multi-row batching instead.\n    \"\"\"\n    self._processing_spec.batch_size = size\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_concurrency","title":"with_concurrency","text":"<pre><code>with_concurrency(threads: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure concurrent requests.</p> <p>Higher concurrency = faster processing but more API load. Adjust based on your provider's rate limits.</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int</code> <p>Number of concurrent threads (1-100, typical: 5-20)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Conservative (free tier)\nbuilder.with_concurrency(5)\n\n# Aggressive (paid tier)\nbuilder.with_concurrency(20)\n\n# Maximum (enterprise)\nbuilder.with_concurrency(50)\n</code></pre> Note <p>Groq supports high concurrency (~100), while OpenAI free tier is limited to ~5.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_concurrency(self, threads: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure concurrent requests.\n\n    Higher concurrency = faster processing but more API load. Adjust based on\n    your provider's rate limits.\n\n    Args:\n        threads: Number of concurrent threads (1-100, typical: 5-20)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Conservative (free tier)\n        builder.with_concurrency(5)\n\n        # Aggressive (paid tier)\n        builder.with_concurrency(20)\n\n        # Maximum (enterprise)\n        builder.with_concurrency(50)\n        ```\n\n    Note:\n        Groq supports high concurrency (~100), while OpenAI free tier is limited to ~5.\n    \"\"\"\n    self._processing_spec.concurrency = threads\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_checkpoint_interval","title":"with_checkpoint_interval","text":"<pre><code>with_checkpoint_interval(rows: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint frequency.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Rows between checkpoints</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_interval(self, rows: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint frequency.\n\n    Args:\n        rows: Rows between checkpoints\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_interval = rows\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_rate_limit","title":"with_rate_limit","text":"<pre><code>with_rate_limit(rpm: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure rate limiting.</p> <p>Prevents hitting API rate limits by throttling requests using token bucket algorithm. Set this below your provider's actual limit for safety.</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute (typical: 20-60 for free tiers, 100+ for paid)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># OpenAI free tier (60 RPM limit)\nbuilder.with_rate_limit(50)\n\n# Groq free tier (30 RPM limit)\nbuilder.with_rate_limit(25)\n\n# Paid tier with high limits\nbuilder.with_rate_limit(100)\n</code></pre> Note <p>Rate limiting is applied per pipeline execution, not globally.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_rate_limit(self, rpm: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure rate limiting.\n\n    Prevents hitting API rate limits by throttling requests using token bucket algorithm.\n    Set this below your provider's actual limit for safety.\n\n    Args:\n        rpm: Requests per minute (typical: 20-60 for free tiers, 100+ for paid)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # OpenAI free tier (60 RPM limit)\n        builder.with_rate_limit(50)\n\n        # Groq free tier (30 RPM limit)\n        builder.with_rate_limit(25)\n\n        # Paid tier with high limits\n        builder.with_rate_limit(100)\n        ```\n\n    Note:\n        Rate limiting is applied per pipeline execution, not globally.\n    \"\"\"\n    self._processing_spec.rate_limit_rpm = rpm\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_max_retries","title":"with_max_retries","text":"<pre><code>with_max_retries(retries: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>Maximum number of retry attempts</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_retries(self, retries: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum retry attempts.\n\n    Args:\n        retries: Maximum number of retry attempts\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_retries = retries\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_max_budget","title":"with_max_budget","text":"<pre><code>with_max_budget(budget: float) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum budget.</p> <p>Pipeline will halt execution if cost exceeds this limit. Warnings are shown at 75% and 90% of budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget</code> <code>float</code> <p>Maximum budget in USD (e.g., 5.0 for $5)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Set $5 budget limit\nbuilder.with_max_budget(5.0)\n\n# For testing with small budget\nbuilder.with_max_budget(0.50)\n\n# For production runs\nbuilder.with_max_budget(100.0)\n</code></pre> Note <p>Budget enforcement uses Decimal precision to avoid floating-point errors.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_budget(self, budget: float) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum budget.\n\n    Pipeline will halt execution if cost exceeds this limit. Warnings are shown\n    at 75% and 90% of budget.\n\n    Args:\n        budget: Maximum budget in USD (e.g., 5.0 for $5)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Set $5 budget limit\n        builder.with_max_budget(5.0)\n\n        # For testing with small budget\n        builder.with_max_budget(0.50)\n\n        # For production runs\n        builder.with_max_budget(100.0)\n        ```\n\n    Note:\n        Budget enforcement uses Decimal precision to avoid floating-point errors.\n    \"\"\"\n    self._processing_spec.max_budget = Decimal(str(budget))\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_error_policy","title":"with_error_policy","text":"<pre><code>with_error_policy(policy: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure error handling policy.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>Error policy ('skip', 'fail', 'retry', 'use_default')</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_error_policy(self, policy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure error handling policy.\n\n    Args:\n        policy: Error policy ('skip', 'fail', 'retry', 'use_default')\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.core.specifications import ErrorPolicy\n\n    self._processing_spec.error_policy = ErrorPolicy(policy.lower())\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_checkpoint_dir","title":"with_checkpoint_dir","text":"<pre><code>with_checkpoint_dir(directory: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to checkpoint directory</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_dir(self, directory: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint directory.\n\n    Args:\n        directory: Path to checkpoint directory\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_dir = Path(directory)\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_parser","title":"with_parser","text":"<pre><code>with_parser(parser: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure response parser.</p> <p>This method allows setting a custom parser. The parser type determines the response_format in the prompt spec.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>any</code> <p>Parser instance (JSONParser, RegexParser, PydanticParser, etc.)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_parser(self, parser: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure response parser.\n\n    This method allows setting a custom parser. The parser type\n    determines the response_format in the prompt spec.\n\n    Args:\n        parser: Parser instance (JSONParser, RegexParser, PydanticParser, etc.)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    # Store the parser for later use in the pipeline\n    # We'll configure response_format based on parser type\n    if hasattr(parser, \"__class__\"):\n        parser_name = parser.__class__.__name__\n        if \"JSON\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            # Update the existing prompt spec's response_format\n            self._prompt_spec.response_format = \"json\"\n        elif \"Regex\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            self._prompt_spec.response_format = \"regex\"\n            if hasattr(parser, \"patterns\"):\n                self._prompt_spec.regex_patterns = parser.patterns\n\n    # Store the parser instance in metadata for the pipeline to use\n    if not hasattr(self, \"_custom_parser\"):\n        self._custom_parser = parser\n\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.to_csv","title":"to_csv","text":"<pre><code>to_csv(path: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV output destination.</p> <p>Alias for with_output(path, format='csv').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output CSV file path</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def to_csv(self, path: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV output destination.\n\n    Alias for with_output(path, format='csv').\n\n    Args:\n        path: Output CSV file path\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    return self.with_output(path, format=\"csv\")\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_output","title":"with_output","text":"<pre><code>with_output(path: str, format: str = 'csv', merge_strategy: str = 'replace') -&gt; PipelineBuilder\n</code></pre> <p>Configure output destination.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path</p> required <code>format</code> <code>str</code> <p>Output format (csv, excel, parquet)</p> <code>'csv'</code> <code>merge_strategy</code> <code>str</code> <p>Merge strategy (replace, append, update)</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_output(\n    self,\n    path: str,\n    format: str = \"csv\",\n    merge_strategy: str = \"replace\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure output destination.\n\n    Args:\n        path: Output file path\n        format: Output format (csv, excel, parquet)\n        merge_strategy: Merge strategy (replace, append, update)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    format_map = {\n        \"csv\": DataSourceType.CSV,\n        \"excel\": DataSourceType.EXCEL,\n        \"parquet\": DataSourceType.PARQUET,\n    }\n\n    merge_map = {\n        \"replace\": MergeStrategy.REPLACE,\n        \"append\": MergeStrategy.APPEND,\n        \"update\": MergeStrategy.UPDATE,\n    }\n\n    self._output_spec = OutputSpec(\n        destination_type=format_map[format.lower()],\n        destination_path=Path(path),\n        merge_strategy=merge_map[merge_strategy.lower()],\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_executor","title":"with_executor","text":"<pre><code>with_executor(executor: ExecutionStrategy) -&gt; PipelineBuilder\n</code></pre> <p>Set custom execution strategy.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ExecutionStrategy</code> <p>ExecutionStrategy instance</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_executor(self, executor: ExecutionStrategy) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set custom execution strategy.\n\n    Args:\n        executor: ExecutionStrategy instance\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = executor\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_async_execution","title":"with_async_execution","text":"<pre><code>with_async_execution(max_concurrency: int = 10) -&gt; PipelineBuilder\n</code></pre> <p>Use async execution strategy.</p> <p>Enables async/await for non-blocking execution. Ideal for FastAPI, aiohttp, and async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_async_execution(self, max_concurrency: int = 10) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use async execution strategy.\n\n    Enables async/await for non-blocking execution.\n    Ideal for FastAPI, aiohttp, and async frameworks.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = AsyncExecutor(max_concurrency=max_concurrency)\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_streaming","title":"with_streaming","text":"<pre><code>with_streaming(chunk_size: int = 1000) -&gt; PipelineBuilder\n</code></pre> <p>Use streaming execution strategy.</p> <p>Processes data in chunks for memory-efficient handling. Ideal for large datasets (100K+ rows).</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_streaming(self, chunk_size: int = 1000) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use streaming execution strategy.\n\n    Processes data in chunks for memory-efficient handling.\n    Ideal for large datasets (100K+ rows).\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = StreamingExecutor(chunk_size=chunk_size)\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_progress_mode","title":"with_progress_mode","text":"<pre><code>with_progress_mode(mode: str = 'auto') -&gt; PipelineBuilder\n</code></pre> <p>Configure progress tracking mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Progress tracking mode - \"auto\": Auto-detect (rich if TTY, else logging) [default] - \"rich\": Beautiful progress bars with ETA - \"logging\": Simple log messages - \"none\": Disable progress tracking</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <pre><code># Use rich progress (beautiful UI)\nbuilder.with_progress_mode(\"rich\")\n\n# Disable progress (faster, cleaner logs)\nbuilder.with_progress_mode(\"none\")\n\n# Auto-detect (recommended)\nbuilder.with_progress_mode(\"auto\")\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_progress_mode(self, mode: str = \"auto\") -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure progress tracking mode.\n\n    Args:\n        mode: Progress tracking mode\n            - \"auto\": Auto-detect (rich if TTY, else logging) [default]\n            - \"rich\": Beautiful progress bars with ETA\n            - \"logging\": Simple log messages\n            - \"none\": Disable progress tracking\n\n    Returns:\n        Self for chaining\n\n    Example:\n        ```python\n        # Use rich progress (beautiful UI)\n        builder.with_progress_mode(\"rich\")\n\n        # Disable progress (faster, cleaner logs)\n        builder.with_progress_mode(\"none\")\n\n        # Auto-detect (recommended)\n        builder.with_progress_mode(\"auto\")\n        ```\n    \"\"\"\n    self._processing_spec.progress_mode = mode\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_stage","title":"with_stage","text":"<pre><code>with_stage(stage_name: str, position: str = 'before_prompt', **stage_kwargs) -&gt; PipelineBuilder\n</code></pre> <p>Add a custom pipeline stage by name.</p> <p>Enables injection of custom processing stages at specific points in the pipeline. Stages must be registered via StageRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Registered stage name (e.g., \"rag_retrieval\")</p> required <code>position</code> <code>str</code> <p>Where to inject the stage. Options: - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing - \"after_parser\": After response parsing</p> <code>'before_prompt'</code> <code>**stage_kwargs</code> <p>Arguments to pass to stage constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name not registered or position invalid</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_stage(\n    self,\n    stage_name: str,\n    position: str = \"before_prompt\",\n    **stage_kwargs,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add a custom pipeline stage by name.\n\n    Enables injection of custom processing stages at specific points\n    in the pipeline. Stages must be registered via StageRegistry.\n\n    Args:\n        stage_name: Registered stage name (e.g., \"rag_retrieval\")\n        position: Where to inject the stage. Options:\n            - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting\n            - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation\n            - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing\n            - \"after_parser\": After response parsing\n        **stage_kwargs: Arguments to pass to stage constructor\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If stage_name not registered or position invalid\n\n    Example:\n        # RAG retrieval example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])\n            .with_stage(\n                \"rag_retrieval\",\n                position=\"before_prompt\",\n                vector_store=\"pinecone\",\n                index_name=\"my-docs\",\n                top_k=5\n            )\n            .with_prompt(\"Context: {retrieved_context}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o\")\n            .build()\n        )\n\n        # Content moderation example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])\n            .with_stage(\n                \"content_moderation\",\n                position=\"before_llm\",\n                block_patterns=[\"spam\", \"offensive\"]\n            )\n            .with_prompt(\"Moderate: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n    \"\"\"\n    from ondine.stages.stage_registry import StageRegistry\n\n    # Validate position\n    valid_positions = [\n        \"after_loader\",\n        \"before_prompt\",\n        \"after_prompt\",\n        \"before_llm\",\n        \"after_llm\",\n        \"before_parser\",\n        \"after_parser\",\n    ]\n    if position not in valid_positions:\n        raise ValueError(\n            f\"Invalid position '{position}'. Must be one of: {', '.join(valid_positions)}\"\n        )\n\n    # Get stage class from registry (this will raise ValueError if not found)\n    stage_class = StageRegistry.get(stage_name)\n\n    # Store stage config for later instantiation\n    self._custom_stages.append(\n        {\n            \"name\": stage_name,\n            \"class\": stage_class,\n            \"position\": position,\n            \"kwargs\": stage_kwargs,\n        }\n    )\n\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_stage--rag-retrieval-example","title":"RAG retrieval example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])     .with_stage(         \"rag_retrieval\",         position=\"before_prompt\",         vector_store=\"pinecone\",         index_name=\"my-docs\",         top_k=5     )     .with_prompt(\"Context: {retrieved_context}\\n\\nQuestion: {question}\\n\\nAnswer:\")     .with_llm(provider=\"openai\", model=\"gpt-4o\")     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_stage--content-moderation-example","title":"Content moderation example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])     .with_stage(         \"content_moderation\",         position=\"before_llm\",         block_patterns=[\"spam\", \"offensive\"]     )     .with_prompt(\"Moderate: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_observer","title":"with_observer","text":"<pre><code>with_observer(name: str, config: dict[str, any] | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Add observability observer to the pipeline.</p> <p>Observers receive events during pipeline execution for monitoring, logging, and tracing.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier (e.g., \"langfuse\", \"opentelemetry\", \"logging\")</p> required <code>config</code> <code>dict[str, any] | None</code> <p>Observer-specific configuration dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If observer not registered</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_observer(\n    self, name: str, config: dict[str, any] | None = None\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add observability observer to the pipeline.\n\n    Observers receive events during pipeline execution for monitoring,\n    logging, and tracing.\n\n    Args:\n        name: Observer identifier (e.g., \"langfuse\", \"opentelemetry\", \"logging\")\n        config: Observer-specific configuration dictionary\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If observer not registered\n\n    Example:\n        # OpenTelemetry for infrastructure monitoring\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"opentelemetry\", config={\n                \"tracer_name\": \"my_pipeline\",\n                \"include_prompts\": False\n            })\n            .build()\n        )\n\n        # Langfuse for LLM-specific observability\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"langfuse\", config={\n                \"public_key\": \"pk-lf-...\",\n                \"secret_key\": \"sk-lf-...\"  # pragma: allowlist secret\n            })\n            .build()\n        )\n\n        # Multiple observers\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", ...)\n            .with_prompt(\"...\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_observer(\"langfuse\", config={...})\n            .with_observer(\"opentelemetry\", config={...})\n            .with_observer(\"logging\", config={\"log_level\": \"DEBUG\"})\n            .build()\n        )\n    \"\"\"\n    from ondine.observability.registry import ObserverRegistry\n\n    # Validate observer is registered\n    if not ObserverRegistry.is_registered(name):\n        available = \", \".join(ObserverRegistry.list_observers())\n        raise ValueError(\n            f\"Observer '{name}' not registered. \"\n            f\"Available observers: {available or 'none'}\"\n        )\n\n    # Store observer config for later instantiation\n    self._observers.append((name, config or {}))\n\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_observer--opentelemetry-for-infrastructure-monitoring","title":"OpenTelemetry for infrastructure monitoring","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"opentelemetry\", config={         \"tracer_name\": \"my_pipeline\",         \"include_prompts\": False     })     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_observer--langfuse-for-llm-specific-observability","title":"Langfuse for LLM-specific observability","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"langfuse\", config={         \"public_key\": \"pk-lf-...\",         \"secret_key\": \"sk-lf-...\"  # pragma: allowlist secret     })     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_observer--multiple-observers","title":"Multiple observers","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", ...)     .with_prompt(\"...\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .with_observer(\"langfuse\", config={...})     .with_observer(\"opentelemetry\", config={...})     .with_observer(\"logging\", config={\"log_level\": \"DEBUG\"})     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_structured_output","title":"with_structured_output","text":"<pre><code>with_structured_output(schema: Any) -&gt; PipelineBuilder\n</code></pre> <p>Configure structured output using a Pydantic model.</p> <p>This enables native schema enforcement, parsing, and auto-retry logic using LlamaIndex's structured_predict capabilities.</p> <p>Automatically configures JSONParser to handle the structured JSON output, unless a custom parser was already configured.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Any</code> <p>Pydantic model class defining the expected output structure</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_structured_output(self, schema: Any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure structured output using a Pydantic model.\n\n    This enables native schema enforcement, parsing, and auto-retry logic\n    using LlamaIndex's structured_predict capabilities.\n\n    Automatically configures JSONParser to handle the structured JSON output,\n    unless a custom parser was already configured.\n\n    Args:\n        schema: Pydantic model class defining the expected output structure\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    if not hasattr(self, \"_custom_metadata\"):\n        self._custom_metadata = {}\n    self._custom_metadata[\"structured_output_model\"] = schema\n\n    # Auto-inject JSONParser if no parser configured\n    # Structured output always returns JSON, so we need a JSON parser\n    if self._custom_parser is None:\n        from ondine.stages.response_parser_stage import JSONParser\n\n        self._custom_parser = JSONParser()\n\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.build","title":"build","text":"<pre><code>build() -&gt; Pipeline\n</code></pre> <p>Build final Pipeline.</p> <p>Validates all configurations and constructs the Pipeline object ready for execution. This is the final step in the builder chain.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required specifications missing (dataset, prompt, or LLM)</p> Example <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_concurrency(10)\n    .with_max_budget(5.0)\n    .build()  # Returns Pipeline object\n)\n\n# Execute the pipeline\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.total_rows} rows\")\n</code></pre> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def build(self) -&gt; Pipeline:\n    \"\"\"\n    Build final Pipeline.\n\n    Validates all configurations and constructs the Pipeline object ready for execution.\n    This is the final step in the builder chain.\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If required specifications missing (dataset, prompt, or LLM)\n\n    Example:\n        ```python\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Summarize: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .with_concurrency(10)\n            .with_max_budget(5.0)\n            .build()  # Returns Pipeline object\n        )\n\n        # Execute the pipeline\n        result = pipeline.execute()\n        print(f\"Processed {result.metrics.total_rows} rows\")\n        ```\n    \"\"\"\n    # Validate required specs\n    if not self._dataset_spec:\n        raise ValueError(\"Dataset specification required\")\n    if not self._prompt_spec:\n        raise ValueError(\"Prompt specification required\")\n\n    # LLM spec is optional if custom client is provided\n    if not self._llm_spec and not self._custom_llm_client:\n        raise ValueError(\"Either LLM specification or custom LLM client required\")\n\n    # Prepare metadata with custom parser, custom client, custom stages, and observers\n    metadata = {}\n    if self._custom_metadata:\n        metadata.update(self._custom_metadata)\n    if self._custom_parser is not None:\n        metadata[\"custom_parser\"] = self._custom_parser\n    if self._custom_llm_client is not None:\n        metadata[\"custom_llm_client\"] = self._custom_llm_client\n    if self._custom_stages:\n        metadata[\"custom_stages\"] = self._custom_stages\n    if self._observers:\n        metadata[\"observers\"] = self._observers\n\n    # Create specifications bundle\n    # If custom client provided but no llm_spec, create a dummy spec\n    llm_spec = self._llm_spec\n    if llm_spec is None and self._custom_llm_client is not None:\n        # Create minimal spec using custom client's attributes\n        llm_spec = LLMSpec(\n            provider=LLMProvider.OPENAI,  # Dummy provider\n            model=self._custom_llm_client.model,\n            temperature=self._custom_llm_client.temperature,\n            max_tokens=self._custom_llm_client.max_tokens,\n        )\n\n    specifications = PipelineSpecifications(\n        dataset=self._dataset_spec,\n        prompt=self._prompt_spec,\n        llm=llm_spec,\n        processing=self._processing_spec,\n        output=self._output_spec,\n        metadata=metadata,\n    )\n\n    # Create and return pipeline\n    return Pipeline(\n        specifications,\n        dataframe=self._dataframe,\n        executor=self._executor,\n    )\n</code></pre>"},{"location":"api/api/pipeline_composer/","title":"pipeline_composer","text":""},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer","title":"pipeline_composer","text":"<p>Pipeline Composer for multi-column, multi-prompt processing.</p> <p>This module enables composing multiple single-prompt pipelines to generate multiple output columns, each with its own independent processing logic.</p> Example <p>composer = (     PipelineComposer(input_data=\"data.xlsx\")     .add_column(\"similarity\", similarity_pipeline)     .add_column(\"explanation\", explanation_pipeline, depends_on=[\"similarity\"])     .execute() )</p>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer","title":"PipelineComposer","text":"<pre><code>PipelineComposer(input_data: str | Path | DataFrame)\n</code></pre> <p>Composes multiple pipelines to process independent columns.</p> <p>Each pipeline processes one output column. Pipelines can depend on outputs from previous pipelines, enabling sequential processing.</p> <p>Design Philosophy: - Keep individual pipelines simple (single responsibility) - Compose complex workflows from simple building blocks - Make dependencies explicit (no hidden coupling) - Fail fast with clear error messages</p> <p>Initialize pipeline composer.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str | Path | DataFrame</code> <p>Either a file path (CSV/Excel) or DataFrame</p> required Design Note <p>We accept both paths and DataFrames to support different workflows: - Path: Lazy loading, memory efficient - DataFrame: Already loaded, faster iteration</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def __init__(self, input_data: str | Path | pd.DataFrame):\n    \"\"\"\n    Initialize pipeline composer.\n\n    Args:\n        input_data: Either a file path (CSV/Excel) or DataFrame\n\n    Design Note:\n        We accept both paths and DataFrames to support different workflows:\n        - Path: Lazy loading, memory efficient\n        - DataFrame: Already loaded, faster iteration\n    \"\"\"\n    if isinstance(input_data, str | Path):\n        self.input_path = str(input_data)\n        self.input_df = None  # Lazy load\n    elif isinstance(input_data, pd.DataFrame):\n        self.input_path = None\n        self.input_df = input_data.copy()\n    else:\n        raise TypeError(\n            f\"input_data must be str, Path, or DataFrame, got {type(input_data)}\"\n        )\n\n    # Storage for column pipelines\n    # Format: (column_name, pipeline, dependencies)\n    self.column_pipelines: list[tuple[str, Pipeline, list[str]]] = []\n\n    logger.info(\"PipelineComposer initialized\")\n</code></pre>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer.add_column","title":"add_column","text":"<pre><code>add_column(column_name: str, pipeline: Pipeline, depends_on: list[str] | None = None) -&gt; PipelineComposer\n</code></pre> <p>Add a pipeline for processing one output column.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of output column</p> required <code>pipeline</code> <code>Pipeline</code> <p>Pipeline to generate this column</p> required <code>depends_on</code> <code>list[str] | None</code> <p>List of columns this depends on (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Self for method chaining (fluent API)</p> Example <p>composer.add_column(\"score\", score_pipeline) composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])</p> Design Note <p>Fluent API (returns self) enables readable chaining: composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def add_column(\n    self,\n    column_name: str,\n    pipeline: Pipeline,\n    depends_on: list[str] | None = None,\n) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Add a pipeline for processing one output column.\n\n    Args:\n        column_name: Name of output column\n        pipeline: Pipeline to generate this column\n        depends_on: List of columns this depends on (optional)\n\n    Returns:\n        Self for method chaining (fluent API)\n\n    Example:\n        composer.add_column(\"score\", score_pipeline)\n        composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])\n\n    Design Note:\n        Fluent API (returns self) enables readable chaining:\n        composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()\n    \"\"\"\n    dependencies = depends_on or []\n\n    # Validate column name unique\n    existing_cols = [col for col, _, _ in self.column_pipelines]\n    if column_name in existing_cols:\n        raise ValueError(f\"Column '{column_name}' already added\")\n\n    self.column_pipelines.append((column_name, pipeline, dependencies))\n\n    logger.info(\n        f\"Added column '{column_name}' with dependencies: {dependencies or 'none'}\"\n    )\n\n    return self  # Enable chaining\n</code></pre>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer.execute","title":"execute","text":"<pre><code>execute() -&gt; ExecutionResult\n</code></pre> <p>Execute all column pipelines in dependency order.</p> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with all columns merged</p> Algorithm <ol> <li>Load input data (lazy if needed)</li> <li>Resolve execution order (topological sort)</li> <li>For each column:    a. Inject current DataFrame into pipeline    b. Execute pipeline    c. Merge result column into DataFrame</li> <li>Aggregate metrics and return final result</li> </ol> Design Note <p>Each pipeline operates on the accumulating DataFrame, so later pipelines can use earlier outputs as inputs.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def execute(self) -&gt; ExecutionResult:\n    \"\"\"\n    Execute all column pipelines in dependency order.\n\n    Returns:\n        ExecutionResult with all columns merged\n\n    Algorithm:\n        1. Load input data (lazy if needed)\n        2. Resolve execution order (topological sort)\n        3. For each column:\n           a. Inject current DataFrame into pipeline\n           b. Execute pipeline\n           c. Merge result column into DataFrame\n        4. Aggregate metrics and return final result\n\n    Design Note:\n        Each pipeline operates on the accumulating DataFrame,\n        so later pipelines can use earlier outputs as inputs.\n    \"\"\"\n    # Lazy load if needed\n    if self.input_df is None:\n        logger.info(f\"Loading input data from {self.input_path}\")\n        # Use DataReader adapter for consistent file loading\n        from ondine.adapters.data_io import create_data_reader\n        from ondine.core.specifications import DataSourceType\n\n        # Detect source type from file extension\n        if self.input_path.endswith(\".xlsx\") or self.input_path.endswith(\".xls\"):\n            source_type = DataSourceType.EXCEL\n        elif self.input_path.endswith(\".csv\"):\n            source_type = DataSourceType.CSV\n        elif self.input_path.endswith(\".parquet\"):\n            source_type = DataSourceType.PARQUET\n        else:\n            raise ValueError(f\"Unsupported file type: {self.input_path}\")\n\n        # Create reader and load data\n        reader = create_data_reader(\n            source_type=source_type, source_path=self.input_path\n        )\n        self.input_df = reader.read()\n\n    # Start with input data\n    df = self.input_df.copy()\n\n    # Resolve execution order\n    execution_order = self._get_execution_order()\n\n    logger.info(\n        f\"Executing {len(execution_order)} column pipelines in order: \"\n        f\"{[col for col, _, _ in execution_order]}\"\n    )\n\n    # Track metrics (keep Decimal precision for costs)\n    from decimal import Decimal\n\n    total_cost = Decimal(\"0.0\")\n    total_errors = []\n\n    # Execute each column pipeline\n    for col_name, pipeline, _deps in execution_order:\n        logger.info(f\"Processing column '{col_name}'...\")\n\n        # Inject current DataFrame (includes previous outputs)\n        pipeline.dataframe = df\n\n        # Execute pipeline\n        result = pipeline.execute()\n\n        # Merge new column\n        if col_name in result.data.columns:\n            df[col_name] = result.data[col_name]\n        else:\n            logger.warning(\n                f\"Pipeline for '{col_name}' didn't produce expected column\"\n            )\n\n        # Accumulate metrics (preserve Decimal precision)\n        cost_to_add = result.costs.total_cost\n        if not isinstance(cost_to_add, Decimal):\n            cost_to_add = Decimal(str(cost_to_add))\n        total_cost += cost_to_add\n        total_errors.extend(result.errors)\n\n        logger.info(\n            f\"Column '{col_name}' complete: \"\n            f\"{len(df)} rows, ${result.costs.total_cost:.4f}\"\n        )\n\n    # Create final result\n    final_result = ExecutionResult(\n        data=df,\n        metrics=ProcessingStats(\n            total_rows=len(df),\n            processed_rows=len(df),\n            failed_rows=len(total_errors),\n            skipped_rows=0,\n            rows_per_second=0.0,  # Aggregate metric not meaningful\n            total_duration_seconds=0.0,  # Would need timing\n        ),\n        costs=CostEstimate(\n            total_cost=total_cost,\n            total_tokens=0,  # Aggregate from individual pipelines\n            input_tokens=0,\n            output_tokens=0,\n            rows=len(df),\n        ),\n        errors=total_errors,\n    )\n\n    logger.info(\n        f\"Composition complete: {len(execution_order)} columns, \"\n        f\"${total_cost:.4f} total cost\"\n    )\n\n    return final_result\n</code></pre>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(config_path: str) -&gt; PipelineComposer\n</code></pre> <p>Load composer configuration from YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to composition config file</p> required <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Configured PipelineComposer</p> Example YAML <p>composition:   input: \"data.xlsx\"   pipelines:     - column: col1       config: pipeline1.yaml     - column: col2       depends_on: [col1]       config: pipeline2.yaml</p> Design Note <p>This enables pure YAML workflows without Python code.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>@classmethod\ndef from_yaml(cls, config_path: str) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Load composer configuration from YAML.\n\n    Args:\n        config_path: Path to composition config file\n\n    Returns:\n        Configured PipelineComposer\n\n    Example YAML:\n        composition:\n          input: \"data.xlsx\"\n          pipelines:\n            - column: col1\n              config: pipeline1.yaml\n            - column: col2\n              depends_on: [col1]\n              config: pipeline2.yaml\n\n    Design Note:\n        This enables pure YAML workflows without Python code.\n    \"\"\"\n    import yaml\n\n    from ondine.config import ConfigLoader\n\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n\n    composition = config.get(\"composition\", {})\n    input_data = composition.get(\"input\")\n\n    if not input_data:\n        raise ValueError(\"composition.input is required\")\n\n    composer = cls(input_data=input_data)\n\n    # Load each pipeline config\n    for pipeline_config in composition.get(\"pipelines\", []):\n        col_name = pipeline_config[\"column\"]\n        config_file = pipeline_config[\"config\"]\n        depends_on = pipeline_config.get(\"depends_on\", [])\n\n        # Load pipeline from its config\n        pipeline_specs = ConfigLoader.from_yaml(config_file)\n        pipeline = Pipeline(pipeline_specs)\n\n        composer.add_column(col_name, pipeline, depends_on)\n\n    return composer\n</code></pre>"},{"location":"api/api/quick/","title":"quick","text":""},{"location":"api/api/quick/#ondine.api.quick","title":"quick","text":"<p>Simplified API with smart defaults for quick pipeline creation.</p> <p>This module provides a high-level API that reduces boilerplate and makes common use cases trivial while still providing access to full functionality.</p> <p>Design Philosophy: - Convention over configuration - Auto-detect from context when possible - Provide sensible defaults for 80% of use cases - Still allow full customization when needed</p>"},{"location":"api/api/quick/#ondine.api.quick.QuickPipeline","title":"QuickPipeline","text":"<p>Simplified pipeline API with smart defaults.</p> <p>Designed for rapid prototyping and common use cases. Automatically detects: - Input columns from prompt template placeholders - Provider from model name (e.g., gpt-4 \u2192 openai, claude \u2192 anthropic) - Parser type (JSON for multi-column, text for single column) - Reasonable defaults for batch size, concurrency, retries</p> <p>Examples:</p> <p>Minimal usage:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"data.csv\",\n...     prompt=\"Categorize this text: {text}\"\n... )\n&gt;&gt;&gt; result = pipeline.execute()\n</code></pre> <p>With explicit outputs:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"products.csv\",\n...     prompt=\"Extract: {description}\",\n...     output_columns=[\"brand\", \"model\", \"price\"]\n... )\n</code></pre> <p>Override defaults:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=df,\n...     prompt=\"Summarize: {content}\",\n...     model=\"gpt-4o\",\n...     temperature=0.7,\n...     max_budget=Decimal(\"5.0\")\n... )\n</code></pre>"},{"location":"api/api/quick/#ondine.api.quick.QuickPipeline.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(data: str | Path | DataFrame, prompt: str, model: str = 'gpt-4o-mini', output_columns: list[str] | str | None = None, provider: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, max_budget: Decimal | float | str | None = None, batch_size: int | None = None, concurrency: int | None = None, **kwargs: Any) -&gt; Pipeline\n</code></pre> <p>Create a pipeline with smart defaults.</p> <p>The simplest way to process data with LLMs. Automatically detects input columns from prompt placeholders and configures optimal settings based on data size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path | DataFrame</code> <p>CSV/Excel/Parquet file path or DataFrame</p> required <code>prompt</code> <code>str</code> <p>Prompt template with {placeholders} matching column names</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o-mini). Provider auto-detected from model name.</p> <code>'gpt-4o-mini'</code> <code>output_columns</code> <code>list[str] | str | None</code> <p>Output column name(s). If None, uses [\"output\"]</p> <code>None</code> <code>provider</code> <code>str | None</code> <p>LLM provider. If None, auto-detected (gpt-4 \u2192 openai, claude \u2192 anthropic)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-1.0, default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens (optional, uses provider default)</p> <code>None</code> <code>max_budget</code> <code>Decimal | float | str | None</code> <p>Maximum cost budget in USD (optional, no limit if not set)</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Rows per batch (optional, auto-sized: 10-500 based on data size)</p> <code>None</code> <code>concurrency</code> <code>int | None</code> <p>Parallel requests (optional, auto-sized: 5-100 based on provider)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to PipelineBuilder</p> <code>{}</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data cannot be loaded or prompt is invalid</p> Example <pre><code>from ondine import QuickPipeline\n\n# Minimal - auto-detects everything\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Categorize: {description}\"\n)\nresult = pipeline.execute()\n\n# With budget control\npipeline = QuickPipeline.create(\n    data=\"reviews.csv\",\n    prompt=\"Sentiment: {review_text}\",\n    model=\"gpt-4o-mini\",\n    max_budget=5.0\n)\n\n# Multi-column output\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Extract from {title}: brand, price, category as JSON\",\n    output_columns=[\"brand\", \"price\", \"category\"]\n)\n\n# Custom provider\npipeline = QuickPipeline.create(\n    data=df,\n    prompt=\"Summarize: {text}\",\n    model=\"llama-3.3-70b-versatile\",\n    provider=\"groq\"\n)\n</code></pre> Note <p>Input columns are automatically detected from {placeholders} in the prompt. Provider is auto-detected from model name (gpt-4 \u2192 openai, claude \u2192 anthropic, llama \u2192 groq).</p> Source code in <code>ondine/api/quick.py</code> <pre><code>@staticmethod\ndef create(\n    data: str | Path | pd.DataFrame,\n    prompt: str,\n    model: str = \"gpt-4o-mini\",\n    output_columns: list[str] | str | None = None,\n    provider: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    max_budget: Decimal | float | str | None = None,\n    batch_size: int | None = None,\n    concurrency: int | None = None,\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"\n    Create a pipeline with smart defaults.\n\n    The simplest way to process data with LLMs. Automatically detects input columns\n    from prompt placeholders and configures optimal settings based on data size.\n\n    Args:\n        data: CSV/Excel/Parquet file path or DataFrame\n        prompt: Prompt template with {placeholders} matching column names\n        model: Model name (default: gpt-4o-mini). Provider auto-detected from model name.\n        output_columns: Output column name(s). If None, uses [\"output\"]\n        provider: LLM provider. If None, auto-detected (gpt-4 \u2192 openai, claude \u2192 anthropic)\n        temperature: Sampling temperature (0.0-1.0, default: 0.0 for deterministic)\n        max_tokens: Max output tokens (optional, uses provider default)\n        max_budget: Maximum cost budget in USD (optional, no limit if not set)\n        batch_size: Rows per batch (optional, auto-sized: 10-500 based on data size)\n        concurrency: Parallel requests (optional, auto-sized: 5-100 based on provider)\n        **kwargs: Additional arguments passed to PipelineBuilder\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If input data cannot be loaded or prompt is invalid\n\n    Example:\n        ```python\n        from ondine import QuickPipeline\n\n        # Minimal - auto-detects everything\n        pipeline = QuickPipeline.create(\n            data=\"products.csv\",\n            prompt=\"Categorize: {description}\"\n        )\n        result = pipeline.execute()\n\n        # With budget control\n        pipeline = QuickPipeline.create(\n            data=\"reviews.csv\",\n            prompt=\"Sentiment: {review_text}\",\n            model=\"gpt-4o-mini\",\n            max_budget=5.0\n        )\n\n        # Multi-column output\n        pipeline = QuickPipeline.create(\n            data=\"products.csv\",\n            prompt=\"Extract from {title}: brand, price, category as JSON\",\n            output_columns=[\"brand\", \"price\", \"category\"]\n        )\n\n        # Custom provider\n        pipeline = QuickPipeline.create(\n            data=df,\n            prompt=\"Summarize: {text}\",\n            model=\"llama-3.3-70b-versatile\",\n            provider=\"groq\"\n        )\n        ```\n\n    Note:\n        Input columns are automatically detected from {placeholders} in the prompt.\n        Provider is auto-detected from model name (gpt-4 \u2192 openai, claude \u2192 anthropic, llama \u2192 groq).\n    \"\"\"\n    # 1. Load data\n    df = QuickPipeline._load_data(data)\n\n    # 2. Auto-detect input columns from prompt template\n    input_columns = QuickPipeline._extract_placeholders(prompt)\n    if not input_columns:\n        raise ValueError(\n            f\"No placeholders found in prompt: {prompt}\\n\"\n            \"Expected format: 'Your prompt with {{column_name}} placeholders'\"\n        )\n\n    # Validate input columns exist in data\n    missing = [col for col in input_columns if col not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Input columns {missing} not found in data. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    # 3. Normalize output columns\n    if output_columns is None:\n        output_columns = [\"output\"]\n    elif isinstance(output_columns, str):\n        output_columns = [output_columns]\n\n    # 4. Auto-detect provider from model name\n    if provider is None:\n        provider = QuickPipeline._detect_provider(model)\n\n    # 5. Auto-select parser (JSON for multi-column, text for single)\n    parser = QuickPipeline._select_parser(output_columns)\n\n    # 6. Smart defaults for batch_size and concurrency\n    if batch_size is None:\n        batch_size = QuickPipeline._default_batch_size(len(df))\n    if concurrency is None:\n        concurrency = QuickPipeline._default_concurrency(provider)\n\n    # 7. Build pipeline\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df, input_columns=input_columns, output_columns=output_columns\n        )\n        .with_prompt(template=prompt)\n        .with_llm(\n            provider=provider,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n    )\n\n    # Add optional parser if multi-column\n    if parser:\n        builder = builder.with_parser(parser)\n\n    # Add batch/concurrency settings\n    # Note: QuickPipeline uses processing_batch_size (internal batching)\n    # not with_batch_size (multi-row batching)\n    builder = builder.with_processing_batch_size(batch_size).with_concurrency(\n        concurrency\n    )\n\n    # Add budget if specified\n    if max_budget is not None:\n        # Convert to float for PipelineBuilder (it expects float)\n        if isinstance(max_budget, Decimal | str):\n            max_budget = float(max_budget)\n        builder = builder.with_max_budget(budget=max_budget)\n\n    # Add sensible retry defaults\n    builder = builder.with_max_retries(3)\n\n    return builder.build()\n</code></pre>"},{"location":"api/cli/","title":"cli","text":""},{"location":"api/cli/#ondine.cli","title":"cli","text":"<p>Command-line interface for LLM Dataset Engine.</p>"},{"location":"api/cli/#ondine.cli.cli","title":"cli","text":"<pre><code>cli(ctx)\n</code></pre> <p>\ud83c\udf0a ONDINE - LLM Dataset Engine</p> <p>Process tabular datasets using LLMs with built-in reliability, cost control, and observability.</p> <p>Examples:</p> <pre><code># Process a dataset\nondine process --config config.yaml\n\n# Estimate cost before processing\nondine estimate --config config.yaml\n\n# Resume from checkpoint\nondine resume --session-id abc-123\n\n# Validate configuration\nondine validate --config config.yaml\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=__version__, prog_name=\"ondine\")\n@click.pass_context\ndef cli(ctx):\n    \"\"\"\n    \ud83c\udf0a ONDINE - LLM Dataset Engine\n\n    Process tabular datasets using LLMs with built-in reliability,\n    cost control, and observability.\n\n    Examples:\n\n        # Process a dataset\n        ondine process --config config.yaml\n\n        # Estimate cost before processing\n        ondine estimate --config config.yaml\n\n        # Resume from checkpoint\n        ondine resume --session-id abc-123\n\n        # Validate configuration\n        ondine validate --config config.yaml\n    \"\"\"\n    # Show banner only for main commands (not for --help)\n    if ctx.invoked_subcommand is not None:\n        show_banner()\n</code></pre>"},{"location":"api/cli/main/","title":"main","text":""},{"location":"api/cli/main/#ondine.cli.main","title":"main","text":"<p>Main CLI entry point for LLM Dataset Engine.</p> <p>Provides command-line interface for processing datasets, estimating costs, and managing pipeline execution.</p>"},{"location":"api/cli/main/#ondine.cli.main.show_banner","title":"show_banner","text":"<pre><code>show_banner()\n</code></pre> <p>Display the Ondine banner (centered, creative, robust).</p> Source code in <code>ondine/cli/main.py</code> <pre><code>def show_banner():\n    \"\"\"Display the Ondine banner (centered, creative, robust).\"\"\"\n    # Color gradient: cyan to magenta\n    lines = ONDINE_ART.strip().split(\"\\n\")\n    colored_lines = []\n    colors = [\n        \"bright_cyan\",\n        \"cyan\",\n        \"bright_blue\",\n        \"blue\",\n        \"bright_magenta\",\n        \"magenta\",\n        \"bright_magenta\",\n        \"blue\",\n        \"bright_blue\",\n        \"cyan\",\n        \"bright_cyan\",\n    ]\n\n    for i, line in enumerate(lines):\n        color = colors[i % len(colors)]\n        colored_lines.append(Text(line, style=f\"bold {color}\"))\n\n    title = Group(*colored_lines)\n    subtitle_text = Text(\"The LLM Dataset Engine\", style=\"dim italic bright_white\")\n    content = Group(title, \"\", subtitle_text)\n\n    console.print()\n    console.print(content)\n    console.print(\"[bold bright_cyan]\" + \"\u2500\" * 80 + \"[/bold bright_cyan]\")\n    console.print()\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.cli","title":"cli","text":"<pre><code>cli(ctx)\n</code></pre> <p>\ud83c\udf0a ONDINE - LLM Dataset Engine</p> <p>Process tabular datasets using LLMs with built-in reliability, cost control, and observability.</p> <p>Examples:</p> <pre><code># Process a dataset\nondine process --config config.yaml\n\n# Estimate cost before processing\nondine estimate --config config.yaml\n\n# Resume from checkpoint\nondine resume --session-id abc-123\n\n# Validate configuration\nondine validate --config config.yaml\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=__version__, prog_name=\"ondine\")\n@click.pass_context\ndef cli(ctx):\n    \"\"\"\n    \ud83c\udf0a ONDINE - LLM Dataset Engine\n\n    Process tabular datasets using LLMs with built-in reliability,\n    cost control, and observability.\n\n    Examples:\n\n        # Process a dataset\n        ondine process --config config.yaml\n\n        # Estimate cost before processing\n        ondine estimate --config config.yaml\n\n        # Resume from checkpoint\n        ondine resume --session-id abc-123\n\n        # Validate configuration\n        ondine validate --config config.yaml\n    \"\"\"\n    # Show banner only for main commands (not for --help)\n    if ctx.invoked_subcommand is not None:\n        show_banner()\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.process","title":"process","text":"<pre><code>process(config: Path, input: Path | None, output: Path | None, provider: str | None, model: str | None, max_budget: float | None, batch_size: int | None, concurrency: int | None, checkpoint_dir: Path | None, dry_run: bool, verbose: bool)\n</code></pre> <p>Process a dataset using LLM transformations.</p> <p>Reads data from config file. INPUT and OUTPUT flags override config values if provided.</p> <p>Examples:</p> <pre><code># Basic usage\nllm-dataset process -c config.yaml -i data.csv -o result.csv\n\n# Override provider and model\nllm-dataset process -c config.yaml -i data.csv -o result.csv \\\n    --provider groq --model openai/gpt-oss-120b\n\n# Set budget limit\nllm-dataset process -c config.yaml -i data.csv -o result.csv \\\n    --max-budget 10.0\n\n# Dry run (estimate only)\nllm-dataset process -c config.yaml -i data.csv -o result.csv --dry-run\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--config\",\n    \"-c\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to YAML/JSON configuration file\",\n)\n@click.option(\n    \"--input\",\n    \"-i\",\n    required=False,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to input data file (overrides config)\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    required=False,\n    type=click.Path(path_type=Path),\n    help=\"Path to output file (overrides config)\",\n)\n@click.option(\n    \"--provider\",\n    type=click.Choice([p.value for p in LLMProvider]),\n    help=\"Override LLM provider from config (use 'ondine list-providers' to see all)\",\n)\n@click.option(\n    \"--model\",\n    help=\"Override model name from config\",\n)\n@click.option(\n    \"--max-budget\",\n    type=float,\n    help=\"Override maximum budget (USD) from config\",\n)\n@click.option(\n    \"--batch-size\",\n    type=int,\n    help=\"Override batch size from config\",\n)\n@click.option(\n    \"--concurrency\",\n    type=int,\n    help=\"Override concurrency from config\",\n)\n@click.option(\n    \"--checkpoint-dir\",\n    type=click.Path(path_type=Path),\n    help=\"Override checkpoint directory from config\",\n)\n@click.option(\n    \"--dry-run\",\n    is_flag=True,\n    help=\"Validate and estimate only, don't execute\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Enable verbose logging\",\n)\ndef process(\n    config: Path,\n    input: Path | None,\n    output: Path | None,\n    provider: str | None,\n    model: str | None,\n    max_budget: float | None,\n    batch_size: int | None,\n    concurrency: int | None,\n    checkpoint_dir: Path | None,\n    dry_run: bool,\n    verbose: bool,\n):\n    \"\"\"\n    Process a dataset using LLM transformations.\n\n    Reads data from config file. INPUT and OUTPUT flags override config values if provided.\n\n    Examples:\n\n        # Basic usage\n        llm-dataset process -c config.yaml -i data.csv -o result.csv\n\n        # Override provider and model\n        llm-dataset process -c config.yaml -i data.csv -o result.csv \\\\\n            --provider groq --model openai/gpt-oss-120b\n\n        # Set budget limit\n        llm-dataset process -c config.yaml -i data.csv -o result.csv \\\\\n            --max-budget 10.0\n\n        # Dry run (estimate only)\n        llm-dataset process -c config.yaml -i data.csv -o result.csv --dry-run\n    \"\"\"\n    try:\n        # Load configuration\n        console.print(f\"[cyan]Loading configuration from {config}...[/cyan]\")\n        specs = ConfigLoader.from_yaml(str(config))\n\n        # Override with CLI arguments (if provided)\n        if input:\n            specs.dataset.source_path = input\n\n        # Set output configuration (if provided)\n        if output:\n            if specs.output:\n                specs.output.destination_path = output\n            else:\n                # Create output spec if not in config\n                from ondine.core.specifications import MergeStrategy, OutputSpec\n\n                # Detect output type from extension\n                output_suffix = output.suffix.lower()\n                if output_suffix == \".csv\":\n                    output_type = DataSourceType.CSV\n                elif output_suffix in [\".xlsx\", \".xls\"]:\n                    output_type = DataSourceType.EXCEL\n                elif output_suffix == \".parquet\":\n                    output_type = DataSourceType.PARQUET\n                else:\n                    output_type = DataSourceType.CSV  # Default\n\n                specs.output = OutputSpec(\n                    destination_type=output_type,\n                    destination_path=output,\n                    merge_strategy=MergeStrategy.REPLACE,\n                )\n\n        if provider:\n            specs.llm.provider = LLMProvider(provider)\n\n        if model:\n            specs.llm.model = model\n\n        if max_budget is not None:\n            from decimal import Decimal\n\n            specs.processing.max_budget = Decimal(str(max_budget))\n\n        if batch_size is not None:\n            specs.processing.batch_size = batch_size\n\n        if concurrency is not None:\n            specs.processing.concurrency = concurrency\n\n        if checkpoint_dir is not None:\n            specs.processing.checkpoint_dir = checkpoint_dir\n\n        # Create pipeline\n        console.print(\"[cyan]Creating pipeline...[/cyan]\")\n        pipeline = Pipeline(specs)\n\n        # Validate\n        console.print(\"[cyan]Validating pipeline...[/cyan]\")\n        validation = pipeline.validate()\n\n        if not validation.is_valid:\n            console.print(\"[red]\u274c Validation failed:[/red]\")\n            for error in validation.errors:\n                console.print(f\"  [red]\u2022 {error}[/red]\")\n            sys.exit(1)\n\n        console.print(\"[green]\u2705 Validation passed[/green]\")\n\n        # Estimate cost\n        console.print(\"\\n[cyan]Estimating cost...[/cyan]\")\n        estimate = pipeline.estimate_cost()\n\n        table = Table(title=\"Cost Estimate\")\n        table.add_column(\"Metric\", style=\"cyan\")\n        table.add_column(\"Value\", style=\"green\")\n\n        table.add_row(\"Total Cost\", f\"${estimate.total_cost}\")\n        table.add_row(\"Total Tokens\", f\"{estimate.total_tokens:,}\")\n        table.add_row(\"Input Tokens\", f\"{estimate.input_tokens:,}\")\n        table.add_row(\"Output Tokens\", f\"{estimate.output_tokens:,}\")\n        table.add_row(\"Rows\", f\"{estimate.rows:,}\")\n\n        console.print(table)\n\n        if dry_run:\n            console.print(\"\\n[yellow]Dry run mode - skipping execution[/yellow]\")\n            return\n\n        # Execute\n        console.print(\"\\n[cyan]Processing dataset...[/cyan]\")\n        result = pipeline.execute()\n\n        # Display results\n        console.print(\"\\n[green]\u2705 Processing complete![/green]\")\n\n        results_table = Table(title=\"Execution Results\")\n        results_table.add_column(\"Metric\", style=\"cyan\")\n        results_table.add_column(\"Value\", style=\"green\")\n\n        results_table.add_row(\"Total Rows\", str(result.metrics.total_rows))\n        results_table.add_row(\"Processed\", str(result.metrics.processed_rows))\n        results_table.add_row(\"Failed\", str(result.metrics.failed_rows))\n        results_table.add_row(\"Skipped\", str(result.metrics.skipped_rows))\n        results_table.add_row(\"Duration\", f\"{result.duration:.2f}s\")\n        results_table.add_row(\"Total Cost\", f\"${result.costs.total_cost}\")\n        results_table.add_row(\n            \"Cost per Row\",\n            f\"${result.costs.total_cost / result.metrics.total_rows:.6f}\",\n        )\n\n        console.print(results_table)\n\n        # Validate output quality\n        console.print(\"\\n[cyan]\ud83d\udcca Validating output quality...[/cyan]\")\n        quality = result.validate_output_quality(specs.dataset.output_columns)\n\n        quality_table = Table(title=\"Quality Report\")\n        quality_table.add_column(\"Metric\", style=\"cyan\")\n        quality_table.add_column(\"Value\", style=\"green\")\n\n        quality_table.add_row(\n            \"Valid Outputs\", f\"{quality.valid_outputs}/{quality.total_rows}\"\n        )\n        quality_table.add_row(\"Success Rate\", f\"{quality.success_rate:.1f}%\")\n        quality_table.add_row(\"Null Outputs\", str(quality.null_outputs))\n        quality_table.add_row(\"Empty Outputs\", str(quality.empty_outputs))\n\n        # Color-code quality score\n        score_color = (\n            \"green\"\n            if quality.quality_score in [\"excellent\", \"good\"]\n            else \"yellow\"\n            if quality.quality_score == \"poor\"\n            else \"red\"\n        )\n        quality_table.add_row(\n            \"Quality Score\",\n            f\"[{score_color}]{quality.quality_score.upper()}[/{score_color}]\",\n        )\n\n        console.print(quality_table)\n\n        # Display warnings and issues\n        if quality.warnings:\n            console.print(\"\\n[yellow]\u26a0\ufe0f  Warnings:[/yellow]\")\n            for warning in quality.warnings:\n                console.print(f\"  [yellow]\u2022 {warning}[/yellow]\")\n\n        if quality.issues:\n            console.print(\"\\n[red]\ud83d\udea8 Issues Detected:[/red]\")\n            for issue in quality.issues:\n                console.print(f\"  [red]\u2022 {issue}[/red]\")\n            console.print(\"\\n[red]Consider:[/red]\")\n            console.print(\n                \"  [dim]\u2022 Review your prompt complexity (simpler prompts often work better)[/dim]\"\n            )\n            console.print(\"  [dim]\u2022 Check LLM provider logs for errors[/dim]\")\n            console.print(\"  [dim]\u2022 Increase max_tokens if outputs are truncated[/dim]\")\n            console.print(\"  [dim]\u2022 Verify API key and rate limits[/dim]\")\n\n        if quality.is_acceptable:\n            console.print(\n                f\"\\n[green]\u2705 Output quality is acceptable ({quality.success_rate:.1f}% success)[/green]\"\n            )\n        else:\n            console.print(\n                f\"\\n[red]\u274c Output quality is below acceptable threshold ({quality.success_rate:.1f}% &lt; 70%)[/red]\"\n            )\n\n        console.print(\n            f\"\\n[green]Output written to: {specs.output.destination_path}[/green]\"\n        )\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        if verbose:\n            console.print_exception()\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.estimate","title":"estimate","text":"<pre><code>estimate(config: Path, input: Path, provider: str | None, model: str | None)\n</code></pre> <p>Estimate processing cost without executing.</p> <p>Useful for budget planning and cost validation before running expensive operations.</p> <p>Examples:</p> <pre><code># Estimate cost\nllm-dataset estimate -c config.yaml -i data.csv\n\n# Estimate with different model\nllm-dataset estimate -c config.yaml -i data.csv --model gpt-4o\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--config\",\n    \"-c\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to YAML/JSON configuration file\",\n)\n@click.option(\n    \"--input\",\n    \"-i\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to input data file\",\n)\n@click.option(\n    \"--provider\",\n    type=click.Choice([p.value for p in LLMProvider]),\n    help=\"Override LLM provider from config (use 'ondine list-providers' to see all)\",\n)\n@click.option(\n    \"--model\",\n    help=\"Override model name from config\",\n)\ndef estimate(\n    config: Path,\n    input: Path,\n    provider: str | None,\n    model: str | None,\n):\n    \"\"\"\n    Estimate processing cost without executing.\n\n    Useful for budget planning and cost validation before running\n    expensive operations.\n\n    Examples:\n\n        # Estimate cost\n        llm-dataset estimate -c config.yaml -i data.csv\n\n        # Estimate with different model\n        llm-dataset estimate -c config.yaml -i data.csv --model gpt-4o\n    \"\"\"\n    try:\n        # Load configuration\n        console.print(f\"[cyan]Loading configuration from {config}...[/cyan]\")\n        specs = ConfigLoader.from_yaml(str(config))\n\n        # Override\n        specs.dataset.source_path = input\n\n        if provider:\n            specs.llm.provider = LLMProvider(provider)\n\n        if model:\n            specs.llm.model = model\n\n        # Create pipeline\n        pipeline = Pipeline(specs)\n\n        # Validate\n        validation = pipeline.validate()\n        if not validation.is_valid:\n            console.print(\"[red]\u274c Validation failed:[/red]\")\n            for error in validation.errors:\n                console.print(f\"  [red]\u2022 {error}[/red]\")\n            sys.exit(1)\n\n        # Estimate\n        console.print(\"[cyan]Estimating cost...[/cyan]\")\n        estimate = pipeline.estimate_cost()\n\n        # Display results\n        table = Table(title=\"Cost Estimate\", show_header=True)\n        table.add_column(\"Metric\", style=\"cyan\", width=20)\n        table.add_column(\"Value\", style=\"green\", width=20)\n\n        table.add_row(\"Total Cost\", f\"${estimate.total_cost}\")\n        table.add_row(\"Total Tokens\", f\"{estimate.total_tokens:,}\")\n        table.add_row(\"Input Tokens\", f\"{estimate.input_tokens:,}\")\n        table.add_row(\"Output Tokens\", f\"{estimate.output_tokens:,}\")\n        table.add_row(\"Rows to Process\", f\"{estimate.rows:,}\")\n        table.add_row(\"Confidence\", estimate.confidence)\n\n        console.print(\"\\n\")\n        console.print(table)\n\n        # Cost per row\n        if estimate.rows &gt; 0:\n            cost_per_row = estimate.total_cost / estimate.rows\n            console.print(f\"\\n[cyan]Cost per row: ${cost_per_row:.6f}[/cyan]\")\n\n        # Warning if expensive\n        if estimate.total_cost &gt; 10.0:\n            console.print(\n                f\"\\n[yellow]\u26a0\ufe0f  Warning: Estimated cost (${estimate.total_cost}) exceeds $10[/yellow]\"\n            )\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.resume","title":"resume","text":"<pre><code>resume(session_id: str, checkpoint_dir: Path, output: Path | None)\n</code></pre> <p>Resume pipeline execution from checkpoint.</p> <p>Useful for recovering from failures or continuing interrupted processing.</p> <p>Examples:</p> <pre><code># Resume from checkpoint\nllm-dataset resume --session-id abc-123-def\n\n# Resume with custom checkpoint directory\nllm-dataset resume -s abc-123 --checkpoint-dir /path/to/checkpoints\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--session-id\",\n    \"-s\",\n    required=True,\n    help=\"Session ID to resume (UUID)\",\n)\n@click.option(\n    \"--checkpoint-dir\",\n    type=click.Path(exists=True, path_type=Path),\n    default=\".checkpoints\",\n    help=\"Checkpoint directory (default: .checkpoints)\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    type=click.Path(path_type=Path),\n    help=\"Override output path\",\n)\ndef resume(\n    session_id: str,\n    checkpoint_dir: Path,\n    output: Path | None,\n):\n    \"\"\"\n    Resume pipeline execution from checkpoint.\n\n    Useful for recovering from failures or continuing interrupted processing.\n\n    Examples:\n\n        # Resume from checkpoint\n        llm-dataset resume --session-id abc-123-def\n\n        # Resume with custom checkpoint directory\n        llm-dataset resume -s abc-123 --checkpoint-dir /path/to/checkpoints\n    \"\"\"\n    try:\n        from ondine.adapters import LocalFileCheckpointStorage\n        from ondine.orchestration import StateManager\n\n        # Load checkpoint\n        console.print(f\"[cyan]Looking for checkpoint in {checkpoint_dir}...[/cyan]\")\n\n        storage = LocalFileCheckpointStorage(str(checkpoint_dir))\n        state_manager = StateManager(storage)\n\n        session_uuid = UUID(session_id)\n\n        if not state_manager.can_resume(session_uuid):\n            console.print(f\"[red]\u274c No checkpoint found for session {session_id}[/red]\")\n            console.print(\n                f\"[yellow]Check checkpoint directory: {checkpoint_dir}[/yellow]\"\n            )\n            sys.exit(1)\n\n        # Load checkpoint\n        checkpoint_info = state_manager.get_latest_checkpoint(session_uuid)\n        console.print(\n            f\"[green]\u2705 Found checkpoint at row {checkpoint_info.row_index}[/green]\"\n        )\n\n        # Resume execution\n        console.print(\"[cyan]Resuming execution...[/cyan]\")\n\n        # Note: Full resume implementation would load the original pipeline\n        # and continue from checkpoint. For now, we show the checkpoint info.\n        console.print(\n            \"\\n[yellow]\u26a0\ufe0f  Full resume functionality requires the original pipeline configuration[/yellow]\"\n        )\n        console.print(\n            \"[yellow]Please use Pipeline.execute(resume_from=session_id) in Python code[/yellow]\"\n        )\n\n        # Display checkpoint info\n        table = Table(title=\"Checkpoint Information\")\n        table.add_column(\"Property\", style=\"cyan\")\n        table.add_column(\"Value\", style=\"green\")\n\n        table.add_row(\"Session ID\", str(checkpoint_info.session_id))\n        table.add_row(\"Checkpoint Path\", checkpoint_info.checkpoint_path)\n        table.add_row(\"Last Row\", str(checkpoint_info.row_index))\n        table.add_row(\"Last Stage\", str(checkpoint_info.stage_index))\n        table.add_row(\"Timestamp\", str(checkpoint_info.timestamp))\n        table.add_row(\"Size\", f\"{checkpoint_info.size_bytes:,} bytes\")\n\n        console.print(table)\n\n    except ValueError:\n        console.print(f\"[red]\u274c Invalid session ID format: {session_id}[/red]\")\n        console.print(\n            \"[yellow]Session ID should be a UUID (e.g., abc-123-def-456)[/yellow]\"\n        )\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.validate","title":"validate","text":"<pre><code>validate(config: Path, verbose: bool)\n</code></pre> <p>Validate pipeline configuration.</p> <p>Checks configuration file for errors and warnings without executing the pipeline.</p> <p>Examples:</p> <pre><code># Validate configuration\nllm-dataset validate -c config.yaml\n\n# Verbose validation\nllm-dataset validate -c config.yaml --verbose\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--config\",\n    \"-c\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to YAML/JSON configuration file\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Show detailed validation results\",\n)\ndef validate(config: Path, verbose: bool):\n    \"\"\"\n    Validate pipeline configuration.\n\n    Checks configuration file for errors and warnings without executing\n    the pipeline.\n\n    Examples:\n\n        # Validate configuration\n        llm-dataset validate -c config.yaml\n\n        # Verbose validation\n        llm-dataset validate -c config.yaml --verbose\n    \"\"\"\n    try:\n        # Load configuration\n        console.print(f\"[cyan]Loading configuration from {config}...[/cyan]\")\n        specs = ConfigLoader.from_yaml(str(config))\n\n        console.print(\"[green]\u2705 Configuration loaded successfully[/green]\")\n\n        # Display configuration summary\n        if verbose:\n            table = Table(title=\"Configuration Summary\")\n            table.add_column(\"Component\", style=\"cyan\")\n            table.add_column(\"Details\", style=\"green\")\n\n            table.add_row(\"Dataset\", f\"{specs.dataset.source_type.value}\")\n            table.add_row(\"Input Columns\", \", \".join(specs.dataset.input_columns))\n            table.add_row(\"Output Columns\", \", \".join(specs.dataset.output_columns))\n            table.add_row(\"LLM Provider\", specs.llm.provider.value)\n            table.add_row(\"Model\", specs.llm.model)\n            table.add_row(\"Batch Size\", str(specs.processing.batch_size))\n            table.add_row(\"Concurrency\", str(specs.processing.concurrency))\n\n            if specs.processing.max_budget:\n                table.add_row(\"Max Budget\", f\"${specs.processing.max_budget}\")\n\n            console.print(\"\\n\")\n            console.print(table)\n\n        # Create pipeline for validation\n        console.print(\"\\n[cyan]Validating pipeline...[/cyan]\")\n        pipeline = Pipeline(specs)\n        validation = pipeline.validate()\n\n        if validation.is_valid:\n            console.print(\"[green]\u2705 Pipeline configuration is valid[/green]\")\n\n            if validation.warnings:\n                console.print(\"\\n[yellow]Warnings:[/yellow]\")\n                for warning in validation.warnings:\n                    console.print(f\"  [yellow]\u2022 {warning}[/yellow]\")\n        else:\n            console.print(\"[red]\u274c Pipeline configuration is invalid[/red]\")\n            console.print(\"\\n[red]Errors:[/red]\")\n            for error in validation.errors:\n                console.print(f\"  [red]\u2022 {error}[/red]\")\n\n            if validation.warnings:\n                console.print(\"\\n[yellow]Warnings:[/yellow]\")\n                for warning in validation.warnings:\n                    console.print(f\"  [yellow]\u2022 {warning}[/yellow]\")\n\n            sys.exit(1)\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints(checkpoint_dir: Path)\n</code></pre> <p>List available checkpoints.</p> <p>Shows all saved checkpoints in the specified directory.</p> <p>Examples:</p> <pre><code># List checkpoints\nllm-dataset list-checkpoints\n\n# List from custom directory\nllm-dataset list-checkpoints --checkpoint-dir /path/to/checkpoints\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--checkpoint-dir\",\n    type=click.Path(exists=True, path_type=Path),\n    default=\".checkpoints\",\n    help=\"Checkpoint directory to list (default: .checkpoints)\",\n)\ndef list_checkpoints(checkpoint_dir: Path):\n    \"\"\"\n    List available checkpoints.\n\n    Shows all saved checkpoints in the specified directory.\n\n    Examples:\n\n        # List checkpoints\n        llm-dataset list-checkpoints\n\n        # List from custom directory\n        llm-dataset list-checkpoints --checkpoint-dir /path/to/checkpoints\n    \"\"\"\n    try:\n        from ondine.adapters import LocalFileCheckpointStorage\n\n        console.print(f\"[cyan]Scanning {checkpoint_dir} for checkpoints...[/cyan]\")\n\n        storage = LocalFileCheckpointStorage(checkpoint_dir)\n        checkpoints = storage.list_checkpoints()\n\n        if not checkpoints:\n            console.print(\"[yellow]No checkpoints found[/yellow]\")\n            return\n\n        # Display checkpoints\n        table = Table(title=f\"Checkpoints in {checkpoint_dir}\")\n        table.add_column(\"Session ID\", style=\"cyan\")\n        table.add_column(\"Row\", style=\"green\")\n        table.add_column(\"Stage\", style=\"green\")\n        table.add_column(\"Timestamp\", style=\"yellow\")\n        table.add_column(\"Size\", style=\"magenta\")\n\n        for cp in checkpoints:\n            table.add_row(\n                str(cp.session_id)[:8] + \"...\",\n                str(cp.row_index),\n                str(cp.stage_index),\n                cp.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                f\"{cp.size_bytes:,} bytes\",\n            )\n\n        console.print(\"\\n\")\n        console.print(table)\n        console.print(f\"\\n[cyan]Total checkpoints: {len(checkpoints)}[/cyan]\")\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.inspect","title":"inspect","text":"<pre><code>inspect(input: Path, head: int)\n</code></pre> <p>Inspect input data file.</p> <p>Shows file info and preview of first N rows.</p> <p>Examples:</p> <pre><code># Inspect CSV file\nllm-dataset inspect -i data.csv\n\n# Show first 10 rows\nllm-dataset inspect -i data.csv --head 10\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--input\",\n    \"-i\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to input file to inspect\",\n)\n@click.option(\n    \"--head\",\n    type=int,\n    default=5,\n    help=\"Number of rows to show (default: 5)\",\n)\ndef inspect(input: Path, head: int):\n    \"\"\"\n    Inspect input data file.\n\n    Shows file info and preview of first N rows.\n\n    Examples:\n\n        # Inspect CSV file\n        llm-dataset inspect -i data.csv\n\n        # Show first 10 rows\n        llm-dataset inspect -i data.csv --head 10\n    \"\"\"\n    try:\n        import pandas as pd\n\n        console.print(f\"[cyan]Inspecting {input}...[/cyan]\")\n\n        # Detect file type\n        suffix = input.suffix.lower()\n\n        if suffix == \".csv\":\n            df = pd.read_csv(input)\n        elif suffix in [\".xlsx\", \".xls\"]:\n            df = pd.read_excel(input)\n        elif suffix == \".parquet\":\n            df = pd.read_parquet(input)\n        else:\n            console.print(f\"[red]\u274c Unsupported file type: {suffix}[/red]\")\n            sys.exit(1)\n\n        # File info\n        info_table = Table(title=\"File Information\")\n        info_table.add_column(\"Property\", style=\"cyan\")\n        info_table.add_column(\"Value\", style=\"green\")\n\n        info_table.add_row(\"File Path\", str(input))\n        info_table.add_row(\"File Type\", suffix[1:].upper())\n        info_table.add_row(\"Total Rows\", f\"{len(df):,}\")\n        info_table.add_row(\"Total Columns\", str(len(df.columns)))\n        info_table.add_row(\n            \"Memory Usage\", f\"{df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\"\n        )\n\n        console.print(\"\\n\")\n        console.print(info_table)\n\n        # Columns\n        console.print(\"\\n[cyan]Columns:[/cyan]\")\n        for col in df.columns:\n            dtype = df[col].dtype\n            null_count = df[col].isnull().sum()\n            console.print(f\"  \u2022 {col} ({dtype}) - {null_count} nulls\")\n\n        # Preview\n        console.print(f\"\\n[cyan]First {head} rows:[/cyan]\")\n        console.print(df.head(head).to_string())\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.list_providers","title":"list_providers","text":"<pre><code>list_providers()\n</code></pre> <p>List all available LLM providers with details.</p> <p>Shows supported providers, their platforms, costs, and requirements.</p> <p>Examples:</p> <pre><code># List all providers\nondine list-providers\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\ndef list_providers():\n    \"\"\"\n    List all available LLM providers with details.\n\n    Shows supported providers, their platforms, costs, and requirements.\n\n    Examples:\n\n        # List all providers\n        ondine list-providers\n    \"\"\"\n    try:\n        # Create table\n        table = Table(title=\"\ud83e\udebd Available LLM Providers\", show_header=True)\n        table.add_column(\"Provider ID\", style=\"cyan\", width=20)\n        table.add_column(\"Name\", style=\"bright_white\", width=20)\n        table.add_column(\"Platform\", style=\"yellow\", width=25)\n        table.add_column(\"Cost\", style=\"magenta\", width=12)\n        table.add_column(\"Use Case\", style=\"white\", width=35)\n\n        # Add rows for each provider\n        for provider in LLMProvider:\n            metadata = PROVIDER_METADATA[provider]\n\n            # Color-code cost\n            cost = metadata[\"cost\"]\n            if \"Free\" in cost or cost == \"Varies\":\n                cost_colored = f\"[green]{cost}[/green]\"\n            elif cost == \"$$\":\n                cost_colored = f\"[yellow]{cost}[/yellow]\"\n            else:  # $$$\n                cost_colored = f\"[red]{cost}[/red]\"\n\n            table.add_row(\n                f\"[bold]{provider.value}[/bold]\",\n                metadata[\"name\"],\n                metadata[\"platform\"],\n                cost_colored,\n                metadata[\"use_case\"],\n            )\n\n        console.print(\"\\n\")\n        console.print(table)\n\n        # Requirements section\n        console.print(\"\\n[cyan]\ud83d\udccb Requirements by Provider:[/cyan]\")\n        for provider in LLMProvider:\n            metadata = PROVIDER_METADATA[provider]\n            console.print(\n                f\"  [bold cyan]{provider.value}[/bold cyan]: {metadata['requirements']}\"\n            )\n\n        # Usage examples\n        console.print(\"\\n[cyan]\ud83d\udca1 Usage Examples:[/cyan]\")\n        console.print(\"  [dim]# Use OpenAI[/dim]\")\n        console.print(\"  ondine process --provider openai --config config.yaml\")\n        console.print(\"\\n  [dim]# Use local MLX on Apple Silicon[/dim]\")\n        console.print(\"  ondine process --provider mlx --config config.yaml\")\n        console.print(\"\\n  [dim]# Use custom API (Ollama, vLLM, Together.AI)[/dim]\")\n        console.print(\n            \"  ondine process --provider openai_compatible --config config.yaml\"\n        )\n        console.print(\n            \"\\n  [dim]\ud83d\udca1 Tip: Set provider in your YAML config file or use --provider flag[/dim]\\n\"\n        )\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/config/","title":"config","text":""},{"location":"api/config/#ondine.config","title":"config","text":"<p>Configuration loading from files.</p>"},{"location":"api/config/#ondine.config.ConfigLoader","title":"ConfigLoader","text":"<p>Loads pipeline configurations from YAML or JSON files.</p> <p>Follows Single Responsibility: only handles config file loading.</p>"},{"location":"api/config/#ondine.config.ConfigLoader.from_yaml","title":"from_yaml  <code>staticmethod</code>","text":"<pre><code>from_yaml(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to YAML file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid YAML or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_yaml(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from YAML file.\n\n    Args:\n        file_path: Path to YAML file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid YAML or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = yaml.safe_load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/#ondine.config.ConfigLoader.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to JSON file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid JSON or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_json(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid JSON or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = json.load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/#ondine.config.ConfigLoader.to_yaml","title":"to_yaml  <code>staticmethod</code>","text":"<pre><code>to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to YAML file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        yaml.dump(config_dict, f, default_flow_style=False, indent=2)\n</code></pre>"},{"location":"api/config/#ondine.config.ConfigLoader.to_json","title":"to_json  <code>staticmethod</code>","text":"<pre><code>to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to JSON file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        json.dump(config_dict, f, indent=2, default=str)\n</code></pre>"},{"location":"api/config/config_loader/","title":"config_loader","text":""},{"location":"api/config/config_loader/#ondine.config.config_loader","title":"config_loader","text":"<p>Configuration loader for YAML and JSON files.</p> <p>Enables loading pipeline configurations from declarative files.</p>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader","title":"ConfigLoader","text":"<p>Loads pipeline configurations from YAML or JSON files.</p> <p>Follows Single Responsibility: only handles config file loading.</p>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.from_yaml","title":"from_yaml  <code>staticmethod</code>","text":"<pre><code>from_yaml(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to YAML file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid YAML or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_yaml(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from YAML file.\n\n    Args:\n        file_path: Path to YAML file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid YAML or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = yaml.safe_load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to JSON file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid JSON or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_json(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid JSON or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = json.load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.to_yaml","title":"to_yaml  <code>staticmethod</code>","text":"<pre><code>to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to YAML file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        yaml.dump(config_dict, f, default_flow_style=False, indent=2)\n</code></pre>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.to_json","title":"to_json  <code>staticmethod</code>","text":"<pre><code>to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to JSON file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        json.dump(config_dict, f, indent=2, default=str)\n</code></pre>"},{"location":"api/core/","title":"core","text":""},{"location":"api/core/#ondine.core","title":"core","text":"<p>Core configuration and data models.</p>"},{"location":"api/core/#ondine.core.ConfigurationError","title":"ConfigurationError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>Invalid configuration or missing required resources.</p> <p>Raised when: - Input file doesn't exist - Invalid parameter values - Missing required configuration - Incompatible settings</p> Example <pre><code># This will raise ConfigurationError if file doesn't exist\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"nonexistent.csv\", ...)\n    .build()\n)\nresult = pipeline.execute()  # Fails here\n</code></pre> Resolution <p>Fix configuration or ensure required files/resources exist.</p>"},{"location":"api/core/#ondine.core.InvalidAPIKeyError","title":"InvalidAPIKeyError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>API key is invalid, missing, or authentication failed.</p> <p>Raised when: - API key environment variable not set - API key is invalid or expired - Authentication fails (401, 403) - Insufficient permissions</p> Example <pre><code># This will raise InvalidAPIKeyError if OPENAI_API_KEY not set\npipeline = (\n    PipelineBuilder.create()\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\nresult = pipeline.execute()  # Fails here if no API key\n</code></pre> Resolution <p>Set valid API key in environment variable or configuration.</p>"},{"location":"api/core/#ondine.core.ModelNotFoundError","title":"ModelNotFoundError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>Model doesn't exist or has been decommissioned.</p> <p>Raised when: - LLM provider returns \"model not found\" error - Model has been decommissioned/deprecated - Invalid model name specified</p> Example <pre><code># This will raise ModelNotFoundError if model is invalid\npipeline = (\n    PipelineBuilder.create()\n    .with_llm(provider=\"groq\", model=\"llama-3.1-70b-versatile\")  # Decommissioned\n    .build()\n)\n</code></pre> Resolution <p>Update model name to a valid, current model.</p>"},{"location":"api/core/#ondine.core.NonRetryableError","title":"NonRetryableError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors that should not be retried.</p> <p>These are fatal errors that indicate configuration issues, invalid inputs, or permanent failures that cannot be resolved by retrying the operation.</p> <p>Examples of non-retryable errors: - Model doesn't exist or has been decommissioned - Invalid API key or authentication failure - File not found or invalid configuration - Quota/credits exhausted (not rate limit)</p> <p>Retryable errors (handled separately): - Rate limit errors (429) - Network timeouts or connection errors - Temporary API unavailability (503)</p> Example <pre><code>try:\n    result = pipeline.execute()\nexcept NonRetryableError as e:\n    # Fatal error - don't retry, fix configuration\n    logger.error(f\"Pipeline failed with fatal error: {e}\")\n    sys.exit(1)\n</code></pre>"},{"location":"api/core/#ondine.core.QuotaExceededError","title":"QuotaExceededError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>API quota or credits exhausted.</p> <p>Raised when: - Account credits/quota exhausted - Billing issue or payment required - Account suspended</p> Note <p>This is different from rate limiting (429), which is retryable. Quota errors indicate a permanent issue until credits are added.</p> Example <pre><code>try:\n    result = pipeline.execute()\nexcept QuotaExceededError:\n    # Need to add credits or upgrade plan\n    logger.error(\"Account quota exceeded - add credits\")\n</code></pre> Resolution <p>Add credits, upgrade plan, or resolve billing issues.</p>"},{"location":"api/core/#ondine.core.CheckpointInfo","title":"CheckpointInfo  <code>dataclass</code>","text":"<pre><code>CheckpointInfo(session_id: UUID, checkpoint_path: str, row_index: int, stage_index: int, timestamp: datetime, size_bytes: int)\n</code></pre> <p>Information about a checkpoint.</p>"},{"location":"api/core/#ondine.core.CostEstimate","title":"CostEstimate  <code>dataclass</code>","text":"<pre><code>CostEstimate(total_cost: Decimal, total_tokens: int, input_tokens: int, output_tokens: int, rows: int, breakdown_by_stage: dict[str, Decimal] = dict(), confidence: str = 'estimate')\n</code></pre> <p>Cost estimation for pipeline execution.</p> <p>Provides detailed cost breakdown with Decimal precision to avoid floating-point errors.</p> <p>Attributes:</p> Name Type Description <code>total_cost</code> <code>Decimal</code> <p>Total cost in USD (Decimal for precision)</p> <code>total_tokens</code> <code>int</code> <p>Total tokens consumed (input + output)</p> <code>input_tokens</code> <code>int</code> <p>Input tokens sent to LLM</p> <code>output_tokens</code> <code>int</code> <p>Output tokens generated by LLM</p> <code>rows</code> <code>int</code> <p>Number of rows processed</p> <code>breakdown_by_stage</code> <code>dict[str, Decimal]</code> <p>Cost breakdown by pipeline stage</p> <code>confidence</code> <code>str</code> <p>Confidence level (estimate, sample-based, actual)</p> Example <pre><code>result = pipeline.execute()\n\n# Access costs\nprint(f\"Total: ${result.costs.total_cost}\")\nprint(f\"Input tokens: {result.costs.input_tokens:,}\")\nprint(f\"Output tokens: {result.costs.output_tokens:,}\")\nprint(f\"Cost per row: ${result.costs.total_cost / result.costs.rows:.4f}\")\n</code></pre>"},{"location":"api/core/#ondine.core.ErrorInfo","title":"ErrorInfo  <code>dataclass</code>","text":"<pre><code>ErrorInfo(row_index: int, stage_name: str, error_type: str, error_message: str, timestamp: datetime, context: dict[str, Any] = dict())\n</code></pre> <p>Information about an error during processing.</p>"},{"location":"api/core/#ondine.core.ExecutionResult","title":"ExecutionResult  <code>dataclass</code>","text":"<pre><code>ExecutionResult(data: DataFrame, metrics: ProcessingStats, costs: CostEstimate, errors: list[ErrorInfo] = list(), execution_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, success: bool = True, metadata: dict[str, Any] = dict())\n</code></pre> <p>Complete result from pipeline execution.</p> <p>Contains all output data, metrics, costs, and execution metadata from a pipeline run.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>DataFrame with input data and generated output columns</p> <code>metrics</code> <code>ProcessingStats</code> <p>Processing statistics (total_rows, success_count, failed_rows, etc.)</p> <code>costs</code> <code>CostEstimate</code> <p>Cost breakdown (total_cost, input_tokens, output_tokens)</p> <code>errors</code> <code>list[ErrorInfo]</code> <p>List of errors encountered during execution</p> <code>execution_id</code> <code>UUID</code> <p>Unique ID for this execution session</p> <code>start_time</code> <code>datetime</code> <p>When execution started</p> <code>end_time</code> <code>datetime | None</code> <p>When execution completed (None if still running)</p> <code>success</code> <code>bool</code> <p>Whether execution completed successfully</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional execution metadata</p> Example <pre><code>result = pipeline.execute()\n\n# Access output data\nprint(result.data.head())\nresult.data.to_csv(\"output.csv\")\n\n# Check metrics\nprint(f\"Total: {result.metrics.total_rows}\")\nprint(f\"Success: {result.metrics.success_count}\")\nprint(f\"Failed: {result.metrics.failed_rows}\")\n\n# Check costs\nprint(f\"Cost: ${result.costs.total_cost}\")\nprint(f\"Tokens: {result.costs.total_tokens}\")\n\n# Check execution time\nprint(f\"Duration: {result.duration:.2f}s\")\n</code></pre>"},{"location":"api/core/#ondine.core.ExecutionResult.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Get execution duration in seconds.</p>"},{"location":"api/core/#ondine.core.ExecutionResult.error_rate","title":"error_rate  <code>property</code>","text":"<pre><code>error_rate: float\n</code></pre> <p>Get error rate as percentage.</p>"},{"location":"api/core/#ondine.core.ExecutionResult.validate_output_quality","title":"validate_output_quality","text":"<pre><code>validate_output_quality(output_columns: list[str]) -&gt; QualityReport\n</code></pre> <p>Validate the quality of output data by checking for null/empty values.</p> <p>Parameters:</p> Name Type Description Default <code>output_columns</code> <code>list[str]</code> <p>List of output column names to check</p> required <p>Returns:</p> Type Description <code>QualityReport</code> <p>QualityReport with quality metrics and warnings</p> Source code in <code>ondine/core/models.py</code> <pre><code>def validate_output_quality(self, output_columns: list[str]) -&gt; \"QualityReport\":\n    \"\"\"\n    Validate the quality of output data by checking for null/empty values.\n\n    Args:\n        output_columns: List of output column names to check\n\n    Returns:\n        QualityReport with quality metrics and warnings\n    \"\"\"\n    total_rows = len(self.data)\n    total_columns = len(output_columns)\n    total_cells = total_rows * total_columns\n\n    # Count null and empty values across ALL output columns\n    null_count = 0\n    empty_count = 0\n\n    for col in output_columns:\n        if col in self.data.columns:\n            # Count nulls (None, NaN, NaT)\n            null_count += self.data[col].isna().sum()\n            # Count empty strings (only for string columns)\n            if self.data[col].dtype == \"object\":\n                empty_count += (self.data[col].astype(str).str.strip() == \"\").sum()\n\n    # Count rows with at least one valid output column\n    # A row is \"valid\" if at least one output column has non-null, non-empty data\n    valid_row_mask = False\n    for col in output_columns:\n        if col in self.data.columns:\n            non_null = ~self.data[col].isna()\n            if self.data[col].dtype == \"object\":\n                non_empty = self.data[col].astype(str).str.strip() != \"\"\n                valid_row_mask |= non_null &amp; non_empty\n            else:\n                valid_row_mask |= non_null\n\n    valid_outputs = (\n        valid_row_mask.sum() if isinstance(valid_row_mask, pd.Series) else 0\n    )\n    success_rate = (valid_outputs / total_rows * 100) if total_rows &gt; 0 else 0.0\n\n    # Determine quality score\n    if success_rate &gt;= 95.0:\n        quality_score = \"excellent\"\n    elif success_rate &gt;= 80.0:\n        quality_score = \"good\"\n    elif success_rate &gt;= 50.0:\n        quality_score = \"poor\"\n    else:\n        quality_score = \"critical\"\n\n    # Generate warnings and issues\n    warnings = []\n    issues = []\n\n    if success_rate &lt; 70.0:\n        issues.append(\n            f\"\u26a0\ufe0f  LOW SUCCESS RATE: Only {success_rate:.1f}% of rows have valid data \"\n            f\"({valid_outputs}/{total_rows} rows with at least one valid column)\"\n        )\n\n    if null_count &gt; total_cells * 0.3:  # &gt; 30% of all cells are null\n        issues.append(\n            f\"\u26a0\ufe0f  HIGH NULL RATE: {null_count} null cells out of {total_cells} total \"\n            f\"({null_count / total_cells * 100:.1f}% of all output cells)\"\n        )\n\n    if empty_count &gt; total_cells * 0.1:  # &gt; 10% of all cells are empty\n        warnings.append(\n            f\"Empty outputs detected: {empty_count} empty cells out of {total_cells} total \"\n            f\"({empty_count / total_cells * 100:.1f}% of all output cells)\"\n        )\n\n    # Check if reported metrics match actual data quality\n    if self.metrics.failed_rows == 0 and null_count &gt; 0:\n        issues.append(\n            f\"\u26a0\ufe0f  METRICS MISMATCH: Pipeline reported 0 failures but \"\n            f\"{null_count} cells have null outputs. This may indicate silent errors.\"\n        )\n\n    return QualityReport(\n        total_rows=total_rows,\n        valid_outputs=valid_outputs,\n        null_outputs=null_count,\n        empty_outputs=empty_count,\n        success_rate=success_rate,\n        quality_score=quality_score,\n        warnings=warnings,\n        issues=issues,\n    )\n</code></pre>"},{"location":"api/core/#ondine.core.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(text: str, tokens_in: int, tokens_out: int, model: str, cost: Decimal, latency_ms: float, metadata: dict[str, Any] = dict())\n</code></pre> <p>Response from a single LLM invocation.</p>"},{"location":"api/core/#ondine.core.ProcessingStats","title":"ProcessingStats  <code>dataclass</code>","text":"<pre><code>ProcessingStats(total_rows: int, processed_rows: int, failed_rows: int, skipped_rows: int, rows_per_second: float, total_duration_seconds: float, stage_durations: dict[str, float] = dict())\n</code></pre> <p>Statistics from pipeline execution.</p> <p>Tracks processing metrics for monitoring and debugging.</p> <p>Attributes:</p> Name Type Description <code>total_rows</code> <code>int</code> <p>Total number of rows in dataset</p> <code>processed_rows</code> <code>int</code> <p>Rows successfully processed</p> <code>failed_rows</code> <code>int</code> <p>Rows that failed processing</p> <code>skipped_rows</code> <code>int</code> <p>Rows skipped due to errors</p> Example <pre><code>result = pipeline.execute()\n\n# Check success rate\nsuccess_rate = result.metrics.success_count / result.metrics.total_rows * 100\nprint(f\"Success rate: {success_rate:.1f}%\")\n\n# Check for failures\nif result.metrics.failed_rows &gt; 0:\n    print(f\"Warning: {result.metrics.failed_rows} rows failed\")\n</code></pre>"},{"location":"api/core/#ondine.core.PromptBatch","title":"PromptBatch  <code>dataclass</code>","text":"<pre><code>PromptBatch(prompts: list[str], metadata: list[RowMetadata], batch_id: int)\n</code></pre> <p>Batch of prompts for processing.</p>"},{"location":"api/core/#ondine.core.ResponseBatch","title":"ResponseBatch  <code>dataclass</code>","text":"<pre><code>ResponseBatch(responses: list[str], metadata: list[RowMetadata], tokens_used: int, cost: Decimal, batch_id: int, latencies_ms: list[float] = list())\n</code></pre> <p>Batch of responses from LLM.</p>"},{"location":"api/core/#ondine.core.RowMetadata","title":"RowMetadata  <code>dataclass</code>","text":"<pre><code>RowMetadata(row_index: int, row_id: Any | None = None, batch_id: int | None = None, attempt: int = 1, custom: dict[str, Any] = dict())\n</code></pre> <p>Metadata for a single row during processing.</p>"},{"location":"api/core/#ondine.core.ValidationResult","title":"ValidationResult  <code>dataclass</code>","text":"<pre><code>ValidationResult(is_valid: bool, errors: list[str] = list(), warnings: list[str] = list())\n</code></pre> <p>Result from validation checks.</p>"},{"location":"api/core/#ondine.core.ValidationResult.add_error","title":"add_error","text":"<pre><code>add_error(error: str) -&gt; None\n</code></pre> <p>Add an error message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_error(self, error: str) -&gt; None:\n    \"\"\"Add an error message.\"\"\"\n    self.errors.append(error)\n    self.is_valid = False\n</code></pre>"},{"location":"api/core/#ondine.core.ValidationResult.add_warning","title":"add_warning","text":"<pre><code>add_warning(warning: str) -&gt; None\n</code></pre> <p>Add a warning message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_warning(self, warning: str) -&gt; None:\n    \"\"\"Add a warning message.\"\"\"\n    self.warnings.append(warning)\n</code></pre>"},{"location":"api/core/#ondine.core.WriteConfirmation","title":"WriteConfirmation  <code>dataclass</code>","text":"<pre><code>WriteConfirmation(path: str, rows_written: int, success: bool, timestamp: datetime = datetime.now(), metadata: dict[str, Any] = dict())\n</code></pre> <p>Confirmation of successful data write.</p>"},{"location":"api/core/#ondine.core.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for data source configuration.</p> <p>Defines how to load input data and which columns to process. Supports CSV, Excel, Parquet, and in-memory DataFrames.</p> <p>Attributes:</p> Name Type Description <code>source_type</code> <code>DataSourceType</code> <p>Type of data source (csv, excel, parquet, dataframe)</p> <code>source_path</code> <code>str | Path | None</code> <p>Path to file (None for DataFrame sources)</p> <code>input_columns</code> <code>list[str]</code> <p>Column names to use in prompts (must exist in data)</p> <code>output_columns</code> <code>list[str]</code> <p>Column names for LLM results (must not overlap with input)</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional filters to apply when loading data</p> <code>sheet_name</code> <code>str | int | None</code> <p>Sheet name or index for Excel files (default: 0)</p> <code>delimiter</code> <code>str</code> <p>CSV delimiter character (default: \",\")</p> <code>encoding</code> <code>str</code> <p>File encoding (default: \"utf-8\")</p> Example <pre><code>spec = DatasetSpec(\n    source_type=DataSourceType.CSV,\n    source_path=\"products.csv\",\n    input_columns=[\"title\", \"description\"],\n    output_columns=[\"category\", \"price_range\"]\n)\n</code></pre>"},{"location":"api/core/#ondine.core.DatasetSpec.validate_source_path","title":"validate_source_path  <code>classmethod</code>","text":"<pre><code>validate_source_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"source_path\")\n@classmethod\ndef validate_source_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/#ondine.core.DatasetSpec.validate_no_overlap","title":"validate_no_overlap  <code>classmethod</code>","text":"<pre><code>validate_no_overlap(v: list[str], info: Any) -&gt; list[str]\n</code></pre> <p>Ensure output columns don't overlap with input columns.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"output_columns\")\n@classmethod\ndef validate_no_overlap(cls, v: list[str], info: Any) -&gt; list[str]:\n    \"\"\"Ensure output columns don't overlap with input columns.\"\"\"\n    if \"input_columns\" in info.data:\n        input_cols = set(info.data[\"input_columns\"])\n        output_cols = set(v)\n        overlap = input_cols &amp; output_cols\n        if overlap:\n            raise ValueError(f\"Output columns overlap with input: {overlap}\")\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.DataSourceType","title":"DataSourceType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported data source types.</p>"},{"location":"api/core/#ondine.core.ErrorPolicy","title":"ErrorPolicy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Error handling policies for processing failures.</p>"},{"location":"api/core/#ondine.core.LLMProvider","title":"LLMProvider","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported LLM providers.</p>"},{"location":"api/core/#ondine.core.LLMSpec","title":"LLMSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for LLM provider configuration.</p> <p>Defines which LLM provider to use, model settings, and authentication. Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom OpenAI-compatible APIs.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>LLMProvider</code> <p>LLM provider (openai, azure_openai, anthropic, groq, mlx, openai_compatible)</p> <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\", \"llama-3.3-70b-versatile\")</p> <code>api_key</code> <code>str | None</code> <p>API key (optional, reads from environment if not provided)</p> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-2.0, default: 0.0 for deterministic output)</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum output tokens (optional, uses model default)</p> <code>top_p</code> <code>float</code> <p>Nucleus sampling parameter (0.0-1.0, default: 1.0)</p> Example <pre><code># OpenAI\nspec = LLMSpec(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    temperature=0.3\n)\n\n# Groq (fast and affordable)\nspec = LLMSpec(\n    provider=LLMProvider.GROQ,\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.0\n)\n\n# Azure with Managed Identity (no API key needed)\nspec = LLMSpec(\n    provider=LLMProvider.AZURE_OPENAI,\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    use_managed_identity=True\n)\n</code></pre> Note <p>Use LLMProviderPresets for pre-configured common providers.</p>"},{"location":"api/core/#ondine.core.LLMSpec.validate_base_url_format","title":"validate_base_url_format  <code>classmethod</code>","text":"<pre><code>validate_base_url_format(v: str | None) -&gt; str | None\n</code></pre> <p>Validate base_url is a valid HTTP(S) URL with a host.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"base_url\")\n@classmethod\ndef validate_base_url_format(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate base_url is a valid HTTP(S) URL with a host.\"\"\"\n    if v is None:\n        return v\n    from urllib.parse import urlparse\n\n    parsed = urlparse(v)\n    if parsed.scheme not in {\"http\", \"https\"}:\n        raise ValueError(\"base_url must start with http:// or https://\")\n    if not parsed.netloc:\n        raise ValueError(\n            \"base_url must include a host (e.g., localhost, api.example.com)\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.LLMSpec.validate_azure_config","title":"validate_azure_config  <code>classmethod</code>","text":"<pre><code>validate_azure_config(v: str | None, info: Any) -&gt; str | None\n</code></pre> <p>Validate Azure-specific configuration.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"azure_endpoint\", \"azure_deployment\")\n@classmethod\ndef validate_azure_config(cls, v: str | None, info: Any) -&gt; str | None:\n    \"\"\"Validate Azure-specific configuration.\"\"\"\n    if info.data.get(\"provider\") == LLMProvider.AZURE_OPENAI and v is None:\n        field_name = info.field_name\n        raise ValueError(f\"{field_name} required for Azure OpenAI provider\")\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.LLMSpec.validate_provider_requirements","title":"validate_provider_requirements","text":"<pre><code>validate_provider_requirements() -&gt; LLMSpec\n</code></pre> <p>Validate provider-specific requirements.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    \"\"\"Validate provider-specific requirements.\"\"\"\n    # Check openai_compatible requires base_url\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre>"},{"location":"api/core/#ondine.core.MergeStrategy","title":"MergeStrategy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Output merge strategies.</p>"},{"location":"api/core/#ondine.core.OutputSpec","title":"OutputSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for output configuration.</p>"},{"location":"api/core/#ondine.core.OutputSpec.validate_destination_path","title":"validate_destination_path  <code>classmethod</code>","text":"<pre><code>validate_destination_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"destination_path\")\n@classmethod\ndef validate_destination_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/#ondine.core.PipelineSpecifications","title":"PipelineSpecifications","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for all pipeline specifications.</p>"},{"location":"api/core/#ondine.core.ProcessingSpec","title":"ProcessingSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for processing parameters.</p> <p>Controls how the pipeline executes: batch sizes, concurrency, error handling, rate limiting, and budget constraints.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of rows per batch (1-1000, default: 100)</p> <code>concurrency</code> <code>int</code> <p>Number of parallel LLM requests (1-20, default: 5)</p> <code>checkpoint_interval</code> <code>int</code> <p>Save checkpoint every N rows (default: 500)</p> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts for failed requests (default: 3)</p> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds (default: 1.0)</p> <code>error_policy</code> <code>ErrorPolicy</code> <p>How to handle errors (retry, skip, fail, use_default)</p> <code>rate_limit_rpm</code> <code>int | None</code> <p>Requests per minute limit (optional, no limit if None)</p> <code>max_budget</code> <code>Decimal | None</code> <p>Maximum cost in USD (optional, no limit if None)</p> <code>enable_preprocessing</code> <code>bool</code> <p>Enable input text preprocessing (default: False)</p> <code>preprocessing_max_length</code> <code>int</code> <p>Max characters after preprocessing (default: 500)</p> <code>auto_retry_failed</code> <code>bool</code> <p>Auto-retry rows with null/empty outputs (default: False)</p> <code>max_retry_attempts</code> <code>int</code> <p>Max retry attempts for failed rows (1-3, default: 1)</p> <code>use_jinja2</code> <code>bool</code> <p>Use Jinja2 for template rendering, enables loops/conditionals (default: False)</p> <code>progress_mode</code> <code>str</code> <p>Progress tracking mode (auto, rich, tqdm, logging, none)</p> Example <pre><code># Conservative settings (free tier)\nspec = ProcessingSpec(\n    batch_size=50,\n    concurrency=5,\n    rate_limit_rpm=25,\n    max_budget=Decimal(\"5.0\")\n)\n\n# Aggressive settings (paid tier)\nspec = ProcessingSpec(\n    batch_size=200,\n    concurrency=20,\n    rate_limit_rpm=100,\n    max_budget=Decimal(\"50.0\")\n)\n\n# Fault-tolerant settings\nspec = ProcessingSpec(\n    max_retries=5,\n    error_policy=ErrorPolicy.RETRY,\n    checkpoint_interval=100\n)\n</code></pre>"},{"location":"api/core/#ondine.core.ProcessingSpec.validate_checkpoint_dir","title":"validate_checkpoint_dir  <code>classmethod</code>","text":"<pre><code>validate_checkpoint_dir(v: str | Path) -&gt; Path\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"checkpoint_dir\")\n@classmethod\ndef validate_checkpoint_dir(cls, v: str | Path) -&gt; Path:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/#ondine.core.PromptSpec","title":"PromptSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for prompt template configuration.</p>"},{"location":"api/core/#ondine.core.PromptSpec.validate_template","title":"validate_template  <code>classmethod</code>","text":"<pre><code>validate_template(v: str) -&gt; str\n</code></pre> <p>Validate template has at least one variable.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"template\")\n@classmethod\ndef validate_template(cls, v: str) -&gt; str:\n    \"\"\"Validate template has at least one variable.\"\"\"\n    if \"{\" not in v or \"}\" not in v:\n        raise ValueError(\n            \"Template must contain at least one variable in {var} format\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.PromptSpec.validate_response_format","title":"validate_response_format  <code>classmethod</code>","text":"<pre><code>validate_response_format(v: str) -&gt; str\n</code></pre> <p>Validate response format is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"response_format\")\n@classmethod\ndef validate_response_format(cls, v: str) -&gt; str:\n    \"\"\"Validate response format is supported.\"\"\"\n    allowed = [\"raw\", \"json\", \"regex\"]\n    if v not in allowed:\n        raise ValueError(f\"response_format must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.PromptSpec.validate_batch_strategy","title":"validate_batch_strategy  <code>classmethod</code>","text":"<pre><code>validate_batch_strategy(v: str) -&gt; str\n</code></pre> <p>Validate batch strategy is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"batch_strategy\")\n@classmethod\ndef validate_batch_strategy(cls, v: str) -&gt; str:\n    \"\"\"Validate batch strategy is supported.\"\"\"\n    allowed = [\"json\", \"csv\"]\n    if v not in allowed:\n        raise ValueError(f\"batch_strategy must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/core/error_handler/","title":"error_handler","text":""},{"location":"api/core/error_handler/#ondine.core.error_handler","title":"error_handler","text":"<p>Error handling system with configurable policies.</p> <p>Implements Strategy pattern for different error handling approaches.</p>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorAction","title":"ErrorAction","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Actions to take on errors.</p>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorDecision","title":"ErrorDecision  <code>dataclass</code>","text":"<pre><code>ErrorDecision(action: ErrorAction, default_value: Any = None, retry_count: int = 0, context: dict[str, Any] | None = None)\n</code></pre> <p>Decision on how to handle an error.</p>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorHandler","title":"ErrorHandler","text":"<pre><code>ErrorHandler(policy: ErrorPolicy = ErrorPolicy.SKIP, max_retries: int = 3, default_value: Any = None, default_value_factory: Callable[[], Any] | None = None)\n</code></pre> <p>Policy-based error handling (orchestrates retry/skip/fail decisions).</p> <p>Scope: Stage execution errors and pipeline-level error handling Policies: SKIP, FAIL, RETRY (delegates to RetryHandler for execution) Use when: Configuring how the pipeline handles errors</p> <p>Policy Behaviors: - SKIP: Log error and skip the row (continue processing) - FAIL: Raise error and stop pipeline - RETRY: Retry the operation (delegates to RetryHandler) - DEFAULT: Return a default value on error</p> Example <p>handler = ErrorHandler(policy=ErrorPolicy.RETRY, max_retries=3) decision = handler.handle_error(exception, context)</p> <p>See Also: - RetryHandler: Executes the actual retry logic - Pipeline._auto_retry_failed_rows(): Row-level quality retry - docs/architecture/decisions/ADR-006-retry-levels.md</p> Design Note <p>ErrorHandler decides WHAT to do (policy) RetryHandler decides HOW to do it (exponential backoff)</p> <p>Initialize error handler.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>ErrorPolicy</code> <p>Error handling policy</p> <code>SKIP</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>default_value</code> <code>Any</code> <p>Static default value (or use default_value_factory)</p> <code>None</code> <code>default_value_factory</code> <code>Callable[[], Any] | None</code> <p>Function to generate default values</p> <code>None</code> Source code in <code>ondine/core/error_handler.py</code> <pre><code>def __init__(\n    self,\n    policy: ErrorPolicy = ErrorPolicy.SKIP,\n    max_retries: int = 3,\n    default_value: Any = None,\n    default_value_factory: Callable[[], Any] | None = None,\n):\n    \"\"\"\n    Initialize error handler.\n\n    Args:\n        policy: Error handling policy\n        max_retries: Maximum retry attempts\n        default_value: Static default value (or use default_value_factory)\n        default_value_factory: Function to generate default values\n    \"\"\"\n    self.policy = policy\n    self.max_retries = max_retries\n    self.default_value = default_value\n\n    # If default_value_factory is provided, use it; otherwise use lambda returning default_value\n    if default_value_factory is not None:\n        self.default_value_factory = default_value_factory\n    else:\n        self.default_value_factory = lambda: default_value\n</code></pre>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorHandler.handle_error","title":"handle_error","text":"<pre><code>handle_error(error: Exception, context: dict[str, Any], attempt: int = 1) -&gt; ErrorDecision\n</code></pre> <p>Decide how to handle an error.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>context</code> <code>dict[str, Any]</code> <p>Error context (row_index, stage, etc.)</p> required <code>attempt</code> <code>int</code> <p>Current attempt number</p> <code>1</code> <p>Returns:</p> Type Description <code>ErrorDecision</code> <p>ErrorDecision with action to take</p> Source code in <code>ondine/core/error_handler.py</code> <pre><code>def handle_error(\n    self,\n    error: Exception,\n    context: dict[str, Any],\n    attempt: int = 1,\n) -&gt; ErrorDecision:\n    \"\"\"\n    Decide how to handle an error.\n\n    Args:\n        error: The exception that occurred\n        context: Error context (row_index, stage, etc.)\n        attempt: Current attempt number\n\n    Returns:\n        ErrorDecision with action to take\n    \"\"\"\n    # Get attempt from context if available, otherwise use parameter\n    attempt = context.get(\"attempt\", attempt)\n    row_index = context.get(\"row_index\", \"unknown\")\n    stage = context.get(\"stage\", \"unknown\")\n\n    # Log the error\n    logger.error(\n        f\"Error in {stage} at row {row_index}: {error}\",\n        exc_info=True,\n    )\n\n    # Check for NonRetryableError first (fail fast)\n    if isinstance(error, NonRetryableError):\n        logger.error(\n            f\"\u274c NON-RETRYABLE ERROR: {error}\\n\"\n            f\"   Type: {type(error).__name__}\\n\"\n            f\"   This error cannot be recovered. Pipeline will terminate.\\n\"\n            f\"   Please fix the configuration and try again.\"\n        )\n        return ErrorDecision(\n            action=ErrorAction.FAIL,\n            context=context,\n        )\n\n    # Check for FATAL errors that should always fail immediately (legacy)\n    if self._is_fatal_error(error):\n        logger.error(\n            f\"\u274c FATAL ERROR: {error}\\n\"\n            f\"   This error cannot be recovered. Pipeline will terminate.\"\n        )\n        return ErrorDecision(\n            action=ErrorAction.FAIL,\n            context=context,\n        )\n\n    # Apply policy\n    if self.policy == ErrorPolicy.RETRY:\n        if attempt &lt; self.max_retries:\n            logger.info(f\"Retrying (attempt {attempt + 1}/{self.max_retries})\")\n            return ErrorDecision(\n                action=ErrorAction.RETRY,\n                retry_count=attempt + 1,\n                context=context,\n            )\n        logger.warning(f\"Max retries ({self.max_retries}) exceeded, skipping\")\n        return ErrorDecision(\n            action=ErrorAction.SKIP,\n            context=context,\n        )\n\n    if self.policy == ErrorPolicy.SKIP:\n        logger.info(f\"Skipping row {row_index} due to error\")\n        return ErrorDecision(\n            action=ErrorAction.SKIP,\n            context=context,\n        )\n\n    if self.policy == ErrorPolicy.USE_DEFAULT:\n        default = self.default_value_factory()\n        logger.info(f\"Using default value for row {row_index}: {default}\")\n        return ErrorDecision(\n            action=ErrorAction.USE_DEFAULT,\n            default_value=default,\n            context=context,\n        )\n\n    if self.policy == ErrorPolicy.FAIL:\n        logger.error(\"Failing pipeline due to error\")\n        return ErrorDecision(\n            action=ErrorAction.FAIL,\n            context=context,\n        )\n\n    # Unknown policy, default to fail\n    return ErrorDecision(\n        action=ErrorAction.FAIL,\n        context=context,\n    )\n</code></pre>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorHandler.should_retry","title":"should_retry","text":"<pre><code>should_retry(error: Exception) -&gt; bool\n</code></pre> <p>Determine if error should be retried.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if retriable</p> Source code in <code>ondine/core/error_handler.py</code> <pre><code>def should_retry(self, error: Exception) -&gt; bool:\n    \"\"\"\n    Determine if error should be retried.\n\n    Args:\n        error: The exception\n\n    Returns:\n        True if retriable\n    \"\"\"\n    # Don't retry fatal errors\n    if self._is_fatal_error(error):\n        return False\n\n    retriable_keywords = [\n        \"rate limit\",\n        \"timeout\",\n        \"network\",\n        \"connection\",\n        \"503\",\n        \"502\",\n        \"429\",\n    ]\n\n    error_str = str(error).lower()\n    return any(keyword in error_str for keyword in retriable_keywords)\n</code></pre>"},{"location":"api/core/exceptions/","title":"exceptions","text":""},{"location":"api/core/exceptions/#ondine.core.exceptions","title":"exceptions","text":"<p>Custom exceptions for Ondine pipeline execution.</p> <p>This module defines exception types for error classification and handling. Distinguishes between retryable (transient) and non-retryable (fatal) errors.</p>"},{"location":"api/core/exceptions/#ondine.core.exceptions.NonRetryableError","title":"NonRetryableError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors that should not be retried.</p> <p>These are fatal errors that indicate configuration issues, invalid inputs, or permanent failures that cannot be resolved by retrying the operation.</p> <p>Examples of non-retryable errors: - Model doesn't exist or has been decommissioned - Invalid API key or authentication failure - File not found or invalid configuration - Quota/credits exhausted (not rate limit)</p> <p>Retryable errors (handled separately): - Rate limit errors (429) - Network timeouts or connection errors - Temporary API unavailability (503)</p> Example <pre><code>try:\n    result = pipeline.execute()\nexcept NonRetryableError as e:\n    # Fatal error - don't retry, fix configuration\n    logger.error(f\"Pipeline failed with fatal error: {e}\")\n    sys.exit(1)\n</code></pre>"},{"location":"api/core/exceptions/#ondine.core.exceptions.ModelNotFoundError","title":"ModelNotFoundError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>Model doesn't exist or has been decommissioned.</p> <p>Raised when: - LLM provider returns \"model not found\" error - Model has been decommissioned/deprecated - Invalid model name specified</p> Example <pre><code># This will raise ModelNotFoundError if model is invalid\npipeline = (\n    PipelineBuilder.create()\n    .with_llm(provider=\"groq\", model=\"llama-3.1-70b-versatile\")  # Decommissioned\n    .build()\n)\n</code></pre> Resolution <p>Update model name to a valid, current model.</p>"},{"location":"api/core/exceptions/#ondine.core.exceptions.InvalidAPIKeyError","title":"InvalidAPIKeyError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>API key is invalid, missing, or authentication failed.</p> <p>Raised when: - API key environment variable not set - API key is invalid or expired - Authentication fails (401, 403) - Insufficient permissions</p> Example <pre><code># This will raise InvalidAPIKeyError if OPENAI_API_KEY not set\npipeline = (\n    PipelineBuilder.create()\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\nresult = pipeline.execute()  # Fails here if no API key\n</code></pre> Resolution <p>Set valid API key in environment variable or configuration.</p>"},{"location":"api/core/exceptions/#ondine.core.exceptions.ConfigurationError","title":"ConfigurationError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>Invalid configuration or missing required resources.</p> <p>Raised when: - Input file doesn't exist - Invalid parameter values - Missing required configuration - Incompatible settings</p> Example <pre><code># This will raise ConfigurationError if file doesn't exist\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"nonexistent.csv\", ...)\n    .build()\n)\nresult = pipeline.execute()  # Fails here\n</code></pre> Resolution <p>Fix configuration or ensure required files/resources exist.</p>"},{"location":"api/core/exceptions/#ondine.core.exceptions.QuotaExceededError","title":"QuotaExceededError","text":"<p>               Bases: <code>NonRetryableError</code></p> <p>API quota or credits exhausted.</p> <p>Raised when: - Account credits/quota exhausted - Billing issue or payment required - Account suspended</p> Note <p>This is different from rate limiting (429), which is retryable. Quota errors indicate a permanent issue until credits are added.</p> Example <pre><code>try:\n    result = pipeline.execute()\nexcept QuotaExceededError:\n    # Need to add credits or upgrade plan\n    logger.error(\"Account quota exceeded - add credits\")\n</code></pre> Resolution <p>Add credits, upgrade plan, or resolve billing issues.</p>"},{"location":"api/core/models/","title":"models","text":""},{"location":"api/core/models/#ondine.core.models","title":"models","text":"<p>Core data models for execution results and metadata.</p> <p>These models represent the outputs and state information from pipeline execution with type safety.</p>"},{"location":"api/core/models/#ondine.core.models.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(text: str, tokens_in: int, tokens_out: int, model: str, cost: Decimal, latency_ms: float, metadata: dict[str, Any] = dict())\n</code></pre> <p>Response from a single LLM invocation.</p>"},{"location":"api/core/models/#ondine.core.models.CostEstimate","title":"CostEstimate  <code>dataclass</code>","text":"<pre><code>CostEstimate(total_cost: Decimal, total_tokens: int, input_tokens: int, output_tokens: int, rows: int, breakdown_by_stage: dict[str, Decimal] = dict(), confidence: str = 'estimate')\n</code></pre> <p>Cost estimation for pipeline execution.</p> <p>Provides detailed cost breakdown with Decimal precision to avoid floating-point errors.</p> <p>Attributes:</p> Name Type Description <code>total_cost</code> <code>Decimal</code> <p>Total cost in USD (Decimal for precision)</p> <code>total_tokens</code> <code>int</code> <p>Total tokens consumed (input + output)</p> <code>input_tokens</code> <code>int</code> <p>Input tokens sent to LLM</p> <code>output_tokens</code> <code>int</code> <p>Output tokens generated by LLM</p> <code>rows</code> <code>int</code> <p>Number of rows processed</p> <code>breakdown_by_stage</code> <code>dict[str, Decimal]</code> <p>Cost breakdown by pipeline stage</p> <code>confidence</code> <code>str</code> <p>Confidence level (estimate, sample-based, actual)</p> Example <pre><code>result = pipeline.execute()\n\n# Access costs\nprint(f\"Total: ${result.costs.total_cost}\")\nprint(f\"Input tokens: {result.costs.input_tokens:,}\")\nprint(f\"Output tokens: {result.costs.output_tokens:,}\")\nprint(f\"Cost per row: ${result.costs.total_cost / result.costs.rows:.4f}\")\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.ProcessingStats","title":"ProcessingStats  <code>dataclass</code>","text":"<pre><code>ProcessingStats(total_rows: int, processed_rows: int, failed_rows: int, skipped_rows: int, rows_per_second: float, total_duration_seconds: float, stage_durations: dict[str, float] = dict())\n</code></pre> <p>Statistics from pipeline execution.</p> <p>Tracks processing metrics for monitoring and debugging.</p> <p>Attributes:</p> Name Type Description <code>total_rows</code> <code>int</code> <p>Total number of rows in dataset</p> <code>processed_rows</code> <code>int</code> <p>Rows successfully processed</p> <code>failed_rows</code> <code>int</code> <p>Rows that failed processing</p> <code>skipped_rows</code> <code>int</code> <p>Rows skipped due to errors</p> Example <pre><code>result = pipeline.execute()\n\n# Check success rate\nsuccess_rate = result.metrics.success_count / result.metrics.total_rows * 100\nprint(f\"Success rate: {success_rate:.1f}%\")\n\n# Check for failures\nif result.metrics.failed_rows &gt; 0:\n    print(f\"Warning: {result.metrics.failed_rows} rows failed\")\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.ErrorInfo","title":"ErrorInfo  <code>dataclass</code>","text":"<pre><code>ErrorInfo(row_index: int, stage_name: str, error_type: str, error_message: str, timestamp: datetime, context: dict[str, Any] = dict())\n</code></pre> <p>Information about an error during processing.</p>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult","title":"ExecutionResult  <code>dataclass</code>","text":"<pre><code>ExecutionResult(data: DataFrame, metrics: ProcessingStats, costs: CostEstimate, errors: list[ErrorInfo] = list(), execution_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, success: bool = True, metadata: dict[str, Any] = dict())\n</code></pre> <p>Complete result from pipeline execution.</p> <p>Contains all output data, metrics, costs, and execution metadata from a pipeline run.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>DataFrame with input data and generated output columns</p> <code>metrics</code> <code>ProcessingStats</code> <p>Processing statistics (total_rows, success_count, failed_rows, etc.)</p> <code>costs</code> <code>CostEstimate</code> <p>Cost breakdown (total_cost, input_tokens, output_tokens)</p> <code>errors</code> <code>list[ErrorInfo]</code> <p>List of errors encountered during execution</p> <code>execution_id</code> <code>UUID</code> <p>Unique ID for this execution session</p> <code>start_time</code> <code>datetime</code> <p>When execution started</p> <code>end_time</code> <code>datetime | None</code> <p>When execution completed (None if still running)</p> <code>success</code> <code>bool</code> <p>Whether execution completed successfully</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional execution metadata</p> Example <pre><code>result = pipeline.execute()\n\n# Access output data\nprint(result.data.head())\nresult.data.to_csv(\"output.csv\")\n\n# Check metrics\nprint(f\"Total: {result.metrics.total_rows}\")\nprint(f\"Success: {result.metrics.success_count}\")\nprint(f\"Failed: {result.metrics.failed_rows}\")\n\n# Check costs\nprint(f\"Cost: ${result.costs.total_cost}\")\nprint(f\"Tokens: {result.costs.total_tokens}\")\n\n# Check execution time\nprint(f\"Duration: {result.duration:.2f}s\")\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Get execution duration in seconds.</p>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult.error_rate","title":"error_rate  <code>property</code>","text":"<pre><code>error_rate: float\n</code></pre> <p>Get error rate as percentage.</p>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult.validate_output_quality","title":"validate_output_quality","text":"<pre><code>validate_output_quality(output_columns: list[str]) -&gt; QualityReport\n</code></pre> <p>Validate the quality of output data by checking for null/empty values.</p> <p>Parameters:</p> Name Type Description Default <code>output_columns</code> <code>list[str]</code> <p>List of output column names to check</p> required <p>Returns:</p> Type Description <code>QualityReport</code> <p>QualityReport with quality metrics and warnings</p> Source code in <code>ondine/core/models.py</code> <pre><code>def validate_output_quality(self, output_columns: list[str]) -&gt; \"QualityReport\":\n    \"\"\"\n    Validate the quality of output data by checking for null/empty values.\n\n    Args:\n        output_columns: List of output column names to check\n\n    Returns:\n        QualityReport with quality metrics and warnings\n    \"\"\"\n    total_rows = len(self.data)\n    total_columns = len(output_columns)\n    total_cells = total_rows * total_columns\n\n    # Count null and empty values across ALL output columns\n    null_count = 0\n    empty_count = 0\n\n    for col in output_columns:\n        if col in self.data.columns:\n            # Count nulls (None, NaN, NaT)\n            null_count += self.data[col].isna().sum()\n            # Count empty strings (only for string columns)\n            if self.data[col].dtype == \"object\":\n                empty_count += (self.data[col].astype(str).str.strip() == \"\").sum()\n\n    # Count rows with at least one valid output column\n    # A row is \"valid\" if at least one output column has non-null, non-empty data\n    valid_row_mask = False\n    for col in output_columns:\n        if col in self.data.columns:\n            non_null = ~self.data[col].isna()\n            if self.data[col].dtype == \"object\":\n                non_empty = self.data[col].astype(str).str.strip() != \"\"\n                valid_row_mask |= non_null &amp; non_empty\n            else:\n                valid_row_mask |= non_null\n\n    valid_outputs = (\n        valid_row_mask.sum() if isinstance(valid_row_mask, pd.Series) else 0\n    )\n    success_rate = (valid_outputs / total_rows * 100) if total_rows &gt; 0 else 0.0\n\n    # Determine quality score\n    if success_rate &gt;= 95.0:\n        quality_score = \"excellent\"\n    elif success_rate &gt;= 80.0:\n        quality_score = \"good\"\n    elif success_rate &gt;= 50.0:\n        quality_score = \"poor\"\n    else:\n        quality_score = \"critical\"\n\n    # Generate warnings and issues\n    warnings = []\n    issues = []\n\n    if success_rate &lt; 70.0:\n        issues.append(\n            f\"\u26a0\ufe0f  LOW SUCCESS RATE: Only {success_rate:.1f}% of rows have valid data \"\n            f\"({valid_outputs}/{total_rows} rows with at least one valid column)\"\n        )\n\n    if null_count &gt; total_cells * 0.3:  # &gt; 30% of all cells are null\n        issues.append(\n            f\"\u26a0\ufe0f  HIGH NULL RATE: {null_count} null cells out of {total_cells} total \"\n            f\"({null_count / total_cells * 100:.1f}% of all output cells)\"\n        )\n\n    if empty_count &gt; total_cells * 0.1:  # &gt; 10% of all cells are empty\n        warnings.append(\n            f\"Empty outputs detected: {empty_count} empty cells out of {total_cells} total \"\n            f\"({empty_count / total_cells * 100:.1f}% of all output cells)\"\n        )\n\n    # Check if reported metrics match actual data quality\n    if self.metrics.failed_rows == 0 and null_count &gt; 0:\n        issues.append(\n            f\"\u26a0\ufe0f  METRICS MISMATCH: Pipeline reported 0 failures but \"\n            f\"{null_count} cells have null outputs. This may indicate silent errors.\"\n        )\n\n    return QualityReport(\n        total_rows=total_rows,\n        valid_outputs=valid_outputs,\n        null_outputs=null_count,\n        empty_outputs=empty_count,\n        success_rate=success_rate,\n        quality_score=quality_score,\n        warnings=warnings,\n        issues=issues,\n    )\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.ValidationResult","title":"ValidationResult  <code>dataclass</code>","text":"<pre><code>ValidationResult(is_valid: bool, errors: list[str] = list(), warnings: list[str] = list())\n</code></pre> <p>Result from validation checks.</p>"},{"location":"api/core/models/#ondine.core.models.ValidationResult.add_error","title":"add_error","text":"<pre><code>add_error(error: str) -&gt; None\n</code></pre> <p>Add an error message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_error(self, error: str) -&gt; None:\n    \"\"\"Add an error message.\"\"\"\n    self.errors.append(error)\n    self.is_valid = False\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.ValidationResult.add_warning","title":"add_warning","text":"<pre><code>add_warning(warning: str) -&gt; None\n</code></pre> <p>Add a warning message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_warning(self, warning: str) -&gt; None:\n    \"\"\"Add a warning message.\"\"\"\n    self.warnings.append(warning)\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.QualityReport","title":"QualityReport  <code>dataclass</code>","text":"<pre><code>QualityReport(total_rows: int, valid_outputs: int, null_outputs: int, empty_outputs: int, success_rate: float, quality_score: str, warnings: list[str] = list(), issues: list[str] = list())\n</code></pre> <p>Quality assessment of pipeline output.</p>"},{"location":"api/core/models/#ondine.core.models.QualityReport.is_acceptable","title":"is_acceptable  <code>property</code>","text":"<pre><code>is_acceptable: bool\n</code></pre> <p>Check if quality is acceptable (&gt;= 70% success).</p>"},{"location":"api/core/models/#ondine.core.models.QualityReport.has_issues","title":"has_issues  <code>property</code>","text":"<pre><code>has_issues: bool\n</code></pre> <p>Check if there are any issues.</p>"},{"location":"api/core/models/#ondine.core.models.WriteConfirmation","title":"WriteConfirmation  <code>dataclass</code>","text":"<pre><code>WriteConfirmation(path: str, rows_written: int, success: bool, timestamp: datetime = datetime.now(), metadata: dict[str, Any] = dict())\n</code></pre> <p>Confirmation of successful data write.</p>"},{"location":"api/core/models/#ondine.core.models.CheckpointInfo","title":"CheckpointInfo  <code>dataclass</code>","text":"<pre><code>CheckpointInfo(session_id: UUID, checkpoint_path: str, row_index: int, stage_index: int, timestamp: datetime, size_bytes: int)\n</code></pre> <p>Information about a checkpoint.</p>"},{"location":"api/core/models/#ondine.core.models.RowMetadata","title":"RowMetadata  <code>dataclass</code>","text":"<pre><code>RowMetadata(row_index: int, row_id: Any | None = None, batch_id: int | None = None, attempt: int = 1, custom: dict[str, Any] = dict())\n</code></pre> <p>Metadata for a single row during processing.</p>"},{"location":"api/core/models/#ondine.core.models.PromptBatch","title":"PromptBatch  <code>dataclass</code>","text":"<pre><code>PromptBatch(prompts: list[str], metadata: list[RowMetadata], batch_id: int)\n</code></pre> <p>Batch of prompts for processing.</p>"},{"location":"api/core/models/#ondine.core.models.ResponseBatch","title":"ResponseBatch  <code>dataclass</code>","text":"<pre><code>ResponseBatch(responses: list[str], metadata: list[RowMetadata], tokens_used: int, cost: Decimal, batch_id: int, latencies_ms: list[float] = list())\n</code></pre> <p>Batch of responses from LLM.</p>"},{"location":"api/core/specifications/","title":"specifications","text":""},{"location":"api/core/specifications/#ondine.core.specifications","title":"specifications","text":"<p>Core specification models for pipeline configuration.</p> <p>These Pydantic models define the configuration contracts for all pipeline components, following the principle of separation between configuration (what to do) and execution (how to do it).</p>"},{"location":"api/core/specifications/#ondine.core.specifications.DataSourceType","title":"DataSourceType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported data source types.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProvider","title":"LLMProvider","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported LLM providers.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.ErrorPolicy","title":"ErrorPolicy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Error handling policies for processing failures.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.MergeStrategy","title":"MergeStrategy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Output merge strategies.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for data source configuration.</p> <p>Defines how to load input data and which columns to process. Supports CSV, Excel, Parquet, and in-memory DataFrames.</p> <p>Attributes:</p> Name Type Description <code>source_type</code> <code>DataSourceType</code> <p>Type of data source (csv, excel, parquet, dataframe)</p> <code>source_path</code> <code>str | Path | None</code> <p>Path to file (None for DataFrame sources)</p> <code>input_columns</code> <code>list[str]</code> <p>Column names to use in prompts (must exist in data)</p> <code>output_columns</code> <code>list[str]</code> <p>Column names for LLM results (must not overlap with input)</p> <code>filters</code> <code>dict[str, Any] | None</code> <p>Optional filters to apply when loading data</p> <code>sheet_name</code> <code>str | int | None</code> <p>Sheet name or index for Excel files (default: 0)</p> <code>delimiter</code> <code>str</code> <p>CSV delimiter character (default: \",\")</p> <code>encoding</code> <code>str</code> <p>File encoding (default: \"utf-8\")</p> Example <pre><code>spec = DatasetSpec(\n    source_type=DataSourceType.CSV,\n    source_path=\"products.csv\",\n    input_columns=[\"title\", \"description\"],\n    output_columns=[\"category\", \"price_range\"]\n)\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.DatasetSpec.validate_source_path","title":"validate_source_path  <code>classmethod</code>","text":"<pre><code>validate_source_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"source_path\")\n@classmethod\ndef validate_source_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.DatasetSpec.validate_no_overlap","title":"validate_no_overlap  <code>classmethod</code>","text":"<pre><code>validate_no_overlap(v: list[str], info: Any) -&gt; list[str]\n</code></pre> <p>Ensure output columns don't overlap with input columns.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"output_columns\")\n@classmethod\ndef validate_no_overlap(cls, v: list[str], info: Any) -&gt; list[str]:\n    \"\"\"Ensure output columns don't overlap with input columns.\"\"\"\n    if \"input_columns\" in info.data:\n        input_cols = set(info.data[\"input_columns\"])\n        output_cols = set(v)\n        overlap = input_cols &amp; output_cols\n        if overlap:\n            raise ValueError(f\"Output columns overlap with input: {overlap}\")\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.PromptSpec","title":"PromptSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for prompt template configuration.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.PromptSpec.validate_template","title":"validate_template  <code>classmethod</code>","text":"<pre><code>validate_template(v: str) -&gt; str\n</code></pre> <p>Validate template has at least one variable.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"template\")\n@classmethod\ndef validate_template(cls, v: str) -&gt; str:\n    \"\"\"Validate template has at least one variable.\"\"\"\n    if \"{\" not in v or \"}\" not in v:\n        raise ValueError(\n            \"Template must contain at least one variable in {var} format\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.PromptSpec.validate_response_format","title":"validate_response_format  <code>classmethod</code>","text":"<pre><code>validate_response_format(v: str) -&gt; str\n</code></pre> <p>Validate response format is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"response_format\")\n@classmethod\ndef validate_response_format(cls, v: str) -&gt; str:\n    \"\"\"Validate response format is supported.\"\"\"\n    allowed = [\"raw\", \"json\", \"regex\"]\n    if v not in allowed:\n        raise ValueError(f\"response_format must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.PromptSpec.validate_batch_strategy","title":"validate_batch_strategy  <code>classmethod</code>","text":"<pre><code>validate_batch_strategy(v: str) -&gt; str\n</code></pre> <p>Validate batch strategy is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"batch_strategy\")\n@classmethod\ndef validate_batch_strategy(cls, v: str) -&gt; str:\n    \"\"\"Validate batch strategy is supported.\"\"\"\n    allowed = [\"json\", \"csv\"]\n    if v not in allowed:\n        raise ValueError(f\"batch_strategy must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec","title":"LLMSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for LLM provider configuration.</p> <p>Defines which LLM provider to use, model settings, and authentication. Supports OpenAI, Azure OpenAI, Anthropic, Groq, MLX, and custom OpenAI-compatible APIs.</p> <p>Attributes:</p> Name Type Description <code>provider</code> <code>LLMProvider</code> <p>LLM provider (openai, azure_openai, anthropic, groq, mlx, openai_compatible)</p> <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o-mini\", \"claude-sonnet-4\", \"llama-3.3-70b-versatile\")</p> <code>api_key</code> <code>str | None</code> <p>API key (optional, reads from environment if not provided)</p> <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0-2.0, default: 0.0 for deterministic output)</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum output tokens (optional, uses model default)</p> <code>top_p</code> <code>float</code> <p>Nucleus sampling parameter (0.0-1.0, default: 1.0)</p> Example <pre><code># OpenAI\nspec = LLMSpec(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    temperature=0.3\n)\n\n# Groq (fast and affordable)\nspec = LLMSpec(\n    provider=LLMProvider.GROQ,\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.0\n)\n\n# Azure with Managed Identity (no API key needed)\nspec = LLMSpec(\n    provider=LLMProvider.AZURE_OPENAI,\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    use_managed_identity=True\n)\n</code></pre> Note <p>Use LLMProviderPresets for pre-configured common providers.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec.validate_base_url_format","title":"validate_base_url_format  <code>classmethod</code>","text":"<pre><code>validate_base_url_format(v: str | None) -&gt; str | None\n</code></pre> <p>Validate base_url is a valid HTTP(S) URL with a host.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"base_url\")\n@classmethod\ndef validate_base_url_format(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate base_url is a valid HTTP(S) URL with a host.\"\"\"\n    if v is None:\n        return v\n    from urllib.parse import urlparse\n\n    parsed = urlparse(v)\n    if parsed.scheme not in {\"http\", \"https\"}:\n        raise ValueError(\"base_url must start with http:// or https://\")\n    if not parsed.netloc:\n        raise ValueError(\n            \"base_url must include a host (e.g., localhost, api.example.com)\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec.validate_azure_config","title":"validate_azure_config  <code>classmethod</code>","text":"<pre><code>validate_azure_config(v: str | None, info: Any) -&gt; str | None\n</code></pre> <p>Validate Azure-specific configuration.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"azure_endpoint\", \"azure_deployment\")\n@classmethod\ndef validate_azure_config(cls, v: str | None, info: Any) -&gt; str | None:\n    \"\"\"Validate Azure-specific configuration.\"\"\"\n    if info.data.get(\"provider\") == LLMProvider.AZURE_OPENAI and v is None:\n        field_name = info.field_name\n        raise ValueError(f\"{field_name} required for Azure OpenAI provider\")\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec.validate_provider_requirements","title":"validate_provider_requirements","text":"<pre><code>validate_provider_requirements() -&gt; LLMSpec\n</code></pre> <p>Validate provider-specific requirements.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    \"\"\"Validate provider-specific requirements.\"\"\"\n    # Check openai_compatible requires base_url\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.ProcessingSpec","title":"ProcessingSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for processing parameters.</p> <p>Controls how the pipeline executes: batch sizes, concurrency, error handling, rate limiting, and budget constraints.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Number of rows per batch (1-1000, default: 100)</p> <code>concurrency</code> <code>int</code> <p>Number of parallel LLM requests (1-20, default: 5)</p> <code>checkpoint_interval</code> <code>int</code> <p>Save checkpoint every N rows (default: 500)</p> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts for failed requests (default: 3)</p> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds (default: 1.0)</p> <code>error_policy</code> <code>ErrorPolicy</code> <p>How to handle errors (retry, skip, fail, use_default)</p> <code>rate_limit_rpm</code> <code>int | None</code> <p>Requests per minute limit (optional, no limit if None)</p> <code>max_budget</code> <code>Decimal | None</code> <p>Maximum cost in USD (optional, no limit if None)</p> <code>enable_preprocessing</code> <code>bool</code> <p>Enable input text preprocessing (default: False)</p> <code>preprocessing_max_length</code> <code>int</code> <p>Max characters after preprocessing (default: 500)</p> <code>auto_retry_failed</code> <code>bool</code> <p>Auto-retry rows with null/empty outputs (default: False)</p> <code>max_retry_attempts</code> <code>int</code> <p>Max retry attempts for failed rows (1-3, default: 1)</p> <code>use_jinja2</code> <code>bool</code> <p>Use Jinja2 for template rendering, enables loops/conditionals (default: False)</p> <code>progress_mode</code> <code>str</code> <p>Progress tracking mode (auto, rich, tqdm, logging, none)</p> Example <pre><code># Conservative settings (free tier)\nspec = ProcessingSpec(\n    batch_size=50,\n    concurrency=5,\n    rate_limit_rpm=25,\n    max_budget=Decimal(\"5.0\")\n)\n\n# Aggressive settings (paid tier)\nspec = ProcessingSpec(\n    batch_size=200,\n    concurrency=20,\n    rate_limit_rpm=100,\n    max_budget=Decimal(\"50.0\")\n)\n\n# Fault-tolerant settings\nspec = ProcessingSpec(\n    max_retries=5,\n    error_policy=ErrorPolicy.RETRY,\n    checkpoint_interval=100\n)\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.ProcessingSpec.validate_checkpoint_dir","title":"validate_checkpoint_dir  <code>classmethod</code>","text":"<pre><code>validate_checkpoint_dir(v: str | Path) -&gt; Path\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"checkpoint_dir\")\n@classmethod\ndef validate_checkpoint_dir(cls, v: str | Path) -&gt; Path:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.OutputSpec","title":"OutputSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for output configuration.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.OutputSpec.validate_destination_path","title":"validate_destination_path  <code>classmethod</code>","text":"<pre><code>validate_destination_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"destination_path\")\n@classmethod\ndef validate_destination_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.PipelineSpecifications","title":"PipelineSpecifications","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for all pipeline specifications.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets","title":"LLMProviderPresets","text":"<p>Pre-configured LLM provider specifications for common use cases.</p> <p>These presets provide convenient access to popular LLM providers with correct base URLs, pricing, and configuration. API keys must be provided at runtime via environment variables or explicit overrides.</p> Example Security Note <p>All presets have api_key=None by default. You must provide API keys at runtime via environment variables or explicit overrides.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets--use-preset-with-env-var-api-key","title":"Use preset with env var API key","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets--override-api-key","title":"Override API key","text":"<p>spec = LLMProviderPresets.TOGETHER_AI_LLAMA_70B.model_copy(     update={\"api_key\": \"your-key\"}  # pragma: allowlist secret ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets.create_custom_openai_compatible","title":"create_custom_openai_compatible  <code>classmethod</code>","text":"<pre><code>create_custom_openai_compatible(provider_name: str, model: str, base_url: str, input_cost_per_1k: float = 0.0, output_cost_per_1k: float = 0.0, **kwargs) -&gt; LLMSpec\n</code></pre> <p>Factory method for custom OpenAI-compatible providers.</p> <p>Use this for providers like vLLM, LocalAI, Anyscale, or any custom OpenAI-compatible API endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Display name for the provider (for logging/metrics)</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>base_url</code> <code>str</code> <p>API endpoint URL (e.g., http://localhost:8000/v1)</p> required <code>input_cost_per_1k</code> <code>float</code> <p>Input token cost per 1K tokens (default: 0.0)</p> <code>0.0</code> <code>output_cost_per_1k</code> <code>float</code> <p>Output token cost per 1K tokens (default: 0.0)</p> <code>0.0</code> <code>**kwargs</code> <p>Additional LLMSpec parameters (temperature, max_tokens, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMSpec</code> <p>Configured LLMSpec for the custom provider</p> Example <p>spec = LLMProviderPresets.create_custom_openai_compatible(     provider_name=\"My vLLM Server\",     model=\"mistral-7b-instruct\",     base_url=\"http://my-server:8000/v1\",     temperature=0.7 )</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@classmethod\ndef create_custom_openai_compatible(\n    cls,\n    provider_name: str,\n    model: str,\n    base_url: str,\n    input_cost_per_1k: float = 0.0,\n    output_cost_per_1k: float = 0.0,\n    **kwargs,\n) -&gt; LLMSpec:\n    \"\"\"\n    Factory method for custom OpenAI-compatible providers.\n\n    Use this for providers like vLLM, LocalAI, Anyscale, or any custom\n    OpenAI-compatible API endpoint.\n\n    Args:\n        provider_name: Display name for the provider (for logging/metrics)\n        model: Model identifier\n        base_url: API endpoint URL (e.g., http://localhost:8000/v1)\n        input_cost_per_1k: Input token cost per 1K tokens (default: 0.0)\n        output_cost_per_1k: Output token cost per 1K tokens (default: 0.0)\n        **kwargs: Additional LLMSpec parameters (temperature, max_tokens, etc.)\n\n    Returns:\n        Configured LLMSpec for the custom provider\n\n    Example:\n        spec = LLMProviderPresets.create_custom_openai_compatible(\n            provider_name=\"My vLLM Server\",\n            model=\"mistral-7b-instruct\",\n            base_url=\"http://my-server:8000/v1\",\n            temperature=0.7\n        )\n    \"\"\"\n    return LLMSpec(\n        provider=LLMProvider.OPENAI_COMPATIBLE,\n        provider_name=provider_name,\n        model=model,\n        base_url=base_url,\n        input_cost_per_1k_tokens=Decimal(str(input_cost_per_1k)),\n        output_cost_per_1k_tokens=Decimal(str(output_cost_per_1k)),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/integrations/","title":"integrations","text":""},{"location":"api/integrations/#ondine.integrations","title":"integrations","text":"<p>Framework integrations for popular orchestration tools.</p>"},{"location":"api/integrations/airflow/","title":"airflow","text":""},{"location":"api/integrations/airflow/#ondine.integrations.airflow","title":"airflow","text":"<p>Airflow integration - Pre-built operators for Apache Airflow.</p> <p>Provides LLMTransformOperator for easy integration into Airflow DAGs.</p>"},{"location":"api/integrations/airflow/#ondine.integrations.airflow.LLMTransformOperator","title":"LLMTransformOperator","text":"<pre><code>LLMTransformOperator(*args, **kwargs)\n</code></pre> <p>Placeholder when Airflow not installed.</p> Source code in <code>ondine/integrations/airflow.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    raise ImportError(\n        \"Apache Airflow is required to use LLMTransformOperator. \"\n        \"Install with: pip install apache-airflow\"\n    )\n</code></pre>"},{"location":"api/integrations/prefect/","title":"prefect","text":""},{"location":"api/integrations/prefect/#ondine.integrations.prefect","title":"prefect","text":"<p>Prefect integration - Pre-built tasks for Prefect workflows.</p> <p>Provides llm_transform_task for easy integration into Prefect flows.</p>"},{"location":"api/integrations/prefect/#ondine.integrations.prefect.llm_transform_task","title":"llm_transform_task","text":"<pre><code>llm_transform_task(config_path: str, input_data: DataFrame | None = None, input_file: str | None = None, output_file: str | None = None, max_budget: float | None = None, provider_override: str | None = None, model_override: str | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Prefect task for LLM dataset transformations.</p> <p>Integrates LLM Dataset Engine into Prefect flows.</p> Example <p>from prefect import flow from ondine.integrations.prefect import llm_transform_task</p> <p>@flow def data_pipeline():     raw_data = load_data()     enriched = llm_transform_task(         config_path='configs/llm_config.yaml',         input_data=raw_data,         max_budget=10.0,     )     save_data(enriched)</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to YAML/JSON configuration</p> required <code>input_data</code> <code>DataFrame | None</code> <p>Input DataFrame (from previous task)</p> <code>None</code> <code>input_file</code> <code>str | None</code> <p>Path to input file (alternative to input_data)</p> <code>None</code> <code>output_file</code> <code>str | None</code> <p>Path to output file (optional)</p> <code>None</code> <code>max_budget</code> <code>float | None</code> <p>Override maximum budget</p> <code>None</code> <code>provider_override</code> <code>str | None</code> <p>Override LLM provider</p> <code>None</code> <code>model_override</code> <code>str | None</code> <p>Override model name</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither input_data nor input_file provided</p> Source code in <code>ondine/integrations/prefect.py</code> <pre><code>@task(name=\"llm_transform\")\ndef llm_transform_task(\n    config_path: str,\n    input_data: pd.DataFrame | None = None,\n    input_file: str | None = None,\n    output_file: str | None = None,\n    max_budget: float | None = None,\n    provider_override: str | None = None,\n    model_override: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Prefect task for LLM dataset transformations.\n\n    Integrates LLM Dataset Engine into Prefect flows.\n\n    Example:\n        from prefect import flow\n        from ondine.integrations.prefect import llm_transform_task\n\n        @flow\n        def data_pipeline():\n            raw_data = load_data()\n            enriched = llm_transform_task(\n                config_path='configs/llm_config.yaml',\n                input_data=raw_data,\n                max_budget=10.0,\n            )\n            save_data(enriched)\n\n    Args:\n        config_path: Path to YAML/JSON configuration\n        input_data: Input DataFrame (from previous task)\n        input_file: Path to input file (alternative to input_data)\n        output_file: Path to output file (optional)\n        max_budget: Override maximum budget\n        provider_override: Override LLM provider\n        model_override: Override model name\n\n    Returns:\n        Result DataFrame\n\n    Raises:\n        ValueError: If neither input_data nor input_file provided\n    \"\"\"\n    if not PREFECT_AVAILABLE:\n        raise ImportError(\n            \"Prefect is required to use llm_transform_task. \"\n            \"Install with: pip install prefect\"\n        )\n\n    # Load configuration\n    specs = ConfigLoader.from_yaml(config_path)\n\n    # Override settings\n    if max_budget is not None:\n        from decimal import Decimal\n\n        specs.processing.max_budget = Decimal(str(max_budget))\n\n    if provider_override:\n        from ondine.core.specifications import LLMProvider\n\n        specs.llm.provider = LLMProvider(provider_override)\n\n    if model_override:\n        specs.llm.model = model_override\n\n    # Get input\n    if input_data is not None:\n        pipeline = Pipeline(specs, dataframe=input_data)\n    elif input_file:\n        specs.dataset.source_path = Path(input_file)\n        pipeline = Pipeline(specs)\n    else:\n        raise ValueError(\"Either input_data or input_file required\")\n\n    # Set output if specified\n    if output_file:\n        from ondine.core.specifications import (\n            DataSourceType,\n            MergeStrategy,\n            OutputSpec,\n        )\n\n        specs.output = OutputSpec(\n            destination_type=DataSourceType.CSV,\n            destination_path=Path(output_file),\n            merge_strategy=MergeStrategy.REPLACE,\n        )\n\n    # Execute\n    result = pipeline.execute()\n\n    # Log metrics (Prefect will capture)\n    print(f\"\u2705 Processed {result.metrics.total_rows} rows\")\n    print(f\"\ud83d\udcb0 Cost: ${result.costs.total_cost}\")\n    print(f\"\u23f1\ufe0f  Duration: {result.duration:.2f}s\")\n\n    return result.data\n</code></pre>"},{"location":"api/observability/","title":"observability","text":""},{"location":"api/observability/#ondine.observability","title":"observability","text":"<p>Observability toolkit for Ondine pipelines.</p> <p>Provides event-driven observability with support for multiple backends: - OpenTelemetry for infrastructure monitoring (Jaeger, Datadog, etc.) - Langfuse for LLM-specific observability (prompts, tokens, costs) - Logging for simple console/file output</p> Usage <p>from ondine import PipelineBuilder</p>"},{"location":"api/observability/#ondine.observability--add-observability-to-pipeline","title":"Add observability to pipeline","text":"<p>pipeline = ( ...     PipelineBuilder.create() ...     .from_csv(\"data.csv\", ...) ...     .with_prompt(\"...\") ...     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\") ...     .with_observer(\"langfuse\", config={ ...         \"public_key\": \"pk-lf-...\", ...         \"secret_key\": \"sk-lf-...\" ...     }) ...     .build() ... )</p>"},{"location":"api/observability/#ondine.observability--create-custom-observers","title":"Create custom observers","text":"<p>from ondine.observability import PipelineObserver, observer</p> <p>@observer(\"custom\") class CustomObserver(PipelineObserver): ...     def on_llm_call(self, event): ...         print(f\"LLM: {event.model} - ${event.cost}\")</p>"},{"location":"api/observability/#ondine.observability.PipelineObserver","title":"PipelineObserver","text":"<pre><code>PipelineObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for all pipeline observers.</p> <p>Observers receive events and can: - Log to files or stdout - Send to external services (Langfuse, OpenTelemetry, etc.) - Update metrics dashboards - Trigger alerts</p> <p>All methods are optional (default: no-op) except on_llm_call(), which is the most critical event for LLM observability.</p> <p>Observer implementations should be fault-tolerant - errors in observers should never crash the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Observer-specific configuration dictionary</p> <code>None</code> Example <p>class MyObserver(PipelineObserver):     def on_llm_call(self, event: LLMCallEvent) -&gt; None:         print(f\"LLM called: {event.model} - ${event.cost}\")</p> <p>Initialize observer with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Observer-specific configuration (e.g., API keys, endpoints)</p> <code>None</code> Source code in <code>ondine/observability/base.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize observer with configuration.\n\n    Args:\n        config: Observer-specific configuration (e.g., API keys, endpoints)\n    \"\"\"\n    self.config = config or {}\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(event: PipelineStartEvent) -&gt; None\n</code></pre> <p>Called when pipeline execution starts.</p> <p>Use this to initialize traces, create root spans, or start timers.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>PipelineStartEvent</code> <p>Pipeline start event with configuration</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_pipeline_start(self, event: PipelineStartEvent) -&gt; None:\n    \"\"\"\n    Called when pipeline execution starts.\n\n    Use this to initialize traces, create root spans, or start timers.\n\n    Args:\n        event: Pipeline start event with configuration\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(event: StageStartEvent) -&gt; None\n</code></pre> <p>Called when a pipeline stage begins execution.</p> <p>Use this to create nested spans or log stage transitions.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StageStartEvent</code> <p>Stage start event with stage details</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_stage_start(self, event: StageStartEvent) -&gt; None:\n    \"\"\"\n    Called when a pipeline stage begins execution.\n\n    Use this to create nested spans or log stage transitions.\n\n    Args:\n        event: Stage start event with stage details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.on_llm_call","title":"on_llm_call  <code>abstractmethod</code>","text":"<pre><code>on_llm_call(event: LLMCallEvent) -&gt; None\n</code></pre> <p>Called on every LLM invocation.</p> <p>This is the MOST CRITICAL method for LLM observability. ALL observers MUST implement this method.</p> <p>Contains full prompt, completion, tokens, cost, and metadata. Observers should handle PII sanitization if needed.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LLMCallEvent</code> <p>LLM call event with request/response details</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>@abstractmethod\ndef on_llm_call(self, event: LLMCallEvent) -&gt; None:\n    \"\"\"\n    Called on every LLM invocation.\n\n    This is the MOST CRITICAL method for LLM observability.\n    ALL observers MUST implement this method.\n\n    Contains full prompt, completion, tokens, cost, and metadata.\n    Observers should handle PII sanitization if needed.\n\n    Args:\n        event: LLM call event with request/response details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.on_stage_end","title":"on_stage_end","text":"<pre><code>on_stage_end(event: StageEndEvent) -&gt; None\n</code></pre> <p>Called when a pipeline stage completes.</p> <p>Use this to close spans, log duration, or record metrics.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StageEndEvent</code> <p>Stage end event with success status and metrics</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_stage_end(self, event: StageEndEvent) -&gt; None:\n    \"\"\"\n    Called when a pipeline stage completes.\n\n    Use this to close spans, log duration, or record metrics.\n\n    Args:\n        event: Stage end event with success status and metrics\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.on_error","title":"on_error","text":"<pre><code>on_error(event: ErrorEvent) -&gt; None\n</code></pre> <p>Called when errors occur during execution.</p> <p>Use this for error tracking, alerting, or debugging.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>ErrorEvent</code> <p>Error event with exception details and context</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_error(self, event: ErrorEvent) -&gt; None:\n    \"\"\"\n    Called when errors occur during execution.\n\n    Use this for error tracking, alerting, or debugging.\n\n    Args:\n        event: Error event with exception details and context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.on_pipeline_end","title":"on_pipeline_end","text":"<pre><code>on_pipeline_end(event: PipelineEndEvent) -&gt; None\n</code></pre> <p>Called when pipeline execution completes.</p> <p>Use this to close root spans, log final metrics, or cleanup.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>PipelineEndEvent</code> <p>Pipeline end event with final statistics</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_pipeline_end(self, event: PipelineEndEvent) -&gt; None:\n    \"\"\"\n    Called when pipeline execution completes.\n\n    Use this to close root spans, log final metrics, or cleanup.\n\n    Args:\n        event: Pipeline end event with final statistics\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush any buffered events.</p> <p>Called at the end of pipeline execution to ensure all events are sent before the pipeline exits.</p> <p>Observers that batch events should send them here.</p> Source code in <code>ondine/observability/base.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"\n    Flush any buffered events.\n\n    Called at the end of pipeline execution to ensure\n    all events are sent before the pipeline exits.\n\n    Observers that batch events should send them here.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.PipelineObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Clean up resources.</p> <p>Called when observer is no longer needed. Close connections, files, or other resources.</p> Source code in <code>ondine/observability/base.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Clean up resources.\n\n    Called when observer is no longer needed.\n    Close connections, files, or other resources.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher","title":"ObserverDispatcher","text":"<pre><code>ObserverDispatcher(observers: list[PipelineObserver])\n</code></pre> <p>Coordinates event dispatch to all registered observers.</p> <p>Ensures: - Events are sent to all observers - Observer failures don't crash the pipeline - Errors are logged for debugging - Clean shutdown with flush and close</p> <p>Design Note: This implementation uses synchronous dispatch for simplicity. The architecture is ready for async dispatch if needed in the future (replace <code>dispatch()</code> with <code>async def dispatch()</code> and use asyncio.gather()).</p> Example <p>observers = [LangfuseObserver(config), OpenTelemetryObserver(config)] dispatcher = ObserverDispatcher(observers)</p> <p>Initialize dispatcher with observers.</p> <p>Parameters:</p> Name Type Description Default <code>observers</code> <code>list[PipelineObserver]</code> <p>List of observer instances to notify</p> required Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def __init__(self, observers: list[PipelineObserver]):\n    \"\"\"\n    Initialize dispatcher with observers.\n\n    Args:\n        observers: List of observer instances to notify\n    \"\"\"\n    self.observers = observers\n    self.logger = logger\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher--dispatch-events","title":"Dispatch events","text":"<p>dispatcher.dispatch(\"llm_call\", LLMCallEvent(...))</p>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher--at-pipeline-end","title":"At pipeline end","text":"<p>dispatcher.flush_all() dispatcher.close_all()</p>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher.dispatch","title":"dispatch","text":"<pre><code>dispatch(event_type: str, event: Any) -&gt; None\n</code></pre> <p>Dispatch event to all observers.</p> <p>Each observer is called in isolation - if one fails, others still receive the event. This ensures pipeline reliability isn't compromised by observability issues.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>str</code> <p>Event type name (e.g., \"llm_call\", \"pipeline_start\")</p> required <code>event</code> <code>Any</code> <p>Event object to dispatch</p> required <p>Note: Errors are logged but not raised to prevent observer failures from crashing the pipeline.</p> Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def dispatch(self, event_type: str, event: Any) -&gt; None:\n    \"\"\"\n    Dispatch event to all observers.\n\n    Each observer is called in isolation - if one fails,\n    others still receive the event. This ensures pipeline\n    reliability isn't compromised by observability issues.\n\n    Args:\n        event_type: Event type name (e.g., \"llm_call\", \"pipeline_start\")\n        event: Event object to dispatch\n\n    Note: Errors are logged but not raised to prevent observer\n    failures from crashing the pipeline.\n    \"\"\"\n    method_name = f\"on_{event_type}\"\n\n    for observer in self.observers:\n        try:\n            # Get the method for this event type\n            method = getattr(observer, method_name, None)\n\n            if method is None:\n                # Observer doesn't implement this event handler (OK - it's optional)\n                continue\n\n            # Call the observer method\n            method(event)\n\n        except Exception as e:\n            # Log the error but continue to other observers\n            # Observer failures should NEVER crash the pipeline\n            self.logger.error(\n                f\"Observer {observer.__class__.__name__} failed on {event_type}: {e}\\n\"\n                f\"Traceback:\\n{traceback.format_exc()}\\n\"\n                f\"Pipeline will continue without this observer's data.\"\n            )\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher.flush_all","title":"flush_all","text":"<pre><code>flush_all() -&gt; None\n</code></pre> <p>Flush all observers.</p> <p>Calls flush() on each observer to ensure buffered events are sent. This should be called at the end of pipeline execution.</p> Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def flush_all(self) -&gt; None:\n    \"\"\"\n    Flush all observers.\n\n    Calls flush() on each observer to ensure buffered events are sent.\n    This should be called at the end of pipeline execution.\n    \"\"\"\n    for observer in self.observers:\n        try:\n            observer.flush()\n        except Exception as e:\n            self.logger.error(\n                f\"Observer {observer.__class__.__name__} flush failed: {e}\\n\"\n                f\"Some events may not have been sent.\"\n            )\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher.close_all","title":"close_all","text":"<pre><code>close_all() -&gt; None\n</code></pre> <p>Close all observers.</p> <p>Calls close() on each observer to clean up resources. This should be called when the pipeline is fully complete.</p> Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def close_all(self) -&gt; None:\n    \"\"\"\n    Close all observers.\n\n    Calls close() on each observer to clean up resources.\n    This should be called when the pipeline is fully complete.\n    \"\"\"\n    for observer in self.observers:\n        try:\n            observer.close()\n        except Exception as e:\n            self.logger.error(\n                f\"Observer {observer.__class__.__name__} close failed: {e}\\n\"\n                f\"Resource cleanup may be incomplete.\"\n            )\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: PipelineObserver) -&gt; None\n</code></pre> <p>Add an observer dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>PipelineObserver</code> <p>Observer instance to add</p> required Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def add_observer(self, observer: PipelineObserver) -&gt; None:\n    \"\"\"\n    Add an observer dynamically.\n\n    Args:\n        observer: Observer instance to add\n    \"\"\"\n    self.observers.append(observer)\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverDispatcher.remove_observer","title":"remove_observer","text":"<pre><code>remove_observer(observer: PipelineObserver) -&gt; None\n</code></pre> <p>Remove an observer dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>PipelineObserver</code> <p>Observer instance to remove</p> required Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def remove_observer(self, observer: PipelineObserver) -&gt; None:\n    \"\"\"\n    Remove an observer dynamically.\n\n    Args:\n        observer: Observer instance to remove\n    \"\"\"\n    if observer in self.observers:\n        self.observers.remove(observer)\n</code></pre>"},{"location":"api/observability/#ondine.observability.ErrorEvent","title":"ErrorEvent  <code>dataclass</code>","text":"<pre><code>ErrorEvent(pipeline_id: UUID, run_id: UUID, timestamp: datetime, trace_id: str, span_id: str, stage_name: str | None = None, row_index: int | None = None, error: Exception = None, error_type: str = '', error_message: str = '', stack_trace: str = '', context: dict[str, Any] = dict())\n</code></pre> <p>Emitted when errors occur during pipeline execution.</p> <p>Captures error context for debugging and alerting.</p>"},{"location":"api/observability/#ondine.observability.LLMCallEvent","title":"LLMCallEvent  <code>dataclass</code>","text":"<pre><code>LLMCallEvent(pipeline_id: UUID, run_id: UUID, stage_name: str, row_index: int, timestamp: datetime, trace_id: str, span_id: str, prompt: str, model: str, provider: str, temperature: float, completion: str, parent_span_id: str | None = None, max_tokens: int | None = None, system_message: str | None = None, finish_reason: str = 'stop', input_tokens: int = 0, output_tokens: int = 0, total_tokens: int = 0, cost: Decimal = (lambda: Decimal('0.0'))(), latency_ms: float = 0.0, rag_context: str | None = None, rag_sources: list[dict] | None = None, rag_technique: str | None = None, retrieval_latency_ms: float | None = None, prompt_template_id: str | None = None, prompt_version: str | None = None, metadata: dict[str, Any] = dict())\n</code></pre> <p>Emitted on every LLM invocation.</p> <p>This is the MOST IMPORTANT event for LLM observability. Contains full prompt/completion text, tokens, cost, and optional RAG metadata.</p> <p>Observers can choose to truncate or sanitize prompts based on their needs.</p>"},{"location":"api/observability/#ondine.observability.PipelineEndEvent","title":"PipelineEndEvent  <code>dataclass</code>","text":"<pre><code>PipelineEndEvent(pipeline_id: UUID, run_id: UUID, success: bool, timestamp: datetime, trace_id: str, span_id: str, total_duration_ms: float = 0.0, rows_processed: int = 0, rows_succeeded: int = 0, rows_failed: int = 0, rows_skipped: int = 0, total_cost: Decimal = (lambda: Decimal('0.0'))(), total_tokens: int = 0, input_tokens: int = 0, output_tokens: int = 0, metrics: dict[str, Any] = dict())\n</code></pre> <p>Emitted when pipeline execution completes.</p> <p>Contains final metrics for the entire pipeline run.</p>"},{"location":"api/observability/#ondine.observability.PipelineStartEvent","title":"PipelineStartEvent  <code>dataclass</code>","text":"<pre><code>PipelineStartEvent(pipeline_id: UUID, run_id: UUID, timestamp: datetime, trace_id: str, span_id: str, config: dict[str, Any] = dict(), metadata: dict[str, Any] = dict(), total_rows: int = 0)\n</code></pre> <p>Emitted when pipeline execution starts.</p> <p>Contains pipeline configuration and metadata for the entire run.</p>"},{"location":"api/observability/#ondine.observability.StageEndEvent","title":"StageEndEvent  <code>dataclass</code>","text":"<pre><code>StageEndEvent(pipeline_id: UUID, run_id: UUID, stage_name: str, success: bool, timestamp: datetime, trace_id: str, span_id: str, duration_ms: float = 0.0, rows_processed: int = 0, error: Exception | None = None, metrics: dict[str, Any] = dict())\n</code></pre> <p>Emitted when a pipeline stage completes successfully or with errors.</p> <p>Contains stage-level metrics and success status.</p>"},{"location":"api/observability/#ondine.observability.StageStartEvent","title":"StageStartEvent  <code>dataclass</code>","text":"<pre><code>StageStartEvent(pipeline_id: UUID, run_id: UUID, stage_name: str, stage_type: str, timestamp: datetime, trace_id: str, span_id: str, parent_span_id: str | None = None, metadata: dict[str, Any] = dict())\n</code></pre> <p>Emitted when a pipeline stage begins execution.</p> <p>Tracks which stage is starting and when.</p>"},{"location":"api/observability/#ondine.observability.LoggingObserver","title":"LoggingObserver","text":"<pre><code>LoggingObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's Simple handler.</p> <p>LlamaIndex automatically logs: - \u2705 LLM calls with prompts and completions - \u2705 Token usage - \u2705 Latency metrics</p> <p>This provides basic console logging without external dependencies.</p> Configuration <ul> <li>(No specific config needed - uses LlamaIndex defaults)</li> </ul> Example <p>observer = LoggingObserver(config={})</p> <p>Initialize logging observer.</p> <p>Configures LlamaIndex's Simple handler for console logging.</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize logging observer.\n\n    Configures LlamaIndex's Simple handler for console logging.\n    \"\"\"\n    super().__init__(config)\n\n    # Configure LlamaIndex's Simple handler (console logging)\n    # This will automatically log all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"simple\", self.config)\n\n    logger.info(\"Logging observer initialized (using LlamaIndex SimpleHandler)\")\n</code></pre>"},{"location":"api/observability/#ondine.observability.LoggingObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically logged by LlamaIndex.</p> <p>No action needed - LlamaIndex's Simple handler logs: - Prompt and completion - Token usage - Latency</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically logged by LlamaIndex.\n\n    No action needed - LlamaIndex's Simple handler logs:\n    - Prompt and completion\n    - Token usage\n    - Latency\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.LoggingObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush (handled by logging module).</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush (handled by logging module).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.LoggingObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by logging module).</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by logging module).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.OpenTelemetryObserver","title":"OpenTelemetryObserver","text":"<pre><code>OpenTelemetryObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's OpenTelemetry handler.</p> <p>LlamaIndex automatically instruments: - \u2705 All LLM calls (prompts, completions, tokens, latency) - \u2705 Embeddings - \u2705 Retrieval operations (when using QueryEngines)</p> <p>This observer configures the LlamaIndex handler and can add pipeline-level spans on top if needed.</p> Configuration <ul> <li>Any config accepted by LlamaIndex's OpenTelemetry handler</li> <li>See: https://docs.llamaindex.ai/en/stable/module_guides/observability/</li> </ul> Example <p>observer = OpenTelemetryObserver(config={})</p> <p>Initialize OpenTelemetry observer.</p> <p>Configures LlamaIndex's global OpenTelemetry handler.</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize OpenTelemetry observer.\n\n    Configures LlamaIndex's global OpenTelemetry handler.\n    \"\"\"\n    super().__init__(config)\n\n    # Configure LlamaIndex's OpenTelemetry handler\n    # This will automatically instrument all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"opentelemetry\", self.config)\n\n    logger.info(\"OpenTelemetry observer initialized (using LlamaIndex handler)\")\n</code></pre>"},{"location":"api/observability/#ondine.observability.OpenTelemetryObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically traced by LlamaIndex.</p> <p>No action needed - LlamaIndex's OpenTelemetry handler captures: - Prompt and completion - Token usage - Latency - Model information</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically traced by LlamaIndex.\n\n    No action needed - LlamaIndex's OpenTelemetry handler captures:\n    - Prompt and completion\n    - Token usage\n    - Latency\n    - Model information\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.OpenTelemetryObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush spans (handled by OpenTelemetry SDK).</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush spans (handled by OpenTelemetry SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.OpenTelemetryObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by OpenTelemetry SDK).</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by OpenTelemetry SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverRegistry","title":"ObserverRegistry","text":"<p>Global registry of observer implementations.</p> <p>Allows dynamic observer registration and instantiation via decorator pattern.</p> Example <p>@observer(\"my_observer\") class MyObserver(PipelineObserver):     def on_llm_call(self, event):         print(f\"LLM called: {event.model}\")</p>"},{"location":"api/observability/#ondine.observability.ObserverRegistry--later-instantiate-by-name","title":"Later, instantiate by name","text":"<p>observer_class = ObserverRegistry.get(\"my_observer\") observer = observer_class(config={...})</p>"},{"location":"api/observability/#ondine.observability.ObserverRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(name: str, observer_class: type[PipelineObserver]) -&gt; None\n</code></pre> <p>Register an observer implementation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier (e.g., \"langfuse\", \"opentelemetry\")</p> required <code>observer_class</code> <code>type[PipelineObserver]</code> <p>Observer class implementing PipelineObserver</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If observer already registered or invalid class</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef register(cls, name: str, observer_class: type[PipelineObserver]) -&gt; None:\n    \"\"\"\n    Register an observer implementation.\n\n    Args:\n        name: Observer identifier (e.g., \"langfuse\", \"opentelemetry\")\n        observer_class: Observer class implementing PipelineObserver\n\n    Raises:\n        ValueError: If observer already registered or invalid class\n    \"\"\"\n    if name in cls._observers:\n        raise ValueError(\n            f\"Observer '{name}' already registered. \"\n            f\"Use a different name or unregister first.\"\n        )\n\n    if not issubclass(observer_class, PipelineObserver):\n        raise ValueError(\n            f\"Observer class must inherit from PipelineObserver, \"\n            f\"got {observer_class.__name__}\"\n        )\n\n    cls._observers[name] = observer_class\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(name: str) -&gt; type[PipelineObserver]\n</code></pre> <p>Get observer class by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier</p> required <p>Returns:</p> Type Description <code>type[PipelineObserver]</code> <p>Observer class (not instantiated)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If observer not found</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; type[PipelineObserver]:\n    \"\"\"\n    Get observer class by name.\n\n    Args:\n        name: Observer identifier\n\n    Returns:\n        Observer class (not instantiated)\n\n    Raises:\n        ValueError: If observer not found\n    \"\"\"\n    if name not in cls._observers:\n        available = \", \".join(cls._observers.keys()) if cls._observers else \"none\"\n        raise ValueError(\n            f\"Observer '{name}' not found. Available observers: {available}\"\n        )\n\n    return cls._observers[name]\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverRegistry.list_observers","title":"list_observers  <code>classmethod</code>","text":"<pre><code>list_observers() -&gt; list[str]\n</code></pre> <p>List all registered observer names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of observer identifiers</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef list_observers(cls) -&gt; list[str]:\n    \"\"\"\n    List all registered observer names.\n\n    Returns:\n        List of observer identifiers\n    \"\"\"\n    return list(cls._observers.keys())\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(name: str) -&gt; bool\n</code></pre> <p>Check if observer is registered.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, name: str) -&gt; bool:\n    \"\"\"\n    Check if observer is registered.\n\n    Args:\n        name: Observer identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    return name in cls._observers\n</code></pre>"},{"location":"api/observability/#ondine.observability.ObserverRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(name: str) -&gt; None\n</code></pre> <p>Unregister an observer (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier</p> required Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef unregister(cls, name: str) -&gt; None:\n    \"\"\"\n    Unregister an observer (mainly for testing).\n\n    Args:\n        name: Observer identifier\n    \"\"\"\n    cls._observers.pop(name, None)\n</code></pre>"},{"location":"api/observability/#ondine.observability.sanitize_event","title":"sanitize_event","text":"<pre><code>sanitize_event(event: LLMCallEvent, config: dict[str, Any] | None = None) -&gt; LLMCallEvent\n</code></pre> <p>Sanitize an LLM call event based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LLMCallEvent</code> <p>LLM call event to sanitize</p> required <code>config</code> <code>dict[str, Any] | None</code> <p>Sanitization configuration: - sanitize_prompts: bool (default: True) - sanitize_completions: bool (default: True) - custom_patterns: dict[str, str] (additional regex patterns) - replacement: str (default: \"[REDACTED]\")</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMCallEvent</code> <p>New LLMCallEvent with sanitized fields</p> Example <p>config = {     \"sanitize_prompts\": True,     \"sanitize_completions\": False,     \"custom_patterns\": {\"account_id\": r\"ACC-\\d{6}\"} } sanitized = sanitize_event(event, config)</p> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_event(\n    event: LLMCallEvent,\n    config: dict[str, Any] | None = None,\n) -&gt; LLMCallEvent:\n    r\"\"\"\n    Sanitize an LLM call event based on configuration.\n\n    Args:\n        event: LLM call event to sanitize\n        config: Sanitization configuration:\n            - sanitize_prompts: bool (default: True)\n            - sanitize_completions: bool (default: True)\n            - custom_patterns: dict[str, str] (additional regex patterns)\n            - replacement: str (default: \"[REDACTED]\")\n\n    Returns:\n        New LLMCallEvent with sanitized fields\n\n    Example:\n        config = {\n            \"sanitize_prompts\": True,\n            \"sanitize_completions\": False,\n            \"custom_patterns\": {\"account_id\": r\"ACC-\\d{6}\"}\n        }\n        sanitized = sanitize_event(event, config)\n    \"\"\"\n    if config is None:\n        config = {}\n\n    sanitize_prompts = config.get(\"sanitize_prompts\", True)\n    sanitize_completions = config.get(\"sanitize_completions\", True)\n    custom_patterns_dict = config.get(\"custom_patterns\", {})\n    replacement = config.get(\"replacement\", \"[REDACTED]\")\n\n    # Build combined patterns\n    patterns = PII_PATTERNS.copy()\n    for name, pattern_str in custom_patterns_dict.items():\n        patterns[name] = re.compile(pattern_str)\n\n    # Create new event with sanitized fields\n    sanitized_prompt = event.prompt\n    sanitized_completion = event.completion\n    sanitized_rag_context = event.rag_context\n\n    if sanitize_prompts:\n        sanitized_prompt = sanitize_text(event.prompt, patterns, replacement)\n        if event.rag_context:\n            sanitized_rag_context = sanitize_text(\n                event.rag_context, patterns, replacement\n            )\n\n    if sanitize_completions:\n        sanitized_completion = sanitize_text(event.completion, patterns, replacement)\n\n    # Return new event (events are immutable dataclasses)\n    # We use replace() to create a new instance\n    from dataclasses import replace\n\n    return replace(\n        event,\n        prompt=sanitized_prompt,\n        completion=sanitized_completion,\n        rag_context=sanitized_rag_context,\n    )\n</code></pre>"},{"location":"api/observability/#ondine.observability.sanitize_text","title":"sanitize_text","text":"<pre><code>sanitize_text(text: str, patterns: dict[str, Pattern] | None = None, replacement: str = '[REDACTED]') -&gt; str\n</code></pre> <p>Sanitize text by replacing PII patterns.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to sanitize</p> required <code>patterns</code> <code>dict[str, Pattern] | None</code> <p>Dictionary of pattern name -&gt; regex pattern (uses defaults if None)</p> <code>None</code> <code>replacement</code> <code>str</code> <p>Replacement string for matched patterns</p> <code>'[REDACTED]'</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized text with PII replaced</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sanitize_text(\"Email me at test@example.com\")\n'Email me at [REDACTED]'\n&gt;&gt;&gt; sanitize_text(\"SSN: 123-45-6789\")\n'SSN: [REDACTED]'\n</code></pre> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_text(\n    text: str,\n    patterns: dict[str, re.Pattern] | None = None,\n    replacement: str = \"[REDACTED]\",\n) -&gt; str:\n    \"\"\"\n    Sanitize text by replacing PII patterns.\n\n    Args:\n        text: Text to sanitize\n        patterns: Dictionary of pattern name -&gt; regex pattern (uses defaults if None)\n        replacement: Replacement string for matched patterns\n\n    Returns:\n        Sanitized text with PII replaced\n\n    Examples:\n        &gt;&gt;&gt; sanitize_text(\"Email me at test@example.com\")\n        'Email me at [REDACTED]'\n        &gt;&gt;&gt; sanitize_text(\"SSN: 123-45-6789\")\n        'SSN: [REDACTED]'\n    \"\"\"\n    if patterns is None:\n        patterns = PII_PATTERNS\n\n    sanitized = text\n    for pattern_name, pattern in patterns.items():\n        sanitized = pattern.sub(replacement, sanitized)\n\n    return sanitized\n</code></pre>"},{"location":"api/observability/base/","title":"base","text":""},{"location":"api/observability/base/#ondine.observability.base","title":"base","text":"<p>Base class for pipeline observers.</p> <p>Observers receive events during pipeline execution and can log, trace, or send data to external observability systems.</p>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver","title":"PipelineObserver","text":"<pre><code>PipelineObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for all pipeline observers.</p> <p>Observers receive events and can: - Log to files or stdout - Send to external services (Langfuse, OpenTelemetry, etc.) - Update metrics dashboards - Trigger alerts</p> <p>All methods are optional (default: no-op) except on_llm_call(), which is the most critical event for LLM observability.</p> <p>Observer implementations should be fault-tolerant - errors in observers should never crash the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Observer-specific configuration dictionary</p> <code>None</code> Example <p>class MyObserver(PipelineObserver):     def on_llm_call(self, event: LLMCallEvent) -&gt; None:         print(f\"LLM called: {event.model} - ${event.cost}\")</p> <p>Initialize observer with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any] | None</code> <p>Observer-specific configuration (e.g., API keys, endpoints)</p> <code>None</code> Source code in <code>ondine/observability/base.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize observer with configuration.\n\n    Args:\n        config: Observer-specific configuration (e.g., API keys, endpoints)\n    \"\"\"\n    self.config = config or {}\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(event: PipelineStartEvent) -&gt; None\n</code></pre> <p>Called when pipeline execution starts.</p> <p>Use this to initialize traces, create root spans, or start timers.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>PipelineStartEvent</code> <p>Pipeline start event with configuration</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_pipeline_start(self, event: PipelineStartEvent) -&gt; None:\n    \"\"\"\n    Called when pipeline execution starts.\n\n    Use this to initialize traces, create root spans, or start timers.\n\n    Args:\n        event: Pipeline start event with configuration\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(event: StageStartEvent) -&gt; None\n</code></pre> <p>Called when a pipeline stage begins execution.</p> <p>Use this to create nested spans or log stage transitions.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StageStartEvent</code> <p>Stage start event with stage details</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_stage_start(self, event: StageStartEvent) -&gt; None:\n    \"\"\"\n    Called when a pipeline stage begins execution.\n\n    Use this to create nested spans or log stage transitions.\n\n    Args:\n        event: Stage start event with stage details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.on_llm_call","title":"on_llm_call  <code>abstractmethod</code>","text":"<pre><code>on_llm_call(event: LLMCallEvent) -&gt; None\n</code></pre> <p>Called on every LLM invocation.</p> <p>This is the MOST CRITICAL method for LLM observability. ALL observers MUST implement this method.</p> <p>Contains full prompt, completion, tokens, cost, and metadata. Observers should handle PII sanitization if needed.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LLMCallEvent</code> <p>LLM call event with request/response details</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>@abstractmethod\ndef on_llm_call(self, event: LLMCallEvent) -&gt; None:\n    \"\"\"\n    Called on every LLM invocation.\n\n    This is the MOST CRITICAL method for LLM observability.\n    ALL observers MUST implement this method.\n\n    Contains full prompt, completion, tokens, cost, and metadata.\n    Observers should handle PII sanitization if needed.\n\n    Args:\n        event: LLM call event with request/response details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.on_stage_end","title":"on_stage_end","text":"<pre><code>on_stage_end(event: StageEndEvent) -&gt; None\n</code></pre> <p>Called when a pipeline stage completes.</p> <p>Use this to close spans, log duration, or record metrics.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>StageEndEvent</code> <p>Stage end event with success status and metrics</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_stage_end(self, event: StageEndEvent) -&gt; None:\n    \"\"\"\n    Called when a pipeline stage completes.\n\n    Use this to close spans, log duration, or record metrics.\n\n    Args:\n        event: Stage end event with success status and metrics\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.on_error","title":"on_error","text":"<pre><code>on_error(event: ErrorEvent) -&gt; None\n</code></pre> <p>Called when errors occur during execution.</p> <p>Use this for error tracking, alerting, or debugging.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>ErrorEvent</code> <p>Error event with exception details and context</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_error(self, event: ErrorEvent) -&gt; None:\n    \"\"\"\n    Called when errors occur during execution.\n\n    Use this for error tracking, alerting, or debugging.\n\n    Args:\n        event: Error event with exception details and context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.on_pipeline_end","title":"on_pipeline_end","text":"<pre><code>on_pipeline_end(event: PipelineEndEvent) -&gt; None\n</code></pre> <p>Called when pipeline execution completes.</p> <p>Use this to close root spans, log final metrics, or cleanup.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>PipelineEndEvent</code> <p>Pipeline end event with final statistics</p> required Source code in <code>ondine/observability/base.py</code> <pre><code>def on_pipeline_end(self, event: PipelineEndEvent) -&gt; None:\n    \"\"\"\n    Called when pipeline execution completes.\n\n    Use this to close root spans, log final metrics, or cleanup.\n\n    Args:\n        event: Pipeline end event with final statistics\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush any buffered events.</p> <p>Called at the end of pipeline execution to ensure all events are sent before the pipeline exits.</p> <p>Observers that batch events should send them here.</p> Source code in <code>ondine/observability/base.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"\n    Flush any buffered events.\n\n    Called at the end of pipeline execution to ensure\n    all events are sent before the pipeline exits.\n\n    Observers that batch events should send them here.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/base/#ondine.observability.base.PipelineObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Clean up resources.</p> <p>Called when observer is no longer needed. Close connections, files, or other resources.</p> Source code in <code>ondine/observability/base.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Clean up resources.\n\n    Called when observer is no longer needed.\n    Close connections, files, or other resources.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/dispatcher/","title":"dispatcher","text":""},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher","title":"dispatcher","text":"<p>Event dispatcher for coordinating observer notifications.</p> <p>Ensures that events are delivered to all observers even if some fail.</p>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher","title":"ObserverDispatcher","text":"<pre><code>ObserverDispatcher(observers: list[PipelineObserver])\n</code></pre> <p>Coordinates event dispatch to all registered observers.</p> <p>Ensures: - Events are sent to all observers - Observer failures don't crash the pipeline - Errors are logged for debugging - Clean shutdown with flush and close</p> <p>Design Note: This implementation uses synchronous dispatch for simplicity. The architecture is ready for async dispatch if needed in the future (replace <code>dispatch()</code> with <code>async def dispatch()</code> and use asyncio.gather()).</p> Example <p>observers = [LangfuseObserver(config), OpenTelemetryObserver(config)] dispatcher = ObserverDispatcher(observers)</p> <p>Initialize dispatcher with observers.</p> <p>Parameters:</p> Name Type Description Default <code>observers</code> <code>list[PipelineObserver]</code> <p>List of observer instances to notify</p> required Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def __init__(self, observers: list[PipelineObserver]):\n    \"\"\"\n    Initialize dispatcher with observers.\n\n    Args:\n        observers: List of observer instances to notify\n    \"\"\"\n    self.observers = observers\n    self.logger = logger\n</code></pre>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher--dispatch-events","title":"Dispatch events","text":"<p>dispatcher.dispatch(\"llm_call\", LLMCallEvent(...))</p>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher--at-pipeline-end","title":"At pipeline end","text":"<p>dispatcher.flush_all() dispatcher.close_all()</p>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher.dispatch","title":"dispatch","text":"<pre><code>dispatch(event_type: str, event: Any) -&gt; None\n</code></pre> <p>Dispatch event to all observers.</p> <p>Each observer is called in isolation - if one fails, others still receive the event. This ensures pipeline reliability isn't compromised by observability issues.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>str</code> <p>Event type name (e.g., \"llm_call\", \"pipeline_start\")</p> required <code>event</code> <code>Any</code> <p>Event object to dispatch</p> required <p>Note: Errors are logged but not raised to prevent observer failures from crashing the pipeline.</p> Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def dispatch(self, event_type: str, event: Any) -&gt; None:\n    \"\"\"\n    Dispatch event to all observers.\n\n    Each observer is called in isolation - if one fails,\n    others still receive the event. This ensures pipeline\n    reliability isn't compromised by observability issues.\n\n    Args:\n        event_type: Event type name (e.g., \"llm_call\", \"pipeline_start\")\n        event: Event object to dispatch\n\n    Note: Errors are logged but not raised to prevent observer\n    failures from crashing the pipeline.\n    \"\"\"\n    method_name = f\"on_{event_type}\"\n\n    for observer in self.observers:\n        try:\n            # Get the method for this event type\n            method = getattr(observer, method_name, None)\n\n            if method is None:\n                # Observer doesn't implement this event handler (OK - it's optional)\n                continue\n\n            # Call the observer method\n            method(event)\n\n        except Exception as e:\n            # Log the error but continue to other observers\n            # Observer failures should NEVER crash the pipeline\n            self.logger.error(\n                f\"Observer {observer.__class__.__name__} failed on {event_type}: {e}\\n\"\n                f\"Traceback:\\n{traceback.format_exc()}\\n\"\n                f\"Pipeline will continue without this observer's data.\"\n            )\n</code></pre>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher.flush_all","title":"flush_all","text":"<pre><code>flush_all() -&gt; None\n</code></pre> <p>Flush all observers.</p> <p>Calls flush() on each observer to ensure buffered events are sent. This should be called at the end of pipeline execution.</p> Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def flush_all(self) -&gt; None:\n    \"\"\"\n    Flush all observers.\n\n    Calls flush() on each observer to ensure buffered events are sent.\n    This should be called at the end of pipeline execution.\n    \"\"\"\n    for observer in self.observers:\n        try:\n            observer.flush()\n        except Exception as e:\n            self.logger.error(\n                f\"Observer {observer.__class__.__name__} flush failed: {e}\\n\"\n                f\"Some events may not have been sent.\"\n            )\n</code></pre>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher.close_all","title":"close_all","text":"<pre><code>close_all() -&gt; None\n</code></pre> <p>Close all observers.</p> <p>Calls close() on each observer to clean up resources. This should be called when the pipeline is fully complete.</p> Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def close_all(self) -&gt; None:\n    \"\"\"\n    Close all observers.\n\n    Calls close() on each observer to clean up resources.\n    This should be called when the pipeline is fully complete.\n    \"\"\"\n    for observer in self.observers:\n        try:\n            observer.close()\n        except Exception as e:\n            self.logger.error(\n                f\"Observer {observer.__class__.__name__} close failed: {e}\\n\"\n                f\"Resource cleanup may be incomplete.\"\n            )\n</code></pre>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: PipelineObserver) -&gt; None\n</code></pre> <p>Add an observer dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>PipelineObserver</code> <p>Observer instance to add</p> required Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def add_observer(self, observer: PipelineObserver) -&gt; None:\n    \"\"\"\n    Add an observer dynamically.\n\n    Args:\n        observer: Observer instance to add\n    \"\"\"\n    self.observers.append(observer)\n</code></pre>"},{"location":"api/observability/dispatcher/#ondine.observability.dispatcher.ObserverDispatcher.remove_observer","title":"remove_observer","text":"<pre><code>remove_observer(observer: PipelineObserver) -&gt; None\n</code></pre> <p>Remove an observer dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>PipelineObserver</code> <p>Observer instance to remove</p> required Source code in <code>ondine/observability/dispatcher.py</code> <pre><code>def remove_observer(self, observer: PipelineObserver) -&gt; None:\n    \"\"\"\n    Remove an observer dynamically.\n\n    Args:\n        observer: Observer instance to remove\n    \"\"\"\n    if observer in self.observers:\n        self.observers.remove(observer)\n</code></pre>"},{"location":"api/observability/events/","title":"events","text":""},{"location":"api/observability/events/#ondine.observability.events","title":"events","text":"<p>Event models for pipeline observability.</p> <p>These event dataclasses are emitted at key points during pipeline execution and dispatched to all registered observers.</p>"},{"location":"api/observability/events/#ondine.observability.events.PipelineStartEvent","title":"PipelineStartEvent  <code>dataclass</code>","text":"<pre><code>PipelineStartEvent(pipeline_id: UUID, run_id: UUID, timestamp: datetime, trace_id: str, span_id: str, config: dict[str, Any] = dict(), metadata: dict[str, Any] = dict(), total_rows: int = 0)\n</code></pre> <p>Emitted when pipeline execution starts.</p> <p>Contains pipeline configuration and metadata for the entire run.</p>"},{"location":"api/observability/events/#ondine.observability.events.StageStartEvent","title":"StageStartEvent  <code>dataclass</code>","text":"<pre><code>StageStartEvent(pipeline_id: UUID, run_id: UUID, stage_name: str, stage_type: str, timestamp: datetime, trace_id: str, span_id: str, parent_span_id: str | None = None, metadata: dict[str, Any] = dict())\n</code></pre> <p>Emitted when a pipeline stage begins execution.</p> <p>Tracks which stage is starting and when.</p>"},{"location":"api/observability/events/#ondine.observability.events.LLMCallEvent","title":"LLMCallEvent  <code>dataclass</code>","text":"<pre><code>LLMCallEvent(pipeline_id: UUID, run_id: UUID, stage_name: str, row_index: int, timestamp: datetime, trace_id: str, span_id: str, prompt: str, model: str, provider: str, temperature: float, completion: str, parent_span_id: str | None = None, max_tokens: int | None = None, system_message: str | None = None, finish_reason: str = 'stop', input_tokens: int = 0, output_tokens: int = 0, total_tokens: int = 0, cost: Decimal = (lambda: Decimal('0.0'))(), latency_ms: float = 0.0, rag_context: str | None = None, rag_sources: list[dict] | None = None, rag_technique: str | None = None, retrieval_latency_ms: float | None = None, prompt_template_id: str | None = None, prompt_version: str | None = None, metadata: dict[str, Any] = dict())\n</code></pre> <p>Emitted on every LLM invocation.</p> <p>This is the MOST IMPORTANT event for LLM observability. Contains full prompt/completion text, tokens, cost, and optional RAG metadata.</p> <p>Observers can choose to truncate or sanitize prompts based on their needs.</p>"},{"location":"api/observability/events/#ondine.observability.events.StageEndEvent","title":"StageEndEvent  <code>dataclass</code>","text":"<pre><code>StageEndEvent(pipeline_id: UUID, run_id: UUID, stage_name: str, success: bool, timestamp: datetime, trace_id: str, span_id: str, duration_ms: float = 0.0, rows_processed: int = 0, error: Exception | None = None, metrics: dict[str, Any] = dict())\n</code></pre> <p>Emitted when a pipeline stage completes successfully or with errors.</p> <p>Contains stage-level metrics and success status.</p>"},{"location":"api/observability/events/#ondine.observability.events.ErrorEvent","title":"ErrorEvent  <code>dataclass</code>","text":"<pre><code>ErrorEvent(pipeline_id: UUID, run_id: UUID, timestamp: datetime, trace_id: str, span_id: str, stage_name: str | None = None, row_index: int | None = None, error: Exception = None, error_type: str = '', error_message: str = '', stack_trace: str = '', context: dict[str, Any] = dict())\n</code></pre> <p>Emitted when errors occur during pipeline execution.</p> <p>Captures error context for debugging and alerting.</p>"},{"location":"api/observability/events/#ondine.observability.events.PipelineEndEvent","title":"PipelineEndEvent  <code>dataclass</code>","text":"<pre><code>PipelineEndEvent(pipeline_id: UUID, run_id: UUID, success: bool, timestamp: datetime, trace_id: str, span_id: str, total_duration_ms: float = 0.0, rows_processed: int = 0, rows_succeeded: int = 0, rows_failed: int = 0, rows_skipped: int = 0, total_cost: Decimal = (lambda: Decimal('0.0'))(), total_tokens: int = 0, input_tokens: int = 0, output_tokens: int = 0, metrics: dict[str, Any] = dict())\n</code></pre> <p>Emitted when pipeline execution completes.</p> <p>Contains final metrics for the entire pipeline run.</p>"},{"location":"api/observability/llamaindex_handlers/","title":"llamaindex_handlers","text":""},{"location":"api/observability/llamaindex_handlers/#ondine.observability.llamaindex_handlers","title":"llamaindex_handlers","text":"<p>LlamaIndex handler management for Ondine observability.</p> <p>This module configures LlamaIndex's built-in observability handlers via Ondine's observer API.</p>"},{"location":"api/observability/llamaindex_handlers/#ondine.observability.llamaindex_handlers.LlamaIndexHandlerManager","title":"LlamaIndexHandlerManager","text":"<p>Manages LlamaIndex global handlers.</p> <p>LlamaIndex provides built-in handlers for various observability platforms. This manager configures them based on Ondine's observer configuration.</p>"},{"location":"api/observability/llamaindex_handlers/#ondine.observability.llamaindex_handlers.LlamaIndexHandlerManager.configure_handler","title":"configure_handler  <code>classmethod</code>","text":"<pre><code>configure_handler(handler_type: str, config: dict[str, Any]) -&gt; None\n</code></pre> <p>Configure a LlamaIndex global handler.</p> <p>Note: LlamaIndex only supports ONE global handler at a time. Calling this multiple times will replace the previous handler. For multiple observers, use configure_multi_handler() instead.</p> <p>Parameters:</p> Name Type Description Default <code>handler_type</code> <code>str</code> <p>Handler type (\"opentelemetry\", \"langfuse\", \"simple\", etc.)</p> required <code>config</code> <code>dict[str, Any]</code> <p>Handler-specific configuration</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If LlamaIndex or handler dependencies not installed</p> Source code in <code>ondine/observability/llamaindex_handlers.py</code> <pre><code>@classmethod\ndef configure_handler(cls, handler_type: str, config: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Configure a LlamaIndex global handler.\n\n    Note: LlamaIndex only supports ONE global handler at a time.\n    Calling this multiple times will replace the previous handler.\n    For multiple observers, use configure_multi_handler() instead.\n\n    Args:\n        handler_type: Handler type (\"opentelemetry\", \"langfuse\", \"simple\", etc.)\n        config: Handler-specific configuration\n\n    Raises:\n        ImportError: If LlamaIndex or handler dependencies not installed\n    \"\"\"\n    from llama_index.core import set_global_handler\n\n    try:\n        if handler_type == \"opentelemetry\":\n            # OpenTelemetry handler for infrastructure monitoring\n            set_global_handler(\"opentelemetry\", **config)\n            logger.info(\"Configured LlamaIndex OpenTelemetry handler\")\n\n        elif handler_type == \"langfuse\":\n            # Langfuse handler for LLM-specific observability\n            public_key = config.get(\"public_key\")\n            secret_key = config.get(\"secret_key\")\n            host = config.get(\"host\", \"https://cloud.langfuse.com\")\n\n            if not public_key or not secret_key:\n                raise ValueError(\n                    \"Langfuse requires 'public_key' and 'secret_key' in config\"\n                )\n\n            set_global_handler(\n                \"langfuse\",\n                public_key=public_key,\n                secret_key=secret_key,\n                host=host,\n            )\n            logger.info(f\"Configured LlamaIndex Langfuse handler (host={host})\")\n\n        elif handler_type == \"simple\" or handler_type == \"logging\":\n            # Simple handler for console logging\n            set_global_handler(\"simple\")\n            logger.info(\"Configured LlamaIndex Simple handler (console logging)\")\n\n        elif handler_type == \"arize_phoenix\":\n            # Arize Phoenix handler\n            set_global_handler(\"arize_phoenix\", **config)\n            logger.info(\"Configured LlamaIndex Arize Phoenix handler\")\n\n        elif handler_type == \"wandb\":\n            # Weights &amp; Biases handler\n            set_global_handler(\"wandb\", **config)\n            logger.info(\"Configured LlamaIndex W&amp;B handler\")\n\n        else:\n            logger.warning(\n                f\"Unknown LlamaIndex handler type: {handler_type}. \"\n                f\"Supported: opentelemetry, langfuse, simple, arize_phoenix, wandb\"\n            )\n            return\n\n        cls._active_handler = handler_type\n\n    except ImportError as e:\n        logger.error(\n            f\"Failed to configure {handler_type} handler: {e}. \"\n            f\"Install required dependencies or use a different observer.\"\n        )\n        raise\n</code></pre>"},{"location":"api/observability/llamaindex_handlers/#ondine.observability.llamaindex_handlers.LlamaIndexHandlerManager.configure_multi_handler","title":"configure_multi_handler  <code>classmethod</code>","text":"<pre><code>configure_multi_handler(handlers: list[tuple[str, dict[str, Any]]]) -&gt; None\n</code></pre> <p>Configure multiple LlamaIndex handlers simultaneously.</p> <p>Uses LlamaIndex's lower-level dispatcher API to register multiple event handlers at once, since set_global_handler() only supports one.</p> <p>Parameters:</p> Name Type Description Default <code>handlers</code> <code>list[tuple[str, dict[str, Any]]]</code> <p>List of (handler_type, config) tuples</p> required <p>Note: This is more advanced and may require custom event handler implementations for some platforms.</p> Source code in <code>ondine/observability/llamaindex_handlers.py</code> <pre><code>@classmethod\ndef configure_multi_handler(\n    cls, handlers: list[tuple[str, dict[str, Any]]]\n) -&gt; None:\n    \"\"\"\n    Configure multiple LlamaIndex handlers simultaneously.\n\n    Uses LlamaIndex's lower-level dispatcher API to register multiple\n    event handlers at once, since set_global_handler() only supports one.\n\n    Args:\n        handlers: List of (handler_type, config) tuples\n\n    Note: This is more advanced and may require custom event handler\n    implementations for some platforms.\n    \"\"\"\n    from llama_index.core.instrumentation import get_dispatcher\n\n    get_dispatcher()\n\n    logger.info(f\"Configuring {len(handlers)} LlamaIndex handlers via dispatcher\")\n\n    # For now, just use the last handler as global\n    # TODO: Implement proper multi-handler support using dispatcher.add_event_handler()\n    if handlers:\n        last_handler_type, last_config = handlers[-1]\n        cls.configure_handler(last_handler_type, last_config)\n\n        if len(handlers) &gt; 1:\n            logger.warning(\n                f\"Multiple LlamaIndex handlers requested but only '{last_handler_type}' \"\n                f\"is active. Full multi-handler support coming soon.\"\n            )\n</code></pre>"},{"location":"api/observability/llamaindex_handlers/#ondine.observability.llamaindex_handlers.LlamaIndexHandlerManager.get_active_handler","title":"get_active_handler  <code>classmethod</code>","text":"<pre><code>get_active_handler() -&gt; str | None\n</code></pre> <p>Get the currently active handler type.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>Handler type string or None if no handler active</p> Source code in <code>ondine/observability/llamaindex_handlers.py</code> <pre><code>@classmethod\ndef get_active_handler(cls) -&gt; str | None:\n    \"\"\"\n    Get the currently active handler type.\n\n    Returns:\n        Handler type string or None if no handler active\n    \"\"\"\n    return cls._active_handler\n</code></pre>"},{"location":"api/observability/llamaindex_handlers/#ondine.observability.llamaindex_handlers.LlamaIndexHandlerManager.reset_handler","title":"reset_handler  <code>classmethod</code>","text":"<pre><code>reset_handler() -&gt; None\n</code></pre> <p>Reset/disable the global handler.</p> Source code in <code>ondine/observability/llamaindex_handlers.py</code> <pre><code>@classmethod\ndef reset_handler(cls) -&gt; None:\n    \"\"\"Reset/disable the global handler.\"\"\"\n    # LlamaIndex doesn't have an official API to disable handlers\n    # but setting it to None or creating a no-op handler works\n    cls._active_handler = None\n    logger.info(\"LlamaIndex handler reset\")\n</code></pre>"},{"location":"api/observability/observer/","title":"observer","text":""},{"location":"api/observability/observer/#ondine.observability.observer","title":"observer","text":"<p>Tracing observer for pipeline execution.</p> <p>Implements ExecutionObserver interface to create OpenTelemetry spans for pipeline and stage execution.</p>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver","title":"TracingObserver","text":"<pre><code>TracingObserver(include_prompts: bool = False)\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that creates OpenTelemetry spans for pipeline execution.</p> <p>Single Responsibility: Create and manage trace spans for observability.</p> <p>The observer creates a hierarchical span structure: - Root span for pipeline execution - Nested spans for each stage - Attributes include metrics, errors, and metadata</p> <p>Parameters:</p> Name Type Description Default <code>include_prompts</code> <code>bool</code> <p>If True, include prompts in spans (PII risk)</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ondine.observability import TracingObserver, enable_tracing\n&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n&gt;&gt;&gt; observer = TracingObserver(include_prompts=False)\n&gt;&gt;&gt; # Attach to pipeline execution\n</code></pre> <p>Initialize tracing observer.</p> Source code in <code>ondine/observability/observer.py</code> <pre><code>def __init__(self, include_prompts: bool = False):\n    \"\"\"Initialize tracing observer.\"\"\"\n    self._include_prompts = include_prompts\n    self._spans: dict[str, trace.Span] = {}  # Track active spans\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Create root span for pipeline execution.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Any</code> <p>Pipeline instance</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context with total rows, etc.</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"\n    Create root span for pipeline execution.\n\n    Args:\n        pipeline: Pipeline instance\n        context: Execution context with total rows, etc.\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    tracer = get_tracer()\n\n    # Create root span\n    span = tracer.start_span(\"pipeline.execute\")\n\n    # Add attributes\n    span.set_attribute(\"ondine.total_rows\", context.total_rows)\n    span.set_attribute(\"ondine.session_id\", context.session_id)\n\n    # Store span for later\n    self._spans[\"pipeline\"] = span\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Create span for stage execution.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>PipelineStage</code> <p>Pipeline stage being executed</p> required <code>context</code> <code>ExecutionContext</code> <p>Current execution context</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"\n    Create span for stage execution.\n\n    Args:\n        stage: Pipeline stage being executed\n        context: Current execution context\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    tracer = get_tracer()\n    stage_name = stage.__class__.__name__\n\n    # Create nested span under pipeline span\n    # Note: OpenTelemetry automatically handles span context\n    span = tracer.start_span(f\"stage.{stage_name}\")\n\n    # Add stage-specific attributes\n    span.set_attribute(\"ondine.stage\", stage_name)\n    span.set_attribute(\"ondine.processed_rows\", context.last_processed_row + 1)\n\n    # Add prompt if stage has it (and sanitize based on flag)\n    if hasattr(stage, \"prompt_template\") and stage.prompt_template:\n        from .sanitizer import sanitize_prompt\n\n        prompt_value = sanitize_prompt(\n            stage.prompt_template, include_prompts=self._include_prompts\n        )\n        span.set_attribute(\"ondine.prompt\", prompt_value)\n\n    # Store span for completion/error handling\n    self._spans[stage_name] = span\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Close stage span with success attributes.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>PipelineStage</code> <p>Pipeline stage that completed</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context after stage</p> required <code>result</code> <code>Any</code> <p>Stage execution result</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"\n    Close stage span with success attributes.\n\n    Args:\n        stage: Pipeline stage that completed\n        context: Execution context after stage\n        result: Stage execution result\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    stage_name = stage.__class__.__name__\n    span = self._spans.get(stage_name)\n\n    if span is not None:\n        # Mark as successful\n        span.set_status(Status(StatusCode.OK))\n\n        # Add completion metrics\n        span.set_attribute(\"ondine.rows_processed\", context.last_processed_row + 1)\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(stage_name, None)\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close stage span with error details.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>PipelineStage</code> <p>Pipeline stage that failed</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context at failure</p> required <code>error</code> <code>Exception</code> <p>Exception that occurred</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"\n    Close stage span with error details.\n\n    Args:\n        stage: Pipeline stage that failed\n        context: Execution context at failure\n        error: Exception that occurred\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    stage_name = stage.__class__.__name__\n    span = self._spans.get(stage_name)\n\n    if span is not None:\n        # Record exception\n        span.record_exception(error)\n\n        # Mark as error\n        span.set_status(Status(StatusCode.ERROR, str(error)))\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(stage_name, None)\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Close root span with final metrics.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Final execution context</p> required <code>result</code> <code>ExecutionResult</code> <p>Pipeline execution result</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"\n    Close root span with final metrics.\n\n    Args:\n        context: Final execution context\n        result: Pipeline execution result\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    span = self._spans.get(\"pipeline\")\n\n    if span is not None:\n        # Add final metrics\n        span.set_attribute(\"ondine.processed_rows\", result.metrics.processed_rows)\n        span.set_attribute(\"ondine.failed_rows\", result.metrics.failed_rows)\n        span.set_attribute(\n            \"ondine.duration_seconds\", result.metrics.total_duration_seconds\n        )\n        span.set_attribute(\"ondine.total_cost\", float(result.costs.total_cost))\n\n        # Mark as successful\n        span.set_status(Status(StatusCode.OK))\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(\"pipeline\", None)\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close root span with error.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Execution context at failure</p> required <code>error</code> <code>Exception</code> <p>Exception that occurred</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"\n    Close root span with error.\n\n    Args:\n        context: Execution context at failure\n        error: Exception that occurred\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    span = self._spans.get(\"pipeline\")\n\n    if span is not None:\n        # Record exception\n        span.record_exception(error)\n\n        # Mark as error\n        span.set_status(Status(StatusCode.ERROR, str(error)))\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(\"pipeline\", None)\n</code></pre>"},{"location":"api/observability/registry/","title":"registry","text":""},{"location":"api/observability/registry/#ondine.observability.registry","title":"registry","text":"<p>Observer registry for plugin-based observability.</p> <p>Allows dynamic registration and discovery of observer implementations.</p>"},{"location":"api/observability/registry/#ondine.observability.registry.ObserverRegistry","title":"ObserverRegistry","text":"<p>Global registry of observer implementations.</p> <p>Allows dynamic observer registration and instantiation via decorator pattern.</p> Example <p>@observer(\"my_observer\") class MyObserver(PipelineObserver):     def on_llm_call(self, event):         print(f\"LLM called: {event.model}\")</p>"},{"location":"api/observability/registry/#ondine.observability.registry.ObserverRegistry--later-instantiate-by-name","title":"Later, instantiate by name","text":"<p>observer_class = ObserverRegistry.get(\"my_observer\") observer = observer_class(config={...})</p>"},{"location":"api/observability/registry/#ondine.observability.registry.ObserverRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(name: str, observer_class: type[PipelineObserver]) -&gt; None\n</code></pre> <p>Register an observer implementation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier (e.g., \"langfuse\", \"opentelemetry\")</p> required <code>observer_class</code> <code>type[PipelineObserver]</code> <p>Observer class implementing PipelineObserver</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If observer already registered or invalid class</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef register(cls, name: str, observer_class: type[PipelineObserver]) -&gt; None:\n    \"\"\"\n    Register an observer implementation.\n\n    Args:\n        name: Observer identifier (e.g., \"langfuse\", \"opentelemetry\")\n        observer_class: Observer class implementing PipelineObserver\n\n    Raises:\n        ValueError: If observer already registered or invalid class\n    \"\"\"\n    if name in cls._observers:\n        raise ValueError(\n            f\"Observer '{name}' already registered. \"\n            f\"Use a different name or unregister first.\"\n        )\n\n    if not issubclass(observer_class, PipelineObserver):\n        raise ValueError(\n            f\"Observer class must inherit from PipelineObserver, \"\n            f\"got {observer_class.__name__}\"\n        )\n\n    cls._observers[name] = observer_class\n</code></pre>"},{"location":"api/observability/registry/#ondine.observability.registry.ObserverRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(name: str) -&gt; type[PipelineObserver]\n</code></pre> <p>Get observer class by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier</p> required <p>Returns:</p> Type Description <code>type[PipelineObserver]</code> <p>Observer class (not instantiated)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If observer not found</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; type[PipelineObserver]:\n    \"\"\"\n    Get observer class by name.\n\n    Args:\n        name: Observer identifier\n\n    Returns:\n        Observer class (not instantiated)\n\n    Raises:\n        ValueError: If observer not found\n    \"\"\"\n    if name not in cls._observers:\n        available = \", \".join(cls._observers.keys()) if cls._observers else \"none\"\n        raise ValueError(\n            f\"Observer '{name}' not found. Available observers: {available}\"\n        )\n\n    return cls._observers[name]\n</code></pre>"},{"location":"api/observability/registry/#ondine.observability.registry.ObserverRegistry.list_observers","title":"list_observers  <code>classmethod</code>","text":"<pre><code>list_observers() -&gt; list[str]\n</code></pre> <p>List all registered observer names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of observer identifiers</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef list_observers(cls) -&gt; list[str]:\n    \"\"\"\n    List all registered observer names.\n\n    Returns:\n        List of observer identifiers\n    \"\"\"\n    return list(cls._observers.keys())\n</code></pre>"},{"location":"api/observability/registry/#ondine.observability.registry.ObserverRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(name: str) -&gt; bool\n</code></pre> <p>Check if observer is registered.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, name: str) -&gt; bool:\n    \"\"\"\n    Check if observer is registered.\n\n    Args:\n        name: Observer identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    return name in cls._observers\n</code></pre>"},{"location":"api/observability/registry/#ondine.observability.registry.ObserverRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(name: str) -&gt; None\n</code></pre> <p>Unregister an observer (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier</p> required Source code in <code>ondine/observability/registry.py</code> <pre><code>@classmethod\ndef unregister(cls, name: str) -&gt; None:\n    \"\"\"\n    Unregister an observer (mainly for testing).\n\n    Args:\n        name: Observer identifier\n    \"\"\"\n    cls._observers.pop(name, None)\n</code></pre>"},{"location":"api/observability/registry/#ondine.observability.registry.observer","title":"observer","text":"<pre><code>observer(name: str) -&gt; Callable\n</code></pre> <p>Decorator to register an observer.</p> <p>Automatically registers the decorated class with the ObserverRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Observer identifier (e.g., \"langfuse\", \"opentelemetry\")</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Decorator function</p> Example <p>@observer(\"custom_logger\") class CustomLogger(PipelineObserver):     def on_llm_call(self, event):         logging.info(f\"LLM: {event.model}\")</p> Source code in <code>ondine/observability/registry.py</code> <pre><code>def observer(name: str) -&gt; Callable:\n    \"\"\"\n    Decorator to register an observer.\n\n    Automatically registers the decorated class with the ObserverRegistry.\n\n    Args:\n        name: Observer identifier (e.g., \"langfuse\", \"opentelemetry\")\n\n    Returns:\n        Decorator function\n\n    Example:\n        @observer(\"custom_logger\")\n        class CustomLogger(PipelineObserver):\n            def on_llm_call(self, event):\n                logging.info(f\"LLM: {event.model}\")\n    \"\"\"\n\n    def decorator(observer_class: type[PipelineObserver]) -&gt; type[PipelineObserver]:\n        ObserverRegistry.register(name, observer_class)\n        return observer_class\n\n    return decorator\n</code></pre>"},{"location":"api/observability/sanitizer/","title":"sanitizer","text":""},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer","title":"sanitizer","text":"<p>PII sanitization for trace attributes.</p> <p>Provides utilities to sanitize sensitive data in prompts and responses before including them in distributed traces.</p>"},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer.sanitize_text","title":"sanitize_text","text":"<pre><code>sanitize_text(text: str, patterns: dict[str, Pattern] | None = None, replacement: str = '[REDACTED]') -&gt; str\n</code></pre> <p>Sanitize text by replacing PII patterns.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to sanitize</p> required <code>patterns</code> <code>dict[str, Pattern] | None</code> <p>Dictionary of pattern name -&gt; regex pattern (uses defaults if None)</p> <code>None</code> <code>replacement</code> <code>str</code> <p>Replacement string for matched patterns</p> <code>'[REDACTED]'</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized text with PII replaced</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sanitize_text(\"Email me at test@example.com\")\n'Email me at [REDACTED]'\n&gt;&gt;&gt; sanitize_text(\"SSN: 123-45-6789\")\n'SSN: [REDACTED]'\n</code></pre> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_text(\n    text: str,\n    patterns: dict[str, re.Pattern] | None = None,\n    replacement: str = \"[REDACTED]\",\n) -&gt; str:\n    \"\"\"\n    Sanitize text by replacing PII patterns.\n\n    Args:\n        text: Text to sanitize\n        patterns: Dictionary of pattern name -&gt; regex pattern (uses defaults if None)\n        replacement: Replacement string for matched patterns\n\n    Returns:\n        Sanitized text with PII replaced\n\n    Examples:\n        &gt;&gt;&gt; sanitize_text(\"Email me at test@example.com\")\n        'Email me at [REDACTED]'\n        &gt;&gt;&gt; sanitize_text(\"SSN: 123-45-6789\")\n        'SSN: [REDACTED]'\n    \"\"\"\n    if patterns is None:\n        patterns = PII_PATTERNS\n\n    sanitized = text\n    for pattern_name, pattern in patterns.items():\n        sanitized = pattern.sub(replacement, sanitized)\n\n    return sanitized\n</code></pre>"},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer.sanitize_prompt","title":"sanitize_prompt","text":"<pre><code>sanitize_prompt(prompt: str, include_prompts: bool = False) -&gt; str\n</code></pre> <p>Sanitize prompt text for tracing (PII-safe by default).</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The original prompt text</p> required <code>include_prompts</code> <code>bool</code> <p>If True, return original prompt (opt-in)</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized prompt (stable hash) or original if opted in</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sanitize_prompt(\"User email: test@example.com\")\n'&lt;sanitized-a1b2c3d4&gt;'\n&gt;&gt;&gt; sanitize_prompt(\"Test prompt\", include_prompts=True)\n'Test prompt'\n</code></pre> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_prompt(prompt: str, include_prompts: bool = False) -&gt; str:\n    \"\"\"\n    Sanitize prompt text for tracing (PII-safe by default).\n\n    Args:\n        prompt: The original prompt text\n        include_prompts: If True, return original prompt (opt-in)\n\n    Returns:\n        Sanitized prompt (stable hash) or original if opted in\n\n    Examples:\n        &gt;&gt;&gt; sanitize_prompt(\"User email: test@example.com\")\n        '&lt;sanitized-a1b2c3d4&gt;'\n        &gt;&gt;&gt; sanitize_prompt(\"Test prompt\", include_prompts=True)\n        'Test prompt'\n    \"\"\"\n    if include_prompts:\n        return prompt\n\n    # Convert to string (defensive: handles non-string types like Mock objects in tests)\n    prompt_str = str(prompt)\n\n    # Stable short digest for deduplication without content disclosure\n    # Use SHA-256 for deterministic hashing across runs (unlike built-in hash())\n    digest = hashlib.sha256(prompt_str.encode(\"utf-8\")).hexdigest()[:8]\n    return f\"&lt;sanitized-{digest}&gt;\"\n</code></pre>"},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer.sanitize_response","title":"sanitize_response","text":"<pre><code>sanitize_response(response: str, include_prompts: bool = False) -&gt; str\n</code></pre> <p>Sanitize LLM response text for tracing (PII-safe by default).</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The original response text</p> required <code>include_prompts</code> <code>bool</code> <p>If True, return original response (opt-in)</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized response (hash) or original if opted in</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sanitize_response(\"SSN: 123-45-6789\")\n'&lt;sanitized-5678&gt;'\n&gt;&gt;&gt; sanitize_response(\"Safe response\", include_prompts=True)\n'Safe response'\n</code></pre> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_response(response: str, include_prompts: bool = False) -&gt; str:\n    \"\"\"\n    Sanitize LLM response text for tracing (PII-safe by default).\n\n    Args:\n        response: The original response text\n        include_prompts: If True, return original response (opt-in)\n\n    Returns:\n        Sanitized response (hash) or original if opted in\n\n    Examples:\n        &gt;&gt;&gt; sanitize_response(\"SSN: 123-45-6789\")\n        '&lt;sanitized-5678&gt;'\n        &gt;&gt;&gt; sanitize_response(\"Safe response\", include_prompts=True)\n        'Safe response'\n    \"\"\"\n    # Reuse same logic as prompt sanitization (DRY principle)\n    return sanitize_prompt(response, include_prompts=include_prompts)\n</code></pre>"},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer.sanitize_event","title":"sanitize_event","text":"<pre><code>sanitize_event(event: LLMCallEvent, config: dict[str, Any] | None = None) -&gt; LLMCallEvent\n</code></pre> <p>Sanitize an LLM call event based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LLMCallEvent</code> <p>LLM call event to sanitize</p> required <code>config</code> <code>dict[str, Any] | None</code> <p>Sanitization configuration: - sanitize_prompts: bool (default: True) - sanitize_completions: bool (default: True) - custom_patterns: dict[str, str] (additional regex patterns) - replacement: str (default: \"[REDACTED]\")</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMCallEvent</code> <p>New LLMCallEvent with sanitized fields</p> Example <p>config = {     \"sanitize_prompts\": True,     \"sanitize_completions\": False,     \"custom_patterns\": {\"account_id\": r\"ACC-\\d{6}\"} } sanitized = sanitize_event(event, config)</p> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_event(\n    event: LLMCallEvent,\n    config: dict[str, Any] | None = None,\n) -&gt; LLMCallEvent:\n    r\"\"\"\n    Sanitize an LLM call event based on configuration.\n\n    Args:\n        event: LLM call event to sanitize\n        config: Sanitization configuration:\n            - sanitize_prompts: bool (default: True)\n            - sanitize_completions: bool (default: True)\n            - custom_patterns: dict[str, str] (additional regex patterns)\n            - replacement: str (default: \"[REDACTED]\")\n\n    Returns:\n        New LLMCallEvent with sanitized fields\n\n    Example:\n        config = {\n            \"sanitize_prompts\": True,\n            \"sanitize_completions\": False,\n            \"custom_patterns\": {\"account_id\": r\"ACC-\\d{6}\"}\n        }\n        sanitized = sanitize_event(event, config)\n    \"\"\"\n    if config is None:\n        config = {}\n\n    sanitize_prompts = config.get(\"sanitize_prompts\", True)\n    sanitize_completions = config.get(\"sanitize_completions\", True)\n    custom_patterns_dict = config.get(\"custom_patterns\", {})\n    replacement = config.get(\"replacement\", \"[REDACTED]\")\n\n    # Build combined patterns\n    patterns = PII_PATTERNS.copy()\n    for name, pattern_str in custom_patterns_dict.items():\n        patterns[name] = re.compile(pattern_str)\n\n    # Create new event with sanitized fields\n    sanitized_prompt = event.prompt\n    sanitized_completion = event.completion\n    sanitized_rag_context = event.rag_context\n\n    if sanitize_prompts:\n        sanitized_prompt = sanitize_text(event.prompt, patterns, replacement)\n        if event.rag_context:\n            sanitized_rag_context = sanitize_text(\n                event.rag_context, patterns, replacement\n            )\n\n    if sanitize_completions:\n        sanitized_completion = sanitize_text(event.completion, patterns, replacement)\n\n    # Return new event (events are immutable dataclasses)\n    # We use replace() to create a new instance\n    from dataclasses import replace\n\n    return replace(\n        event,\n        prompt=sanitized_prompt,\n        completion=sanitized_completion,\n        rag_context=sanitized_rag_context,\n    )\n</code></pre>"},{"location":"api/observability/tracer/","title":"tracer","text":""},{"location":"api/observability/tracer/#ondine.observability.tracer","title":"tracer","text":"<p>OpenTelemetry tracer setup and management.</p> <p>Provides simple API for enabling/disabling distributed tracing with console or Jaeger exporters.</p>"},{"location":"api/observability/tracer/#ondine.observability.tracer.get_tracer","title":"get_tracer","text":"<pre><code>get_tracer() -&gt; trace.Tracer\n</code></pre> <p>Get current tracer instance (or no-op if disabled).</p> <p>Returns:</p> Type Description <code>Tracer</code> <p>Active tracer or no-op tracer if tracing is disabled</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tracer = get_tracer()\n&gt;&gt;&gt; with tracer.start_as_current_span(\"my-operation\"):\n...     # Do work\n...     pass\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def get_tracer() -&gt; trace.Tracer:\n    \"\"\"\n    Get current tracer instance (or no-op if disabled).\n\n    Returns:\n        Active tracer or no-op tracer if tracing is disabled\n\n    Examples:\n        &gt;&gt;&gt; tracer = get_tracer()\n        &gt;&gt;&gt; with tracer.start_as_current_span(\"my-operation\"):\n        ...     # Do work\n        ...     pass\n    \"\"\"\n    global _TRACER\n\n    if not _TRACING_ENABLED or _TRACER is None:\n        # Return no-op tracer that does nothing\n        return trace.get_tracer(__name__)\n\n    return _TRACER\n</code></pre>"},{"location":"api/observability/tracer/#ondine.observability.tracer.enable_tracing","title":"enable_tracing","text":"<pre><code>enable_tracing(exporter: str = 'console', endpoint: str | None = None, service_name: str = 'ondine-pipeline') -&gt; None\n</code></pre> <p>Enable distributed tracing (opt-in).</p> <p>Parameters:</p> Name Type Description Default <code>exporter</code> <code>str</code> <p>Exporter type (\"console\" or \"jaeger\")</p> <code>'console'</code> <code>endpoint</code> <code>str | None</code> <p>Jaeger endpoint (required if exporter=\"jaeger\")</p> <code>None</code> <code>service_name</code> <code>str</code> <p>Service name for traces</p> <code>'ondine-pipeline'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Console exporter (for development)\n&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Jaeger exporter (for production)\n&gt;&gt;&gt; enable_tracing(\n...     exporter=\"jaeger\",\n...     endpoint=\"http://localhost:14268/api/traces\"\n... )\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def enable_tracing(\n    exporter: str = \"console\",\n    endpoint: str | None = None,\n    service_name: str = \"ondine-pipeline\",\n) -&gt; None:\n    \"\"\"\n    Enable distributed tracing (opt-in).\n\n    Args:\n        exporter: Exporter type (\"console\" or \"jaeger\")\n        endpoint: Jaeger endpoint (required if exporter=\"jaeger\")\n        service_name: Service name for traces\n\n    Examples:\n        &gt;&gt;&gt; # Console exporter (for development)\n        &gt;&gt;&gt; enable_tracing(exporter=\"console\")\n\n        &gt;&gt;&gt; # Jaeger exporter (for production)\n        &gt;&gt;&gt; enable_tracing(\n        ...     exporter=\"jaeger\",\n        ...     endpoint=\"http://localhost:14268/api/traces\"\n        ... )\n    \"\"\"\n    global _TRACING_ENABLED, _TRACER_PROVIDER, _TRACER\n\n    # Make idempotent - clean up existing tracing if already enabled\n    if _TRACING_ENABLED:\n        disable_tracing()\n\n    # Create resource with service name\n    resource = Resource.create(attributes={\"service.name\": service_name})\n\n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n\n    # Configure exporter\n    if exporter == \"console\":\n        span_exporter = ConsoleSpanExporter()\n    elif exporter == \"jaeger\":\n        if endpoint is None:\n            raise ValueError(\"endpoint is required for Jaeger exporter\")\n        span_exporter = JaegerExporter(\n            collector_endpoint=endpoint,\n        )\n    else:\n        raise ValueError(f\"Unknown exporter: {exporter}. Use 'console' or 'jaeger'\")\n\n    # Add span processor (gracefully handle export failures)\n    try:\n        processor = BatchSpanProcessor(span_exporter)\n        provider.add_span_processor(processor)\n    except Exception as e:\n        # Don't break pipeline if exporter setup fails\n        import logging\n\n        logger = logging.getLogger(__name__)\n        logger.warning(f\"Failed to setup span processor: {e}. Tracing will be disabled\")\n        return\n\n    # Set global provider\n    trace.set_tracer_provider(provider)\n\n    # Store module state\n    _TRACER_PROVIDER = provider\n    _TRACER = trace.get_tracer(__name__)\n    _TRACING_ENABLED = True\n</code></pre>"},{"location":"api/observability/tracer/#ondine.observability.tracer.disable_tracing","title":"disable_tracing","text":"<pre><code>disable_tracing() -&gt; None\n</code></pre> <p>Disable tracing and cleanup resources.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n&gt;&gt;&gt; # ... do work ...\n&gt;&gt;&gt; disable_tracing()\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def disable_tracing() -&gt; None:\n    \"\"\"\n    Disable tracing and cleanup resources.\n\n    Examples:\n        &gt;&gt;&gt; enable_tracing(exporter=\"console\")\n        &gt;&gt;&gt; # ... do work ...\n        &gt;&gt;&gt; disable_tracing()\n    \"\"\"\n    global _TRACING_ENABLED, _TRACER_PROVIDER, _TRACER\n\n    if _TRACER_PROVIDER is not None:\n        # Flush any pending spans\n        try:\n            _TRACER_PROVIDER.shutdown()\n        except Exception:  # nosec B110\n            # Silently ignore shutdown errors - tracing cleanup is non-critical\n            # Prevents exceptions during cleanup from breaking application shutdown\n            pass\n\n    _TRACING_ENABLED = False\n    _TRACER_PROVIDER = None\n    _TRACER = None\n</code></pre>"},{"location":"api/observability/tracer/#ondine.observability.tracer.is_tracing_enabled","title":"is_tracing_enabled","text":"<pre><code>is_tracing_enabled() -&gt; bool\n</code></pre> <p>Check if tracing is currently enabled.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if tracing is enabled, False otherwise</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_tracing_enabled()\nFalse\n&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n&gt;&gt;&gt; is_tracing_enabled()\nTrue\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def is_tracing_enabled() -&gt; bool:\n    \"\"\"\n    Check if tracing is currently enabled.\n\n    Returns:\n        True if tracing is enabled, False otherwise\n\n    Examples:\n        &gt;&gt;&gt; is_tracing_enabled()\n        False\n        &gt;&gt;&gt; enable_tracing(exporter=\"console\")\n        &gt;&gt;&gt; is_tracing_enabled()\n        True\n    \"\"\"\n    return _TRACING_ENABLED\n</code></pre>"},{"location":"api/observability/observers/","title":"observers","text":""},{"location":"api/observability/observers/#ondine.observability.observers","title":"observers","text":"<p>Official observer implementations for Ondine.</p> <p>This module contains built-in observers for popular observability platforms: - OpenTelemetry: Infrastructure monitoring (Jaeger, Datadog, etc.) - Langfuse: LLM-specific observability (prompts, tokens, costs) - LoggingObserver: Simple file/console logging</p>"},{"location":"api/observability/observers/#ondine.observability.observers.LoggingObserver","title":"LoggingObserver","text":"<pre><code>LoggingObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's Simple handler.</p> <p>LlamaIndex automatically logs: - \u2705 LLM calls with prompts and completions - \u2705 Token usage - \u2705 Latency metrics</p> <p>This provides basic console logging without external dependencies.</p> Configuration <ul> <li>(No specific config needed - uses LlamaIndex defaults)</li> </ul> Example <p>observer = LoggingObserver(config={})</p> <p>Initialize logging observer.</p> <p>Configures LlamaIndex's Simple handler for console logging.</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize logging observer.\n\n    Configures LlamaIndex's Simple handler for console logging.\n    \"\"\"\n    super().__init__(config)\n\n    # Configure LlamaIndex's Simple handler (console logging)\n    # This will automatically log all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"simple\", self.config)\n\n    logger.info(\"Logging observer initialized (using LlamaIndex SimpleHandler)\")\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.LoggingObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically logged by LlamaIndex.</p> <p>No action needed - LlamaIndex's Simple handler logs: - Prompt and completion - Token usage - Latency</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically logged by LlamaIndex.\n\n    No action needed - LlamaIndex's Simple handler logs:\n    - Prompt and completion\n    - Token usage\n    - Latency\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.LoggingObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush (handled by logging module).</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush (handled by logging module).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.LoggingObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by logging module).</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by logging module).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.OpenTelemetryObserver","title":"OpenTelemetryObserver","text":"<pre><code>OpenTelemetryObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's OpenTelemetry handler.</p> <p>LlamaIndex automatically instruments: - \u2705 All LLM calls (prompts, completions, tokens, latency) - \u2705 Embeddings - \u2705 Retrieval operations (when using QueryEngines)</p> <p>This observer configures the LlamaIndex handler and can add pipeline-level spans on top if needed.</p> Configuration <ul> <li>Any config accepted by LlamaIndex's OpenTelemetry handler</li> <li>See: https://docs.llamaindex.ai/en/stable/module_guides/observability/</li> </ul> Example <p>observer = OpenTelemetryObserver(config={})</p> <p>Initialize OpenTelemetry observer.</p> <p>Configures LlamaIndex's global OpenTelemetry handler.</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize OpenTelemetry observer.\n\n    Configures LlamaIndex's global OpenTelemetry handler.\n    \"\"\"\n    super().__init__(config)\n\n    # Configure LlamaIndex's OpenTelemetry handler\n    # This will automatically instrument all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"opentelemetry\", self.config)\n\n    logger.info(\"OpenTelemetry observer initialized (using LlamaIndex handler)\")\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.OpenTelemetryObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically traced by LlamaIndex.</p> <p>No action needed - LlamaIndex's OpenTelemetry handler captures: - Prompt and completion - Token usage - Latency - Model information</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically traced by LlamaIndex.\n\n    No action needed - LlamaIndex's OpenTelemetry handler captures:\n    - Prompt and completion\n    - Token usage\n    - Latency\n    - Model information\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.OpenTelemetryObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush spans (handled by OpenTelemetry SDK).</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush spans (handled by OpenTelemetry SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.OpenTelemetryObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by OpenTelemetry SDK).</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by OpenTelemetry SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.LangfuseObserver","title":"LangfuseObserver","text":"<pre><code>LangfuseObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's Langfuse handler.</p> <p>LlamaIndex automatically tracks: - \u2705 Full prompts and completions - \u2705 Token usage and costs - \u2705 Latency metrics - \u2705 Model information - \u2705 Prompt versioning</p> Configuration <ul> <li>public_key: Langfuse public key (required)</li> <li>secret_key: Langfuse secret key (required)</li> <li>host: Langfuse host URL (optional, defaults to cloud)</li> </ul> Example <p>observer = LangfuseObserver(config={     \"public_key\": \"pk-lf-...\",     \"secret_key\": \"sk-lf-...\", })</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required config (public_key, secret_key) missing</p> <p>Initialize Langfuse observer.</p> <p>Configures LlamaIndex's global Langfuse handler.</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize Langfuse observer.\n\n    Configures LlamaIndex's global Langfuse handler.\n    \"\"\"\n    super().__init__(config)\n\n    # Validate required config\n    if not self.config.get(\"public_key\") or not self.config.get(\"secret_key\"):\n        raise ValueError(\n            \"Langfuse requires 'public_key' and 'secret_key' in config. \"\n            \"Get your keys from: https://cloud.langfuse.com\"\n        )\n\n    # Configure LlamaIndex's Langfuse handler\n    # This will automatically track all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"langfuse\", self.config)\n\n    logger.info(\"Langfuse observer initialized (using LlamaIndex handler)\")\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.LangfuseObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically tracked by LlamaIndex.</p> <p>No action needed - LlamaIndex's Langfuse handler captures: - Full prompt and completion text - Token usage and costs - Model information - Latency metrics</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically tracked by LlamaIndex.\n\n    No action needed - LlamaIndex's Langfuse handler captures:\n    - Full prompt and completion text\n    - Token usage and costs\n    - Model information\n    - Latency metrics\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.LangfuseObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush events (handled by LlamaIndex/Langfuse SDK).</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush events (handled by LlamaIndex/Langfuse SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/#ondine.observability.observers.LangfuseObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by LlamaIndex/Langfuse SDK).</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by LlamaIndex/Langfuse SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/langfuse_observer/","title":"langfuse_observer","text":""},{"location":"api/observability/observers/langfuse_observer/#ondine.observability.observers.langfuse_observer","title":"langfuse_observer","text":"<p>Langfuse observer for LLM-specific observability.</p> <p>Delegates to LlamaIndex's built-in Langfuse handler for automatic tracking of prompts, completions, tokens, and costs.</p>"},{"location":"api/observability/observers/langfuse_observer/#ondine.observability.observers.langfuse_observer.LangfuseObserver","title":"LangfuseObserver","text":"<pre><code>LangfuseObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's Langfuse handler.</p> <p>LlamaIndex automatically tracks: - \u2705 Full prompts and completions - \u2705 Token usage and costs - \u2705 Latency metrics - \u2705 Model information - \u2705 Prompt versioning</p> Configuration <ul> <li>public_key: Langfuse public key (required)</li> <li>secret_key: Langfuse secret key (required)</li> <li>host: Langfuse host URL (optional, defaults to cloud)</li> </ul> Example <p>observer = LangfuseObserver(config={     \"public_key\": \"pk-lf-...\",     \"secret_key\": \"sk-lf-...\", })</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required config (public_key, secret_key) missing</p> <p>Initialize Langfuse observer.</p> <p>Configures LlamaIndex's global Langfuse handler.</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize Langfuse observer.\n\n    Configures LlamaIndex's global Langfuse handler.\n    \"\"\"\n    super().__init__(config)\n\n    # Validate required config\n    if not self.config.get(\"public_key\") or not self.config.get(\"secret_key\"):\n        raise ValueError(\n            \"Langfuse requires 'public_key' and 'secret_key' in config. \"\n            \"Get your keys from: https://cloud.langfuse.com\"\n        )\n\n    # Configure LlamaIndex's Langfuse handler\n    # This will automatically track all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"langfuse\", self.config)\n\n    logger.info(\"Langfuse observer initialized (using LlamaIndex handler)\")\n</code></pre>"},{"location":"api/observability/observers/langfuse_observer/#ondine.observability.observers.langfuse_observer.LangfuseObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically tracked by LlamaIndex.</p> <p>No action needed - LlamaIndex's Langfuse handler captures: - Full prompt and completion text - Token usage and costs - Model information - Latency metrics</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically tracked by LlamaIndex.\n\n    No action needed - LlamaIndex's Langfuse handler captures:\n    - Full prompt and completion text\n    - Token usage and costs\n    - Model information\n    - Latency metrics\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/observers/langfuse_observer/#ondine.observability.observers.langfuse_observer.LangfuseObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush events (handled by LlamaIndex/Langfuse SDK).</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush events (handled by LlamaIndex/Langfuse SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/langfuse_observer/#ondine.observability.observers.langfuse_observer.LangfuseObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by LlamaIndex/Langfuse SDK).</p> Source code in <code>ondine/observability/observers/langfuse_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by LlamaIndex/Langfuse SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/logging_observer/","title":"logging_observer","text":""},{"location":"api/observability/observers/logging_observer/#ondine.observability.observers.logging_observer","title":"logging_observer","text":"<p>Logging observer for simple console/file observability.</p> <p>Delegates to LlamaIndex's Simple handler for automatic LLM call logging.</p>"},{"location":"api/observability/observers/logging_observer/#ondine.observability.observers.logging_observer.LoggingObserver","title":"LoggingObserver","text":"<pre><code>LoggingObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's Simple handler.</p> <p>LlamaIndex automatically logs: - \u2705 LLM calls with prompts and completions - \u2705 Token usage - \u2705 Latency metrics</p> <p>This provides basic console logging without external dependencies.</p> Configuration <ul> <li>(No specific config needed - uses LlamaIndex defaults)</li> </ul> Example <p>observer = LoggingObserver(config={})</p> <p>Initialize logging observer.</p> <p>Configures LlamaIndex's Simple handler for console logging.</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize logging observer.\n\n    Configures LlamaIndex's Simple handler for console logging.\n    \"\"\"\n    super().__init__(config)\n\n    # Configure LlamaIndex's Simple handler (console logging)\n    # This will automatically log all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"simple\", self.config)\n\n    logger.info(\"Logging observer initialized (using LlamaIndex SimpleHandler)\")\n</code></pre>"},{"location":"api/observability/observers/logging_observer/#ondine.observability.observers.logging_observer.LoggingObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically logged by LlamaIndex.</p> <p>No action needed - LlamaIndex's Simple handler logs: - Prompt and completion - Token usage - Latency</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically logged by LlamaIndex.\n\n    No action needed - LlamaIndex's Simple handler logs:\n    - Prompt and completion\n    - Token usage\n    - Latency\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/observers/logging_observer/#ondine.observability.observers.logging_observer.LoggingObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush (handled by logging module).</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush (handled by logging module).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/logging_observer/#ondine.observability.observers.logging_observer.LoggingObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by logging module).</p> Source code in <code>ondine/observability/observers/logging_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by logging module).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/opentelemetry_observer/","title":"opentelemetry_observer","text":""},{"location":"api/observability/observers/opentelemetry_observer/#ondine.observability.observers.opentelemetry_observer","title":"opentelemetry_observer","text":"<p>OpenTelemetry observer for infrastructure monitoring.</p> <p>Delegates to LlamaIndex's built-in OpenTelemetry handler for LLM call tracking, while adding pipeline-level observability on top.</p>"},{"location":"api/observability/observers/opentelemetry_observer/#ondine.observability.observers.opentelemetry_observer.OpenTelemetryObserver","title":"OpenTelemetryObserver","text":"<pre><code>OpenTelemetryObserver(config: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>PipelineObserver</code></p> <p>Observer that delegates to LlamaIndex's OpenTelemetry handler.</p> <p>LlamaIndex automatically instruments: - \u2705 All LLM calls (prompts, completions, tokens, latency) - \u2705 Embeddings - \u2705 Retrieval operations (when using QueryEngines)</p> <p>This observer configures the LlamaIndex handler and can add pipeline-level spans on top if needed.</p> Configuration <ul> <li>Any config accepted by LlamaIndex's OpenTelemetry handler</li> <li>See: https://docs.llamaindex.ai/en/stable/module_guides/observability/</li> </ul> Example <p>observer = OpenTelemetryObserver(config={})</p> <p>Initialize OpenTelemetry observer.</p> <p>Configures LlamaIndex's global OpenTelemetry handler.</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def __init__(self, config: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize OpenTelemetry observer.\n\n    Configures LlamaIndex's global OpenTelemetry handler.\n    \"\"\"\n    super().__init__(config)\n\n    # Configure LlamaIndex's OpenTelemetry handler\n    # This will automatically instrument all LLM calls!\n    LlamaIndexHandlerManager.configure_handler(\"opentelemetry\", self.config)\n\n    logger.info(\"OpenTelemetry observer initialized (using LlamaIndex handler)\")\n</code></pre>"},{"location":"api/observability/observers/opentelemetry_observer/#ondine.observability.observers.opentelemetry_observer.OpenTelemetryObserver.on_llm_call","title":"on_llm_call","text":"<pre><code>on_llm_call(event: Any) -&gt; None\n</code></pre> <p>LLM calls are automatically traced by LlamaIndex.</p> <p>No action needed - LlamaIndex's OpenTelemetry handler captures: - Prompt and completion - Token usage - Latency - Model information</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def on_llm_call(self, event: Any) -&gt; None:\n    \"\"\"\n    LLM calls are automatically traced by LlamaIndex.\n\n    No action needed - LlamaIndex's OpenTelemetry handler captures:\n    - Prompt and completion\n    - Token usage\n    - Latency\n    - Model information\n    \"\"\"\n    # LlamaIndex handles this automatically!\n    pass\n</code></pre>"},{"location":"api/observability/observers/opentelemetry_observer/#ondine.observability.observers.opentelemetry_observer.OpenTelemetryObserver.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush spans (handled by OpenTelemetry SDK).</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush spans (handled by OpenTelemetry SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/observability/observers/opentelemetry_observer/#ondine.observability.observers.opentelemetry_observer.OpenTelemetryObserver.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Cleanup (handled by OpenTelemetry SDK).</p> Source code in <code>ondine/observability/observers/opentelemetry_observer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Cleanup (handled by OpenTelemetry SDK).\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/","title":"orchestration","text":""},{"location":"api/orchestration/#ondine.orchestration","title":"orchestration","text":"<p>Orchestration engine for pipeline execution control.</p>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor","title":"AsyncExecutor","text":"<pre><code>AsyncExecutor(max_concurrency: int = 10)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Asynchronous execution strategy.</p> <p>Uses asyncio for true non-blocking execution. Leverages LlamaIndex's async methods (acomplete) for concurrent LLM calls without threads.</p> <p>Benefits: - Non-blocking (works with FastAPI, aiohttp) - Better resource utilization - Higher concurrency without thread overhead - Ideal for I/O-bound operations</p> <p>Initialize async executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def __init__(self, max_concurrency: int = 10):\n    \"\"\"\n    Initialize async executor.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n    \"\"\"\n    self.max_concurrency = max_concurrency\n    self.logger = logger\n    self.semaphore = asyncio.Semaphore(max_concurrency)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages asynchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages with async/await\n        result_data = await self._execute_stages_async(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate (leverage LlamaIndex token counts from intermediate_data)\n        token_tracking = context.intermediate_data.get(\"token_tracking\", {})\n        input_tokens = token_tracking.get(\"input_tokens\", 0)\n        output_tokens = token_tracking.get(\"output_tokens\", 0)\n\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Async pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Async executor supports async.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Async executor supports async.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Async executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Async executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute_async(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"Alias for execute() method.\"\"\"\n    return await self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext","title":"ExecutionContext  <code>dataclass</code>","text":"<pre><code>ExecutionContext(session_id: UUID = uuid4(), pipeline_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, current_stage_index: int = 0, last_processed_row: int = 0, total_rows: int = 0, accumulated_cost: Decimal = (lambda: Decimal('0.0'))(), accumulated_tokens: int = 0, intermediate_data: dict[str, Any] = dict(), failed_rows: int = 0, skipped_rows: int = 0, observers: list[ExecutionObserver] = list(), observer_dispatcher: Optional[ObserverDispatcher] = None, trace_id: str = (lambda: str(uuid4()))(), span_id: str = (lambda: str(uuid4()))())\n</code></pre> <p>Lightweight orchestration state (passed between pipeline stages).</p> <p>Scope: Runtime execution state and progress tracking Pattern: Memento (serializable for checkpointing)</p> <p>Cost Tracking in ExecutionContext: - Simple accumulation for orchestration purposes - Used by: Executors to track overall progress - NOT for: Detailed accounting (use CostTracker for that)</p> <p>Why separate from CostTracker? - ExecutionContext = orchestration state (stage progress, session ID, timing) - CostTracker = detailed accounting (per-stage breakdowns, thread-safe entries, metrics) - Different concerns, different use cases</p> <p>ExecutionContext is: - Passed between stages in the pipeline - Serialized for checkpointing - Focused on execution orchestration</p> <p>CostTracker is: - Used within LLMInvocationStage for detailed tracking - Thread-safe for concurrent operations - Focused on cost reporting and analytics</p> <p>See Also: - CostTracker: For detailed cost accounting with breakdowns - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Carries shared state between stages and tracks progress. Immutable for most fields to prevent accidental modification.</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.update_stage","title":"update_stage","text":"<pre><code>update_stage(stage_index: int) -&gt; None\n</code></pre> <p>Update current stage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_stage(self, stage_index: int) -&gt; None:\n    \"\"\"Update current stage.\"\"\"\n    self.current_stage_index = stage_index\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.update_row","title":"update_row","text":"<pre><code>update_row(row_index: int) -&gt; None\n</code></pre> <p>Update last processed row.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_row(self, row_index: int) -&gt; None:\n    \"\"\"Update last processed row.\"\"\"\n    self.last_processed_row = row_index\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.add_cost","title":"add_cost","text":"<pre><code>add_cost(cost: Decimal, tokens: int) -&gt; None\n</code></pre> <p>Add cost and token usage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def add_cost(self, cost: Decimal, tokens: int) -&gt; None:\n    \"\"\"Add cost and token usage.\"\"\"\n    self.accumulated_cost += cost\n    self.accumulated_tokens += tokens\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.notify_progress","title":"notify_progress","text":"<pre><code>notify_progress() -&gt; None\n</code></pre> <p>Notify all observers of progress update.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def notify_progress(self) -&gt; None:\n    \"\"\"Notify all observers of progress update.\"\"\"\n    for observer in self.observers:\n        try:\n            observer.on_progress_update(self)\n        except Exception:  # nosec B110\n            # Silently ignore observer errors to not break pipeline\n            # Observers are non-critical, pipeline should continue even if they fail\n            pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.get_progress","title":"get_progress","text":"<pre><code>get_progress() -&gt; float\n</code></pre> <p>Get completion percentage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_progress(self) -&gt; float:\n    \"\"\"Get completion percentage.\"\"\"\n    if self.total_rows == 0:\n        return 0.0\n    return (self.last_processed_row / self.total_rows) * 100\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; ProcessingStats\n</code></pre> <p>Get processing statistics.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_stats(self) -&gt; ProcessingStats:\n    \"\"\"Get processing statistics.\"\"\"\n    duration = (\n        (datetime.now() - self.start_time).total_seconds()\n        if self.end_time is None\n        else (self.end_time - self.start_time).total_seconds()\n    )\n\n    # last_processed_row is 0-based index, so add 1 for count\n    actual_processed = (\n        self.last_processed_row + 1 if self.last_processed_row &gt;= 0 else 0\n    )\n\n    rows_per_second = actual_processed / duration if duration &gt; 0 else 0.0\n\n    return ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=actual_processed,\n        failed_rows=self.failed_rows,\n        skipped_rows=self.skipped_rows,\n        rows_per_second=rows_per_second,\n        total_duration_seconds=duration,\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.to_checkpoint","title":"to_checkpoint","text":"<pre><code>to_checkpoint() -&gt; dict[str, Any]\n</code></pre> <p>Serialize to checkpoint dictionary (Memento pattern).</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation for persistence</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_checkpoint(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize to checkpoint dictionary (Memento pattern).\n\n    Returns:\n        Dictionary representation for persistence\n    \"\"\"\n    return {\n        \"session_id\": str(self.session_id),\n        \"pipeline_id\": str(self.pipeline_id),\n        \"start_time\": self.start_time.isoformat(),\n        \"end_time\": self.end_time.isoformat() if self.end_time else None,\n        \"current_stage_index\": self.current_stage_index,\n        \"last_processed_row\": self.last_processed_row,\n        \"total_rows\": self.total_rows,\n        \"accumulated_cost\": str(self.accumulated_cost),\n        \"accumulated_tokens\": self.accumulated_tokens,\n        \"intermediate_data\": self.intermediate_data,\n        \"failed_rows\": self.failed_rows,\n        \"skipped_rows\": self.skipped_rows,\n    }\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.from_checkpoint","title":"from_checkpoint  <code>classmethod</code>","text":"<pre><code>from_checkpoint(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Deserialize from checkpoint dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data</p> required <p>Returns:</p> Type Description <code>ExecutionContext</code> <p>Restored ExecutionContext</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_checkpoint(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"\n    Deserialize from checkpoint dictionary.\n\n    Args:\n        data: Checkpoint data\n\n    Returns:\n        Restored ExecutionContext\n    \"\"\"\n    return cls(\n        session_id=UUID(data[\"session_id\"]),\n        pipeline_id=UUID(data[\"pipeline_id\"]),\n        start_time=datetime.fromisoformat(data[\"start_time\"]),\n        end_time=(\n            datetime.fromisoformat(data[\"end_time\"])\n            if data.get(\"end_time\")\n            else None\n        ),\n        current_stage_index=data[\"current_stage_index\"],\n        last_processed_row=data[\"last_processed_row\"],\n        total_rows=data[\"total_rows\"],\n        accumulated_cost=Decimal(data[\"accumulated_cost\"]),\n        accumulated_tokens=data[\"accumulated_tokens\"],\n        intermediate_data=data.get(\"intermediate_data\", {}),\n        failed_rows=data.get(\"failed_rows\", 0),\n        skipped_rows=data.get(\"skipped_rows\", 0),\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Alias for to_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Alias for to_checkpoint().\"\"\"\n    return self.to_checkpoint()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Alias for from_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"Alias for from_checkpoint().\"\"\"\n    return cls.from_checkpoint(data)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy","title":"ExecutionStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution strategies.</p> <p>Follows Strategy pattern: defines interface for executing pipeline stages in different modes (sync, async, streaming).</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name for logging.</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.execute","title":"execute  <code>abstractmethod</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]\n</code></pre> <p>Execute pipeline stages.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>List of pipeline stages to execute</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context for state management</p> required <p>Returns:</p> Type Description <code>ExecutionResult | Iterator[DataFrame] | AsyncIterator[DataFrame]</code> <p>ExecutionResult or iterator for streaming</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]:\n    \"\"\"\n    Execute pipeline stages.\n\n    Args:\n        stages: List of pipeline stages to execute\n        context: Execution context for state management\n\n    Returns:\n        ExecutionResult or iterator for streaming\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.supports_async","title":"supports_async  <code>abstractmethod</code>","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Whether this strategy supports async execution.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_async(self) -&gt; bool:\n    \"\"\"Whether this strategy supports async execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.supports_streaming","title":"supports_streaming  <code>abstractmethod</code>","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Whether this strategy supports streaming.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_streaming(self) -&gt; bool:\n    \"\"\"Whether this strategy supports streaming.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver","title":"CostTrackingObserver","text":"<pre><code>CostTrackingObserver(warning_threshold: float = 0.75)\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that tracks and warns about costs.</p> <p>Initialize cost tracking observer.</p> <p>Parameters:</p> Name Type Description Default <code>warning_threshold</code> <code>float</code> <p>Warn when this fraction of budget used</p> <code>0.75</code> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self, warning_threshold: float = 0.75):\n    \"\"\"\n    Initialize cost tracking observer.\n\n    Args:\n        warning_threshold: Warn when this fraction of budget used\n    \"\"\"\n    self.logger = get_logger(__name__)\n    self.warning_threshold = warning_threshold\n    self.max_budget: float | None = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Set max budget if available.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Set max budget if available.\"\"\"\n    # Could extract from pipeline specs\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on stage start.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Check cost after stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Check cost after stage completion.\"\"\"\n    if self.max_budget:\n        usage_ratio = float(context.accumulated_cost) / self.max_budget\n\n        if usage_ratio &gt;= self.warning_threshold:\n            self.logger.warning(\n                f\"Cost warning: {usage_ratio * 100:.1f}% of budget used \"\n                f\"(${context.accumulated_cost:.4f} / ${self.max_budget:.2f})\"\n            )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>No action on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"No action on error.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log final cost summary.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log final cost summary.\"\"\"\n    self.logger.info(\n        f\"Cost summary:\\n\"\n        f\"  Total: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Input tokens: {result.costs.input_tokens:,}\\n\"\n        f\"  Output tokens: {result.costs.output_tokens:,}\\n\"\n        f\"  Cost per row: ${float(result.costs.total_cost) / result.metrics.total_rows:.6f}\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log cost at failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log cost at failure.\"\"\"\n    self.logger.info(f\"Cost at failure: ${context.accumulated_cost:.4f}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on progress update for cost tracking.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on progress update for cost tracking.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver","title":"ExecutionObserver","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution observers.</p> <p>Observers can monitor pipeline execution without coupling to the executor implementation.</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_pipeline_start","title":"on_pipeline_start  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before first stage execution.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before first stage execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_stage_start","title":"on_stage_start  <code>abstractmethod</code>","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before each stage.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before each stage.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_stage_complete","title":"on_stage_complete  <code>abstractmethod</code>","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Called after successful stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Called after successful stage completion.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_stage_error","title":"on_stage_error  <code>abstractmethod</code>","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on stage failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Called on stage failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_pipeline_complete","title":"on_pipeline_complete  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Called after all stages complete.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Called after all stages complete.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_pipeline_error","title":"on_pipeline_error  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on fatal pipeline failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Called on fatal pipeline failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Called periodically during execution for progress updates.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Called periodically during execution for progress updates.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver","title":"LoggingObserver","text":"<pre><code>LoggingObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that logs execution events.</p> <p>Initialize logging observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize logging observer.\"\"\"\n    self.logger = get_logger(__name__)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log pipeline start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Log pipeline start.\"\"\"\n    self.logger.info(f\"Pipeline execution started (session: {context.session_id})\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Log stage start.\"\"\"\n    self.logger.info(\n        f\"Starting stage: {stage.name} (progress: {context.get_progress():.1f}%)\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Log stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Log stage completion.\"\"\"\n    self.logger.info(\n        f\"Completed stage: {stage.name} (cost: ${context.accumulated_cost:.4f})\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log stage error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Log stage error.\"\"\"\n    self.logger.error(f\"Stage {stage.name} failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log pipeline completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log pipeline completion.\"\"\"\n    self.logger.info(\n        f\"Pipeline execution completed successfully\\n\"\n        f\"  Processed: {result.metrics.processed_rows} rows\\n\"\n        f\"  Duration: {result.metrics.total_duration_seconds:.2f}s\\n\"\n        f\"  Total cost: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Errors: {result.metrics.failed_rows}\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log pipeline error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log pipeline error.\"\"\"\n    self.logger.error(f\"Pipeline execution failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Log progress update.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Log progress update.\"\"\"\n    # Make progress very visible with separators\n    self.logger.info(\n        f\"\u2501\u2501\u2501\u2501\u2501\u2501 PROGRESS: {context.last_processed_row}/{context.total_rows} rows \"\n        f\"({context.get_progress():.1f}%) | Cost: ${context.accumulated_cost:.4f} \u2501\u2501\u2501\u2501\u2501\u2501\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver","title":"ProgressBarObserver","text":"<pre><code>ProgressBarObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that displays progress bar with tqdm.</p> <p>Initialize progress bar observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize progress bar observer.\"\"\"\n    self.progress_bar: tqdm | None = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Initialize progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Initialize progress bar.\"\"\"\n    if context.total_rows &gt; 0:\n        self.progress_bar = tqdm(\n            total=context.total_rows,\n            desc=\"Processing\",\n            unit=\"rows\",\n        )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar description.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar description.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Stage: {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Update progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Update progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Handle error in progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Handle error in progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Error in {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Close progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Close progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close progress bar on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Close progress bar on error.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar with current row count.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar with current row count.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.set_postfix(\n            {\n                \"cost\": f\"${context.accumulated_cost:.4f}\",\n                \"progress\": f\"{context.get_progress():.1f}%\",\n            }\n        )\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingProgressTracker","title":"LoggingProgressTracker","text":"<pre><code>LoggingProgressTracker()\n</code></pre> <p>               Bases: <code>ProgressTracker</code></p> <p>Fallback progress tracker using standard logging.</p> <p>Used when: - Not running in a TTY (CI, logs to file) - rich library not available - User explicitly requests logging mode</p> <p>Provides basic progress updates via log messages without fancy UI.</p> Example <pre><code>tracker = LoggingProgressTracker()\n\nwith tracker:\n    task = tracker.start_stage(\"Classification\", total_rows=1000)\n    # Logs: \"Starting Classification (1000 rows)\"\n\n    for i in range(1000):\n        tracker.update(task, advance=1)\n        # Logs periodically: \"Classification: 250/1000 (25%)\"\n\n    tracker.finish(task)\n    # Logs: \"Completed Classification\"\n</code></pre> <p>Initialize logging tracker.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize logging tracker.\"\"\"\n    from ondine.utils import get_logger\n\n    self.logger = get_logger(__name__)\n    self.tasks: dict[str, dict[str, Any]] = {}\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingProgressTracker.start_stage","title":"start_stage","text":"<pre><code>start_stage(stage_name: str, total_rows: int, **metadata: Any) -&gt; str\n</code></pre> <p>Start tracking via logging.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def start_stage(self, stage_name: str, total_rows: int, **metadata: Any) -&gt; str:\n    \"\"\"Start tracking via logging.\"\"\"\n    self.tasks[stage_name] = {\n        \"total\": total_rows,\n        \"current\": 0,\n        \"cost\": Decimal(\"0.0\"),\n        \"last_log_percent\": 0,\n    }\n    self.logger.info(f\"Starting {stage_name} ({total_rows} rows)\")\n    return stage_name\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingProgressTracker.update","title":"update","text":"<pre><code>update(task_id: str, advance: int = 1, **metadata: Any) -&gt; None\n</code></pre> <p>Update progress via periodic logging.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def update(self, task_id: str, advance: int = 1, **metadata: Any) -&gt; None:\n    \"\"\"Update progress via periodic logging.\"\"\"\n    if task_id not in self.tasks:\n        return\n\n    task = self.tasks[task_id]\n    task[\"current\"] += advance\n\n    if \"cost\" in metadata:\n        task[\"cost\"] += Decimal(str(metadata[\"cost\"]))\n\n    # Log at 25%, 50%, 75%, 100%\n    percent = (task[\"current\"] / task[\"total\"]) * 100\n    milestones = [25, 50, 75, 100]\n\n    for milestone in milestones:\n        if percent &gt;= milestone and task[\"last_log_percent\"] &lt; milestone:\n            self.logger.info(\n                f\"{task_id}: {task['current']}/{task['total']} \"\n                f\"({percent:.1f}%) | Cost: ${task['cost']:.4f}\"\n            )\n            task[\"last_log_percent\"] = milestone\n            break\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingProgressTracker.finish","title":"finish","text":"<pre><code>finish(task_id: str) -&gt; None\n</code></pre> <p>Log completion.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def finish(self, task_id: str) -&gt; None:\n    \"\"\"Log completion.\"\"\"\n    if task_id in self.tasks:\n        task = self.tasks[task_id]\n        self.logger.info(\n            f\"Completed {task_id}: {task['current']}/{task['total']} rows, \"\n            f\"${task['cost']:.4f}\"\n        )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressTracker","title":"ProgressTracker","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for progress tracking.</p> <p>Enables pluggable progress tracking implementations (rich, tqdm, logging) without coupling pipeline code to specific libraries.</p> <p>Design Pattern: Strategy Pattern - Pipeline depends on ProgressTracker interface (abstraction) - Concrete implementations (RichProgressTracker, TqdmProgressTracker) are interchangeable - Follows Dependency Inversion Principle (SOLID)</p> Example <pre><code>tracker = create_progress_tracker(mode=\"auto\")\n\nwith tracker:\n    task_id = tracker.start_stage(\"Classification\", total_rows=1000)\n\n    for row in rows:\n        process(row)\n        tracker.update(task_id, advance=1, cost=0.001)\n\n    tracker.finish(task_id)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressTracker.start_stage","title":"start_stage  <code>abstractmethod</code>","text":"<pre><code>start_stage(stage_name: str, total_rows: int, **metadata: Any) -&gt; str\n</code></pre> <p>Start tracking a new stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Human-readable stage name (e.g., \"Primary Category Classification\")</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows to process</p> required <code>**metadata</code> <code>Any</code> <p>Additional metadata (cost_so_far, stage_number, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Task ID for updating progress</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>@abstractmethod\ndef start_stage(self, stage_name: str, total_rows: int, **metadata: Any) -&gt; str:\n    \"\"\"\n    Start tracking a new stage.\n\n    Args:\n        stage_name: Human-readable stage name (e.g., \"Primary Category Classification\")\n        total_rows: Total number of rows to process\n        **metadata: Additional metadata (cost_so_far, stage_number, etc.)\n\n    Returns:\n        Task ID for updating progress\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressTracker.update","title":"update  <code>abstractmethod</code>","text":"<pre><code>update(task_id: str, advance: int = 1, **metadata: Any) -&gt; None\n</code></pre> <p>Update progress for a task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>Task identifier from start_stage()</p> required <code>advance</code> <code>int</code> <p>Number of rows processed</p> <code>1</code> <code>**metadata</code> <code>Any</code> <p>Additional metadata (cost, tokens, etc.)</p> <code>{}</code> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>@abstractmethod\ndef update(self, task_id: str, advance: int = 1, **metadata: Any) -&gt; None:\n    \"\"\"\n    Update progress for a task.\n\n    Args:\n        task_id: Task identifier from start_stage()\n        advance: Number of rows processed\n        **metadata: Additional metadata (cost, tokens, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressTracker.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(task_id: str) -&gt; None\n</code></pre> <p>Mark task as complete.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>Task identifier</p> required Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>@abstractmethod\ndef finish(self, task_id: str) -&gt; None:\n    \"\"\"\n    Mark task as complete.\n\n    Args:\n        task_id: Task identifier\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.RichProgressTracker","title":"RichProgressTracker","text":"<pre><code>RichProgressTracker()\n</code></pre> <p>               Bases: <code>ProgressTracker</code></p> <p>Progress tracker using rich.progress for beautiful terminal UI.</p> <p>Features: - Multiple progress bars (one per stage) - Automatic ETA and throughput calculation - Color-coded output - Cost tracking per stage - Thread-safe for concurrent execution</p> Requires <p>rich library (already a dependency)</p> Example <pre><code>tracker = RichProgressTracker()\n\nwith tracker:\n    task = tracker.start_stage(\"Stage 1: Classification\", total_rows=1000)\n\n    for i in range(1000):\n        process_row(i)\n        tracker.update(task, advance=1, cost=0.001)\n\n    tracker.finish(task)\n</code></pre> <p>Initialize rich progress tracker.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize rich progress tracker.\"\"\"\n    from rich.progress import (\n        BarColumn,\n        Progress,\n        SpinnerColumn,\n        TaskProgressColumn,\n        TextColumn,\n        TimeElapsedColumn,\n        TimeRemainingColumn,\n    )\n\n    self.progress = Progress(\n        SpinnerColumn(),\n        TextColumn(\"[bold blue]{task.description}\"),\n        BarColumn(),\n        TaskProgressColumn(),\n        TimeRemainingColumn(),\n        TimeElapsedColumn(),\n        TextColumn(\"[bold green]${task.fields[cost]:.4f}\"),\n        expand=True,\n    )\n    self.tasks: dict[str, Any] = {}\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.RichProgressTracker.start_stage","title":"start_stage","text":"<pre><code>start_stage(stage_name: str, total_rows: int, **metadata: Any) -&gt; str\n</code></pre> <p>Start tracking a stage with rich progress bar.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def start_stage(self, stage_name: str, total_rows: int, **metadata: Any) -&gt; str:\n    \"\"\"Start tracking a stage with rich progress bar.\"\"\"\n    task_id = self.progress.add_task(\n        f\"\ud83d\ude80 {stage_name}\",\n        total=total_rows,\n        cost=metadata.get(\"cost\", 0.0),\n    )\n    self.tasks[stage_name] = task_id\n    return stage_name  # Use stage_name as task_id for simplicity\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.RichProgressTracker.update","title":"update","text":"<pre><code>update(task_id: str, advance: int = 1, **metadata: Any) -&gt; None\n</code></pre> <p>Update progress bar.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def update(self, task_id: str, advance: int = 1, **metadata: Any) -&gt; None:\n    \"\"\"Update progress bar.\"\"\"\n    if task_id not in self.tasks:\n        return\n\n    rich_task_id = self.tasks[task_id]\n\n    # Update cost if provided\n    update_kwargs = {\"advance\": advance}\n    if \"cost\" in metadata:\n        # Get current cost and add new cost\n        task = self.progress.tasks[rich_task_id]\n        current_cost = task.fields.get(\"cost\", 0.0)\n        new_cost = float(current_cost) + float(metadata[\"cost\"])\n        update_kwargs[\"cost\"] = new_cost\n\n    self.progress.update(rich_task_id, **update_kwargs)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.RichProgressTracker.finish","title":"finish","text":"<pre><code>finish(task_id: str) -&gt; None\n</code></pre> <p>Mark task as complete.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def finish(self, task_id: str) -&gt; None:\n    \"\"\"Mark task as complete.\"\"\"\n    if task_id in self.tasks:\n        rich_task_id = self.tasks[task_id]\n        self.progress.update(rich_task_id, completed=True)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager","title":"StateManager","text":"<pre><code>StateManager(storage: CheckpointStorage, checkpoint_interval: int = 500)\n</code></pre> <p>Manages execution state persistence and recovery.</p> <p>Follows Single Responsibility: only handles state management. Uses Strategy pattern for pluggable storage backends.</p> <p>Initialize state manager.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>CheckpointStorage</code> <p>Checkpoint storage backend</p> required <code>checkpoint_interval</code> <code>int</code> <p>Rows between checkpoints</p> <code>500</code> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def __init__(self, storage: CheckpointStorage, checkpoint_interval: int = 500):\n    \"\"\"\n    Initialize state manager.\n\n    Args:\n        storage: Checkpoint storage backend\n        checkpoint_interval: Rows between checkpoints\n    \"\"\"\n    self.storage = storage\n    self.checkpoint_interval = checkpoint_interval\n    self._last_checkpoint_row = 0\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.should_checkpoint","title":"should_checkpoint","text":"<pre><code>should_checkpoint(current_row: int) -&gt; bool\n</code></pre> <p>Check if checkpoint should be saved.</p> <p>Parameters:</p> Name Type Description Default <code>current_row</code> <code>int</code> <p>Current row index</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint due</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def should_checkpoint(self, current_row: int) -&gt; bool:\n    \"\"\"\n    Check if checkpoint should be saved.\n\n    Args:\n        current_row: Current row index\n\n    Returns:\n        True if checkpoint due\n    \"\"\"\n    return (current_row - self._last_checkpoint_row) &gt;= self.checkpoint_interval\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(context: ExecutionContext) -&gt; bool\n</code></pre> <p>Save checkpoint for execution context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Execution context to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def save_checkpoint(self, context: ExecutionContext) -&gt; bool:\n    \"\"\"\n    Save checkpoint for execution context.\n\n    Args:\n        context: Execution context to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        checkpoint_data = context.to_checkpoint()\n        success = self.storage.save(context.session_id, checkpoint_data)\n\n        if success:\n            self._last_checkpoint_row = context.last_processed_row\n            logger.info(f\"Checkpoint saved at row {context.last_processed_row}\")\n\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to save checkpoint: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(session_id: UUID) -&gt; ExecutionContext | None\n</code></pre> <p>Load checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>ExecutionContext | None</code> <p>Restored execution context or None</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def load_checkpoint(self, session_id: UUID) -&gt; ExecutionContext | None:\n    \"\"\"\n    Load checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Restored execution context or None\n    \"\"\"\n    try:\n        checkpoint_data = self.storage.load(session_id)\n\n        if checkpoint_data is None:\n            return None\n\n        context = ExecutionContext.from_checkpoint(checkpoint_data)\n        logger.info(f\"Checkpoint loaded from row {context.last_processed_row}\")\n\n        return context\n    except Exception as e:\n        logger.error(f\"Failed to load checkpoint: {e}\")\n        return None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.can_resume","title":"can_resume","text":"<pre><code>can_resume(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if session can be resumed.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint exists</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def can_resume(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if session can be resumed.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if checkpoint exists\n    \"\"\"\n    return self.storage.exists(session_id)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.cleanup_checkpoints","title":"cleanup_checkpoints","text":"<pre><code>cleanup_checkpoints(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoints for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def cleanup_checkpoints(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoints for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    try:\n        success = self.storage.delete(session_id)\n        if success:\n            logger.info(f\"Checkpoints cleaned up for session {session_id}\")\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to cleanup checkpoints: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    return self.storage.list_checkpoints()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor","title":"StreamingExecutor","text":"<pre><code>StreamingExecutor(chunk_size: int = 1000)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Streaming execution strategy.</p> <p>Processes data in chunks to maintain constant memory usage. Ideal for very large datasets (100K+ rows) that don't fit in memory.</p> <p>Benefits: - Constant memory footprint - Can process unlimited dataset sizes - Checkpoints at chunk boundaries - Early results available</p> <p>Initialize streaming executor.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self, chunk_size: int = 1000):\n    \"\"\"\n    Initialize streaming executor.\n\n    Args:\n        chunk_size: Number of rows per chunk\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Execute stages in streaming mode.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrames with processed chunks</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Execute stages in streaming mode.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Yields:\n        DataFrames with processed chunks\n    \"\"\"\n    self.logger.info(f\"Starting streaming execution (chunk_size={self.chunk_size})\")\n\n    # Get data loader stage\n    data_loader = stages[0]\n\n    # Stream data in chunks\n    chunk_index = 0\n    total_rows_processed = 0\n\n    # Read data in chunks\n    for chunk in self._read_chunks(data_loader, context):\n        self.logger.info(f\"Processing chunk {chunk_index} ({len(chunk)} rows)\")\n\n        # Process chunk through remaining stages\n        result_chunk = self._process_chunk(chunk, stages[1:], context)\n\n        # Update context\n        total_rows_processed += len(result_chunk)\n        context.update_row(total_rows_processed - 1)\n\n        # Yield result\n        yield result_chunk\n\n        chunk_index += 1\n\n    self.logger.info(\n        f\"Streaming execution complete: {total_rows_processed} rows, \"\n        f\"{chunk_index} chunks\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Streaming executor doesn't support async.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Streaming executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Streaming executor supports streaming.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Streaming executor supports streaming.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute_stream(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Alias for execute() method.\"\"\"\n    return self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingResult","title":"StreamingResult","text":"<pre><code>StreamingResult()\n</code></pre> <p>Result container for streaming execution.</p> <p>Provides access to metrics after consuming the stream.</p> <p>Initialize streaming result.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize streaming result.\"\"\"\n    self.chunks_processed = 0\n    self.total_rows = 0\n    self.total_cost = Decimal(\"0.0\")\n    self.start_time = datetime.now()\n    self.end_time = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingResult.add_chunk","title":"add_chunk","text":"<pre><code>add_chunk(chunk: DataFrame, cost: Decimal)\n</code></pre> <p>Add chunk statistics.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def add_chunk(self, chunk: pd.DataFrame, cost: Decimal):\n    \"\"\"Add chunk statistics.\"\"\"\n    self.chunks_processed += 1\n    self.total_rows += len(chunk)\n    self.total_cost += cost\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingResult.finalize","title":"finalize","text":"<pre><code>finalize() -&gt; ExecutionResult\n</code></pre> <p>Create final ExecutionResult.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def finalize(self) -&gt; ExecutionResult:\n    \"\"\"Create final ExecutionResult.\"\"\"\n    self.end_time = datetime.now()\n    duration = (self.end_time - self.start_time).total_seconds()\n\n    stats = ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=self.total_rows,\n        failed_rows=0,\n        skipped_rows=0,\n        rows_per_second=self.total_rows / duration if duration &gt; 0 else 0,\n        total_duration_seconds=duration,\n    )\n\n    costs = CostEstimate(\n        total_cost=self.total_cost,\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=self.total_rows,\n        confidence=\"actual\",\n    )\n\n    return ExecutionResult(\n        data=pd.DataFrame(),  # Streaming doesn't return full data\n        metrics=stats,\n        costs=costs,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        success=True,\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor","title":"SyncExecutor","text":"<pre><code>SyncExecutor()\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Synchronous execution strategy.</p> <p>Uses ThreadPoolExecutor for concurrent LLM calls while maintaining sequential stage execution. This is the default strategy that preserves current behavior.</p> <p>Initialize synchronous executor.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize synchronous executor.\"\"\"\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages synchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages sequentially\n        result_data = self._execute_stages(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate (leverage LlamaIndex token counts from intermediate_data)\n        token_tracking = context.intermediate_data.get(\"token_tracking\", {})\n        input_tokens = token_tracking.get(\"input_tokens\", 0)\n        output_tokens = token_tracking.get(\"output_tokens\", 0)\n\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Sync executor doesn't support async.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Sync executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.create_progress_tracker","title":"create_progress_tracker","text":"<pre><code>create_progress_tracker(mode: str = 'auto') -&gt; ProgressTracker\n</code></pre> <p>Factory function to create appropriate progress tracker.</p> <p>Automatically detects the best progress tracker based on environment and available libraries, or uses explicit mode if specified.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Progress tracker mode - \"auto\": Auto-detect (rich if TTY, else logging) - \"rich\": Use rich.progress (beautiful UI) - \"tqdm\": Use tqdm (simple, compatible) - \"logging\": Use standard logging (fallback) - \"none\": Disable progress tracking</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ProgressTracker</code> <p>ProgressTracker implementation</p> Example <pre><code># Auto-detect best option\ntracker = create_progress_tracker(mode=\"auto\")\n\n# Force rich (will fail if not available)\ntracker = create_progress_tracker(mode=\"rich\")\n\n# Force logging (always works)\ntracker = create_progress_tracker(mode=\"logging\")\n</code></pre> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def create_progress_tracker(mode: str = \"auto\") -&gt; ProgressTracker:\n    \"\"\"\n    Factory function to create appropriate progress tracker.\n\n    Automatically detects the best progress tracker based on environment\n    and available libraries, or uses explicit mode if specified.\n\n    Args:\n        mode: Progress tracker mode\n            - \"auto\": Auto-detect (rich if TTY, else logging)\n            - \"rich\": Use rich.progress (beautiful UI)\n            - \"tqdm\": Use tqdm (simple, compatible)\n            - \"logging\": Use standard logging (fallback)\n            - \"none\": Disable progress tracking\n\n    Returns:\n        ProgressTracker implementation\n\n    Example:\n        ```python\n        # Auto-detect best option\n        tracker = create_progress_tracker(mode=\"auto\")\n\n        # Force rich (will fail if not available)\n        tracker = create_progress_tracker(mode=\"rich\")\n\n        # Force logging (always works)\n        tracker = create_progress_tracker(mode=\"logging\")\n        ```\n    \"\"\"\n    if mode == \"none\":\n        return NoOpProgressTracker()\n\n    if mode == \"auto\":\n        # Auto-detect best option\n        if sys.stdout.isatty():\n            # Running in terminal - try rich first\n            try:\n                from rich.progress import Progress  # noqa: F401\n\n                return RichProgressTracker()\n            except ImportError:\n                # rich not available, fall back to logging\n                return LoggingProgressTracker()\n        else:\n            # Non-TTY environment (CI, logs to file) - use logging\n            return LoggingProgressTracker()\n\n    elif mode == \"rich\":\n        return RichProgressTracker()\n\n    elif mode == \"tqdm\":\n        # Future: implement TqdmProgressTracker\n        raise NotImplementedError(\n            \"tqdm tracker not yet implemented, use 'rich' or 'logging'\"\n        )\n\n    elif mode == \"logging\":\n        return LoggingProgressTracker()\n\n    else:\n        raise ValueError(\n            f\"Invalid progress mode: {mode}. \"\n            f\"Use 'auto', 'rich', 'tqdm', 'logging', or 'none'\"\n        )\n</code></pre>"},{"location":"api/orchestration/async_executor/","title":"async_executor","text":""},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor","title":"async_executor","text":"<p>Asynchronous execution strategy.</p> <p>Provides async/await support for non-blocking execution, ideal for integration with FastAPI, aiohttp, and other async frameworks.</p>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor","title":"AsyncExecutor","text":"<pre><code>AsyncExecutor(max_concurrency: int = 10)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Asynchronous execution strategy.</p> <p>Uses asyncio for true non-blocking execution. Leverages LlamaIndex's async methods (acomplete) for concurrent LLM calls without threads.</p> <p>Benefits: - Non-blocking (works with FastAPI, aiohttp) - Better resource utilization - Higher concurrency without thread overhead - Ideal for I/O-bound operations</p> <p>Initialize async executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def __init__(self, max_concurrency: int = 10):\n    \"\"\"\n    Initialize async executor.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n    \"\"\"\n    self.max_concurrency = max_concurrency\n    self.logger = logger\n    self.semaphore = asyncio.Semaphore(max_concurrency)\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages asynchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages with async/await\n        result_data = await self._execute_stages_async(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate (leverage LlamaIndex token counts from intermediate_data)\n        token_tracking = context.intermediate_data.get(\"token_tracking\", {})\n        input_tokens = token_tracking.get(\"input_tokens\", 0)\n        output_tokens = token_tracking.get(\"output_tokens\", 0)\n\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Async pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Async executor supports async.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Async executor supports async.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Async executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Async executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute_async(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"Alias for execute() method.\"\"\"\n    return await self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/execution_context/","title":"execution_context","text":""},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context","title":"execution_context","text":"<p>Execution context for carrying runtime state between stages.</p> <p>Implements Memento pattern for checkpoint serialization.</p>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext","title":"ExecutionContext  <code>dataclass</code>","text":"<pre><code>ExecutionContext(session_id: UUID = uuid4(), pipeline_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, current_stage_index: int = 0, last_processed_row: int = 0, total_rows: int = 0, accumulated_cost: Decimal = (lambda: Decimal('0.0'))(), accumulated_tokens: int = 0, intermediate_data: dict[str, Any] = dict(), failed_rows: int = 0, skipped_rows: int = 0, observers: list[ExecutionObserver] = list(), observer_dispatcher: Optional[ObserverDispatcher] = None, trace_id: str = (lambda: str(uuid4()))(), span_id: str = (lambda: str(uuid4()))())\n</code></pre> <p>Lightweight orchestration state (passed between pipeline stages).</p> <p>Scope: Runtime execution state and progress tracking Pattern: Memento (serializable for checkpointing)</p> <p>Cost Tracking in ExecutionContext: - Simple accumulation for orchestration purposes - Used by: Executors to track overall progress - NOT for: Detailed accounting (use CostTracker for that)</p> <p>Why separate from CostTracker? - ExecutionContext = orchestration state (stage progress, session ID, timing) - CostTracker = detailed accounting (per-stage breakdowns, thread-safe entries, metrics) - Different concerns, different use cases</p> <p>ExecutionContext is: - Passed between stages in the pipeline - Serialized for checkpointing - Focused on execution orchestration</p> <p>CostTracker is: - Used within LLMInvocationStage for detailed tracking - Thread-safe for concurrent operations - Focused on cost reporting and analytics</p> <p>See Also: - CostTracker: For detailed cost accounting with breakdowns - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Carries shared state between stages and tracks progress. Immutable for most fields to prevent accidental modification.</p>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.update_stage","title":"update_stage","text":"<pre><code>update_stage(stage_index: int) -&gt; None\n</code></pre> <p>Update current stage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_stage(self, stage_index: int) -&gt; None:\n    \"\"\"Update current stage.\"\"\"\n    self.current_stage_index = stage_index\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.update_row","title":"update_row","text":"<pre><code>update_row(row_index: int) -&gt; None\n</code></pre> <p>Update last processed row.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_row(self, row_index: int) -&gt; None:\n    \"\"\"Update last processed row.\"\"\"\n    self.last_processed_row = row_index\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.add_cost","title":"add_cost","text":"<pre><code>add_cost(cost: Decimal, tokens: int) -&gt; None\n</code></pre> <p>Add cost and token usage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def add_cost(self, cost: Decimal, tokens: int) -&gt; None:\n    \"\"\"Add cost and token usage.\"\"\"\n    self.accumulated_cost += cost\n    self.accumulated_tokens += tokens\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.notify_progress","title":"notify_progress","text":"<pre><code>notify_progress() -&gt; None\n</code></pre> <p>Notify all observers of progress update.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def notify_progress(self) -&gt; None:\n    \"\"\"Notify all observers of progress update.\"\"\"\n    for observer in self.observers:\n        try:\n            observer.on_progress_update(self)\n        except Exception:  # nosec B110\n            # Silently ignore observer errors to not break pipeline\n            # Observers are non-critical, pipeline should continue even if they fail\n            pass\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.get_progress","title":"get_progress","text":"<pre><code>get_progress() -&gt; float\n</code></pre> <p>Get completion percentage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_progress(self) -&gt; float:\n    \"\"\"Get completion percentage.\"\"\"\n    if self.total_rows == 0:\n        return 0.0\n    return (self.last_processed_row / self.total_rows) * 100\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; ProcessingStats\n</code></pre> <p>Get processing statistics.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_stats(self) -&gt; ProcessingStats:\n    \"\"\"Get processing statistics.\"\"\"\n    duration = (\n        (datetime.now() - self.start_time).total_seconds()\n        if self.end_time is None\n        else (self.end_time - self.start_time).total_seconds()\n    )\n\n    # last_processed_row is 0-based index, so add 1 for count\n    actual_processed = (\n        self.last_processed_row + 1 if self.last_processed_row &gt;= 0 else 0\n    )\n\n    rows_per_second = actual_processed / duration if duration &gt; 0 else 0.0\n\n    return ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=actual_processed,\n        failed_rows=self.failed_rows,\n        skipped_rows=self.skipped_rows,\n        rows_per_second=rows_per_second,\n        total_duration_seconds=duration,\n    )\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.to_checkpoint","title":"to_checkpoint","text":"<pre><code>to_checkpoint() -&gt; dict[str, Any]\n</code></pre> <p>Serialize to checkpoint dictionary (Memento pattern).</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation for persistence</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_checkpoint(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize to checkpoint dictionary (Memento pattern).\n\n    Returns:\n        Dictionary representation for persistence\n    \"\"\"\n    return {\n        \"session_id\": str(self.session_id),\n        \"pipeline_id\": str(self.pipeline_id),\n        \"start_time\": self.start_time.isoformat(),\n        \"end_time\": self.end_time.isoformat() if self.end_time else None,\n        \"current_stage_index\": self.current_stage_index,\n        \"last_processed_row\": self.last_processed_row,\n        \"total_rows\": self.total_rows,\n        \"accumulated_cost\": str(self.accumulated_cost),\n        \"accumulated_tokens\": self.accumulated_tokens,\n        \"intermediate_data\": self.intermediate_data,\n        \"failed_rows\": self.failed_rows,\n        \"skipped_rows\": self.skipped_rows,\n    }\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.from_checkpoint","title":"from_checkpoint  <code>classmethod</code>","text":"<pre><code>from_checkpoint(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Deserialize from checkpoint dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data</p> required <p>Returns:</p> Type Description <code>ExecutionContext</code> <p>Restored ExecutionContext</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_checkpoint(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"\n    Deserialize from checkpoint dictionary.\n\n    Args:\n        data: Checkpoint data\n\n    Returns:\n        Restored ExecutionContext\n    \"\"\"\n    return cls(\n        session_id=UUID(data[\"session_id\"]),\n        pipeline_id=UUID(data[\"pipeline_id\"]),\n        start_time=datetime.fromisoformat(data[\"start_time\"]),\n        end_time=(\n            datetime.fromisoformat(data[\"end_time\"])\n            if data.get(\"end_time\")\n            else None\n        ),\n        current_stage_index=data[\"current_stage_index\"],\n        last_processed_row=data[\"last_processed_row\"],\n        total_rows=data[\"total_rows\"],\n        accumulated_cost=Decimal(data[\"accumulated_cost\"]),\n        accumulated_tokens=data[\"accumulated_tokens\"],\n        intermediate_data=data.get(\"intermediate_data\", {}),\n        failed_rows=data.get(\"failed_rows\", 0),\n        skipped_rows=data.get(\"skipped_rows\", 0),\n    )\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Alias for to_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Alias for to_checkpoint().\"\"\"\n    return self.to_checkpoint()\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Alias for from_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"Alias for from_checkpoint().\"\"\"\n    return cls.from_checkpoint(data)\n</code></pre>"},{"location":"api/orchestration/execution_strategy/","title":"execution_strategy","text":""},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy","title":"execution_strategy","text":"<p>Execution strategy abstraction for different execution modes.</p> <p>Implements Strategy pattern to support sync, async, and streaming execution without modifying core pipeline logic.</p>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy","title":"ExecutionStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution strategies.</p> <p>Follows Strategy pattern: defines interface for executing pipeline stages in different modes (sync, async, streaming).</p>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name for logging.</p>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.execute","title":"execute  <code>abstractmethod</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]\n</code></pre> <p>Execute pipeline stages.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>List of pipeline stages to execute</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context for state management</p> required <p>Returns:</p> Type Description <code>ExecutionResult | Iterator[DataFrame] | AsyncIterator[DataFrame]</code> <p>ExecutionResult or iterator for streaming</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]:\n    \"\"\"\n    Execute pipeline stages.\n\n    Args:\n        stages: List of pipeline stages to execute\n        context: Execution context for state management\n\n    Returns:\n        ExecutionResult or iterator for streaming\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.supports_async","title":"supports_async  <code>abstractmethod</code>","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Whether this strategy supports async execution.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_async(self) -&gt; bool:\n    \"\"\"Whether this strategy supports async execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.supports_streaming","title":"supports_streaming  <code>abstractmethod</code>","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Whether this strategy supports streaming.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_streaming(self) -&gt; bool:\n    \"\"\"Whether this strategy supports streaming.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/","title":"observers","text":""},{"location":"api/orchestration/observers/#ondine.orchestration.observers","title":"observers","text":"<p>Execution observers for monitoring and logging.</p> <p>Implements Observer pattern for decoupled event notification.</p>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver","title":"ExecutionObserver","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution observers.</p> <p>Observers can monitor pipeline execution without coupling to the executor implementation.</p>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_pipeline_start","title":"on_pipeline_start  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before first stage execution.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before first stage execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_stage_start","title":"on_stage_start  <code>abstractmethod</code>","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before each stage.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before each stage.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_stage_complete","title":"on_stage_complete  <code>abstractmethod</code>","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Called after successful stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Called after successful stage completion.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_stage_error","title":"on_stage_error  <code>abstractmethod</code>","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on stage failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Called on stage failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_pipeline_complete","title":"on_pipeline_complete  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Called after all stages complete.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Called after all stages complete.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_pipeline_error","title":"on_pipeline_error  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on fatal pipeline failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Called on fatal pipeline failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Called periodically during execution for progress updates.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Called periodically during execution for progress updates.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver","title":"ProgressBarObserver","text":"<pre><code>ProgressBarObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that displays progress bar with tqdm.</p> <p>Initialize progress bar observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize progress bar observer.\"\"\"\n    self.progress_bar: tqdm | None = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Initialize progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Initialize progress bar.\"\"\"\n    if context.total_rows &gt; 0:\n        self.progress_bar = tqdm(\n            total=context.total_rows,\n            desc=\"Processing\",\n            unit=\"rows\",\n        )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar description.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar description.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Stage: {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Update progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Update progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Handle error in progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Handle error in progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Error in {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Close progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Close progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close progress bar on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Close progress bar on error.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar with current row count.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar with current row count.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.set_postfix(\n            {\n                \"cost\": f\"${context.accumulated_cost:.4f}\",\n                \"progress\": f\"{context.get_progress():.1f}%\",\n            }\n        )\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver","title":"LoggingObserver","text":"<pre><code>LoggingObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that logs execution events.</p> <p>Initialize logging observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize logging observer.\"\"\"\n    self.logger = get_logger(__name__)\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log pipeline start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Log pipeline start.\"\"\"\n    self.logger.info(f\"Pipeline execution started (session: {context.session_id})\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Log stage start.\"\"\"\n    self.logger.info(\n        f\"Starting stage: {stage.name} (progress: {context.get_progress():.1f}%)\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Log stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Log stage completion.\"\"\"\n    self.logger.info(\n        f\"Completed stage: {stage.name} (cost: ${context.accumulated_cost:.4f})\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log stage error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Log stage error.\"\"\"\n    self.logger.error(f\"Stage {stage.name} failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log pipeline completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log pipeline completion.\"\"\"\n    self.logger.info(\n        f\"Pipeline execution completed successfully\\n\"\n        f\"  Processed: {result.metrics.processed_rows} rows\\n\"\n        f\"  Duration: {result.metrics.total_duration_seconds:.2f}s\\n\"\n        f\"  Total cost: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Errors: {result.metrics.failed_rows}\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log pipeline error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log pipeline error.\"\"\"\n    self.logger.error(f\"Pipeline execution failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Log progress update.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Log progress update.\"\"\"\n    # Make progress very visible with separators\n    self.logger.info(\n        f\"\u2501\u2501\u2501\u2501\u2501\u2501 PROGRESS: {context.last_processed_row}/{context.total_rows} rows \"\n        f\"({context.get_progress():.1f}%) | Cost: ${context.accumulated_cost:.4f} \u2501\u2501\u2501\u2501\u2501\u2501\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver","title":"CostTrackingObserver","text":"<pre><code>CostTrackingObserver(warning_threshold: float = 0.75)\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that tracks and warns about costs.</p> <p>Initialize cost tracking observer.</p> <p>Parameters:</p> Name Type Description Default <code>warning_threshold</code> <code>float</code> <p>Warn when this fraction of budget used</p> <code>0.75</code> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self, warning_threshold: float = 0.75):\n    \"\"\"\n    Initialize cost tracking observer.\n\n    Args:\n        warning_threshold: Warn when this fraction of budget used\n    \"\"\"\n    self.logger = get_logger(__name__)\n    self.warning_threshold = warning_threshold\n    self.max_budget: float | None = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Set max budget if available.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Set max budget if available.\"\"\"\n    # Could extract from pipeline specs\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on stage start.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Check cost after stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Check cost after stage completion.\"\"\"\n    if self.max_budget:\n        usage_ratio = float(context.accumulated_cost) / self.max_budget\n\n        if usage_ratio &gt;= self.warning_threshold:\n            self.logger.warning(\n                f\"Cost warning: {usage_ratio * 100:.1f}% of budget used \"\n                f\"(${context.accumulated_cost:.4f} / ${self.max_budget:.2f})\"\n            )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>No action on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"No action on error.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log final cost summary.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log final cost summary.\"\"\"\n    self.logger.info(\n        f\"Cost summary:\\n\"\n        f\"  Total: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Input tokens: {result.costs.input_tokens:,}\\n\"\n        f\"  Output tokens: {result.costs.output_tokens:,}\\n\"\n        f\"  Cost per row: ${float(result.costs.total_cost) / result.metrics.total_rows:.6f}\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log cost at failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log cost at failure.\"\"\"\n    self.logger.info(f\"Cost at failure: ${context.accumulated_cost:.4f}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on progress update for cost tracking.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on progress update for cost tracking.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/","title":"pipeline_executor","text":""},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor","title":"pipeline_executor","text":"<p>Pipeline executor for orchestrating stage execution.</p> <p>Implements the complete execution flow with state machine management as specified in the design document.</p>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.ExecutionState","title":"ExecutionState","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Pipeline execution states.</p>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor","title":"PipelineExecutor","text":"<pre><code>PipelineExecutor(stages: list[PipelineStage], state_manager: StateManager, observers: list[ExecutionObserver] | None = None)\n</code></pre> <p>Orchestrates pipeline execution with state management.</p> <p>Implements Command and Mediator patterns for coordinating stages, observers, and state management.</p> State Machine <p>IDLE \u2192 INITIALIZING \u2192 EXECUTING \u2192 [PAUSED \u2194 EXECUTING] \u2192 COMPLETED                          \u2193                       FAILED</p> <p>Initialize pipeline executor.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Ordered list of processing stages</p> required <code>state_manager</code> <code>StateManager</code> <p>State manager for checkpointing</p> required <code>observers</code> <code>list[ExecutionObserver] | None</code> <p>Optional execution observers</p> <code>None</code> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def __init__(\n    self,\n    stages: list[PipelineStage],\n    state_manager: StateManager,\n    observers: list[ExecutionObserver] | None = None,\n):\n    \"\"\"\n    Initialize pipeline executor.\n\n    Args:\n        stages: Ordered list of processing stages\n        state_manager: State manager for checkpointing\n        observers: Optional execution observers\n    \"\"\"\n    self.execution_id = uuid4()\n    self.stages = stages\n    self.state_manager = state_manager\n    self.observers = observers or []\n    self.state = ExecutionState.IDLE\n    self.context: ExecutionContext | None = None\n    self.logger = get_logger(f\"{__name__}.{self.execution_id}\")\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; PipelineExecutor\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>PipelineExecutor</code> <p>Self for chaining</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"PipelineExecutor\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.execute","title":"execute","text":"<pre><code>execute(pipeline: Any) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Any</code> <p>Pipeline instance to execute</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If pipeline in invalid state</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def execute(self, pipeline: Any) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Args:\n        pipeline: Pipeline instance to execute\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        RuntimeError: If pipeline in invalid state\n    \"\"\"\n    if self.state not in [ExecutionState.IDLE, ExecutionState.FAILED]:\n        raise RuntimeError(f\"Cannot execute from state: {self.state}\")\n\n    try:\n        # Initialize\n        self.state = ExecutionState.INITIALIZING\n        self.context = self._initialize_context()\n\n        # Check for existing checkpoint\n        if self.state_manager.can_resume(self.context.session_id):\n            self.logger.info(\"Found existing checkpoint, resuming...\")\n            self.context = self.state_manager.load_checkpoint(\n                self.context.session_id\n            )\n\n        # Notify observers\n        self._notify_pipeline_start(pipeline)\n\n        # Execute stages\n        self.state = ExecutionState.EXECUTING\n        result_data = self._execute_all_stages(pipeline)\n\n        # Mark completion\n        self.state = ExecutionState.COMPLETED\n        self.context.end_time = datetime.now()\n\n        # Create result\n        result = self._create_execution_result(result_data)\n\n        # Cleanup checkpoints\n        self.state_manager.cleanup_checkpoints(self.context.session_id)\n\n        # Notify observers\n        self._notify_pipeline_complete(result)\n\n        return result\n\n    except Exception as e:\n        self.state = ExecutionState.FAILED\n        self._notify_pipeline_error(e)\n\n        # Save checkpoint on failure\n        if self.context:\n            self.state_manager.save_checkpoint(self.context)\n\n        raise\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.pause","title":"pause","text":"<pre><code>pause() -&gt; None\n</code></pre> <p>Gracefully pause execution.</p> <p>Finishes current batch and saves checkpoint.</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def pause(self) -&gt; None:\n    \"\"\"\n    Gracefully pause execution.\n\n    Finishes current batch and saves checkpoint.\n    \"\"\"\n    if self.state != ExecutionState.EXECUTING:\n        raise RuntimeError(f\"Cannot pause from state: {self.state}\")\n\n    self.logger.info(\"Pausing execution...\")\n    self.state = ExecutionState.PAUSED\n\n    # Save checkpoint\n    if self.context:\n        self.state_manager.save_checkpoint(self.context)\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.resume","title":"resume","text":"<pre><code>resume(session_id: UUID) -&gt; ExecutionResult\n</code></pre> <p>Resume from saved checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session ID to resume</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no checkpoint found</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def resume(self, session_id: UUID) -&gt; ExecutionResult:\n    \"\"\"\n    Resume from saved checkpoint.\n\n    Args:\n        session_id: Session ID to resume\n\n    Returns:\n        ExecutionResult\n\n    Raises:\n        ValueError: If no checkpoint found\n    \"\"\"\n    if not self.state_manager.can_resume(session_id):\n        raise ValueError(f\"No checkpoint found for session {session_id}\")\n\n    self.context = self.state_manager.load_checkpoint(session_id)\n    if not self.context:\n        raise ValueError(\"Failed to load checkpoint\")\n\n    self.logger.info(f\"Resuming from row {self.context.last_processed_row}\")\n\n    # Continue execution\n    # Note: Would need to reconstruct pipeline and skip processed rows\n    raise NotImplementedError(\"Resume functionality coming soon\")\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.cancel","title":"cancel","text":"<pre><code>cancel() -&gt; None\n</code></pre> <p>Immediately stop and save checkpoint.</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def cancel(self) -&gt; None:\n    \"\"\"\n    Immediately stop and save checkpoint.\n    \"\"\"\n    self.logger.info(\"Cancelling execution...\")\n\n    # Save checkpoint\n    if self.context:\n        self.state_manager.save_checkpoint(self.context)\n\n    self.state = ExecutionState.IDLE\n</code></pre>"},{"location":"api/orchestration/progress_tracker/","title":"progress_tracker","text":""},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker","title":"progress_tracker","text":"<p>Progress tracking abstraction with pluggable implementations.</p> <p>Provides a generic interface for progress tracking that can be implemented using different libraries (rich, tqdm, logging) without coupling pipeline code to specific implementations.</p>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.ProgressTracker","title":"ProgressTracker","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for progress tracking.</p> <p>Enables pluggable progress tracking implementations (rich, tqdm, logging) without coupling pipeline code to specific libraries.</p> <p>Design Pattern: Strategy Pattern - Pipeline depends on ProgressTracker interface (abstraction) - Concrete implementations (RichProgressTracker, TqdmProgressTracker) are interchangeable - Follows Dependency Inversion Principle (SOLID)</p> Example <pre><code>tracker = create_progress_tracker(mode=\"auto\")\n\nwith tracker:\n    task_id = tracker.start_stage(\"Classification\", total_rows=1000)\n\n    for row in rows:\n        process(row)\n        tracker.update(task_id, advance=1, cost=0.001)\n\n    tracker.finish(task_id)\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.ProgressTracker.start_stage","title":"start_stage  <code>abstractmethod</code>","text":"<pre><code>start_stage(stage_name: str, total_rows: int, **metadata: Any) -&gt; str\n</code></pre> <p>Start tracking a new stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Human-readable stage name (e.g., \"Primary Category Classification\")</p> required <code>total_rows</code> <code>int</code> <p>Total number of rows to process</p> required <code>**metadata</code> <code>Any</code> <p>Additional metadata (cost_so_far, stage_number, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Task ID for updating progress</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>@abstractmethod\ndef start_stage(self, stage_name: str, total_rows: int, **metadata: Any) -&gt; str:\n    \"\"\"\n    Start tracking a new stage.\n\n    Args:\n        stage_name: Human-readable stage name (e.g., \"Primary Category Classification\")\n        total_rows: Total number of rows to process\n        **metadata: Additional metadata (cost_so_far, stage_number, etc.)\n\n    Returns:\n        Task ID for updating progress\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.ProgressTracker.update","title":"update  <code>abstractmethod</code>","text":"<pre><code>update(task_id: str, advance: int = 1, **metadata: Any) -&gt; None\n</code></pre> <p>Update progress for a task.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>Task identifier from start_stage()</p> required <code>advance</code> <code>int</code> <p>Number of rows processed</p> <code>1</code> <code>**metadata</code> <code>Any</code> <p>Additional metadata (cost, tokens, etc.)</p> <code>{}</code> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>@abstractmethod\ndef update(self, task_id: str, advance: int = 1, **metadata: Any) -&gt; None:\n    \"\"\"\n    Update progress for a task.\n\n    Args:\n        task_id: Task identifier from start_stage()\n        advance: Number of rows processed\n        **metadata: Additional metadata (cost, tokens, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.ProgressTracker.finish","title":"finish  <code>abstractmethod</code>","text":"<pre><code>finish(task_id: str) -&gt; None\n</code></pre> <p>Mark task as complete.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>Task identifier</p> required Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>@abstractmethod\ndef finish(self, task_id: str) -&gt; None:\n    \"\"\"\n    Mark task as complete.\n\n    Args:\n        task_id: Task identifier\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.RichProgressTracker","title":"RichProgressTracker","text":"<pre><code>RichProgressTracker()\n</code></pre> <p>               Bases: <code>ProgressTracker</code></p> <p>Progress tracker using rich.progress for beautiful terminal UI.</p> <p>Features: - Multiple progress bars (one per stage) - Automatic ETA and throughput calculation - Color-coded output - Cost tracking per stage - Thread-safe for concurrent execution</p> Requires <p>rich library (already a dependency)</p> Example <pre><code>tracker = RichProgressTracker()\n\nwith tracker:\n    task = tracker.start_stage(\"Stage 1: Classification\", total_rows=1000)\n\n    for i in range(1000):\n        process_row(i)\n        tracker.update(task, advance=1, cost=0.001)\n\n    tracker.finish(task)\n</code></pre> <p>Initialize rich progress tracker.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize rich progress tracker.\"\"\"\n    from rich.progress import (\n        BarColumn,\n        Progress,\n        SpinnerColumn,\n        TaskProgressColumn,\n        TextColumn,\n        TimeElapsedColumn,\n        TimeRemainingColumn,\n    )\n\n    self.progress = Progress(\n        SpinnerColumn(),\n        TextColumn(\"[bold blue]{task.description}\"),\n        BarColumn(),\n        TaskProgressColumn(),\n        TimeRemainingColumn(),\n        TimeElapsedColumn(),\n        TextColumn(\"[bold green]${task.fields[cost]:.4f}\"),\n        expand=True,\n    )\n    self.tasks: dict[str, Any] = {}\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.RichProgressTracker.start_stage","title":"start_stage","text":"<pre><code>start_stage(stage_name: str, total_rows: int, **metadata: Any) -&gt; str\n</code></pre> <p>Start tracking a stage with rich progress bar.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def start_stage(self, stage_name: str, total_rows: int, **metadata: Any) -&gt; str:\n    \"\"\"Start tracking a stage with rich progress bar.\"\"\"\n    task_id = self.progress.add_task(\n        f\"\ud83d\ude80 {stage_name}\",\n        total=total_rows,\n        cost=metadata.get(\"cost\", 0.0),\n    )\n    self.tasks[stage_name] = task_id\n    return stage_name  # Use stage_name as task_id for simplicity\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.RichProgressTracker.update","title":"update","text":"<pre><code>update(task_id: str, advance: int = 1, **metadata: Any) -&gt; None\n</code></pre> <p>Update progress bar.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def update(self, task_id: str, advance: int = 1, **metadata: Any) -&gt; None:\n    \"\"\"Update progress bar.\"\"\"\n    if task_id not in self.tasks:\n        return\n\n    rich_task_id = self.tasks[task_id]\n\n    # Update cost if provided\n    update_kwargs = {\"advance\": advance}\n    if \"cost\" in metadata:\n        # Get current cost and add new cost\n        task = self.progress.tasks[rich_task_id]\n        current_cost = task.fields.get(\"cost\", 0.0)\n        new_cost = float(current_cost) + float(metadata[\"cost\"])\n        update_kwargs[\"cost\"] = new_cost\n\n    self.progress.update(rich_task_id, **update_kwargs)\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.RichProgressTracker.finish","title":"finish","text":"<pre><code>finish(task_id: str) -&gt; None\n</code></pre> <p>Mark task as complete.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def finish(self, task_id: str) -&gt; None:\n    \"\"\"Mark task as complete.\"\"\"\n    if task_id in self.tasks:\n        rich_task_id = self.tasks[task_id]\n        self.progress.update(rich_task_id, completed=True)\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.LoggingProgressTracker","title":"LoggingProgressTracker","text":"<pre><code>LoggingProgressTracker()\n</code></pre> <p>               Bases: <code>ProgressTracker</code></p> <p>Fallback progress tracker using standard logging.</p> <p>Used when: - Not running in a TTY (CI, logs to file) - rich library not available - User explicitly requests logging mode</p> <p>Provides basic progress updates via log messages without fancy UI.</p> Example <pre><code>tracker = LoggingProgressTracker()\n\nwith tracker:\n    task = tracker.start_stage(\"Classification\", total_rows=1000)\n    # Logs: \"Starting Classification (1000 rows)\"\n\n    for i in range(1000):\n        tracker.update(task, advance=1)\n        # Logs periodically: \"Classification: 250/1000 (25%)\"\n\n    tracker.finish(task)\n    # Logs: \"Completed Classification\"\n</code></pre> <p>Initialize logging tracker.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize logging tracker.\"\"\"\n    from ondine.utils import get_logger\n\n    self.logger = get_logger(__name__)\n    self.tasks: dict[str, dict[str, Any]] = {}\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.LoggingProgressTracker.start_stage","title":"start_stage","text":"<pre><code>start_stage(stage_name: str, total_rows: int, **metadata: Any) -&gt; str\n</code></pre> <p>Start tracking via logging.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def start_stage(self, stage_name: str, total_rows: int, **metadata: Any) -&gt; str:\n    \"\"\"Start tracking via logging.\"\"\"\n    self.tasks[stage_name] = {\n        \"total\": total_rows,\n        \"current\": 0,\n        \"cost\": Decimal(\"0.0\"),\n        \"last_log_percent\": 0,\n    }\n    self.logger.info(f\"Starting {stage_name} ({total_rows} rows)\")\n    return stage_name\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.LoggingProgressTracker.update","title":"update","text":"<pre><code>update(task_id: str, advance: int = 1, **metadata: Any) -&gt; None\n</code></pre> <p>Update progress via periodic logging.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def update(self, task_id: str, advance: int = 1, **metadata: Any) -&gt; None:\n    \"\"\"Update progress via periodic logging.\"\"\"\n    if task_id not in self.tasks:\n        return\n\n    task = self.tasks[task_id]\n    task[\"current\"] += advance\n\n    if \"cost\" in metadata:\n        task[\"cost\"] += Decimal(str(metadata[\"cost\"]))\n\n    # Log at 25%, 50%, 75%, 100%\n    percent = (task[\"current\"] / task[\"total\"]) * 100\n    milestones = [25, 50, 75, 100]\n\n    for milestone in milestones:\n        if percent &gt;= milestone and task[\"last_log_percent\"] &lt; milestone:\n            self.logger.info(\n                f\"{task_id}: {task['current']}/{task['total']} \"\n                f\"({percent:.1f}%) | Cost: ${task['cost']:.4f}\"\n            )\n            task[\"last_log_percent\"] = milestone\n            break\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.LoggingProgressTracker.finish","title":"finish","text":"<pre><code>finish(task_id: str) -&gt; None\n</code></pre> <p>Log completion.</p> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def finish(self, task_id: str) -&gt; None:\n    \"\"\"Log completion.\"\"\"\n    if task_id in self.tasks:\n        task = self.tasks[task_id]\n        self.logger.info(\n            f\"Completed {task_id}: {task['current']}/{task['total']} rows, \"\n            f\"${task['cost']:.4f}\"\n        )\n</code></pre>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.NoOpProgressTracker","title":"NoOpProgressTracker","text":"<p>               Bases: <code>ProgressTracker</code></p> <p>No-op tracker that does nothing (for disabling progress).</p>"},{"location":"api/orchestration/progress_tracker/#ondine.orchestration.progress_tracker.create_progress_tracker","title":"create_progress_tracker","text":"<pre><code>create_progress_tracker(mode: str = 'auto') -&gt; ProgressTracker\n</code></pre> <p>Factory function to create appropriate progress tracker.</p> <p>Automatically detects the best progress tracker based on environment and available libraries, or uses explicit mode if specified.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Progress tracker mode - \"auto\": Auto-detect (rich if TTY, else logging) - \"rich\": Use rich.progress (beautiful UI) - \"tqdm\": Use tqdm (simple, compatible) - \"logging\": Use standard logging (fallback) - \"none\": Disable progress tracking</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ProgressTracker</code> <p>ProgressTracker implementation</p> Example <pre><code># Auto-detect best option\ntracker = create_progress_tracker(mode=\"auto\")\n\n# Force rich (will fail if not available)\ntracker = create_progress_tracker(mode=\"rich\")\n\n# Force logging (always works)\ntracker = create_progress_tracker(mode=\"logging\")\n</code></pre> Source code in <code>ondine/orchestration/progress_tracker.py</code> <pre><code>def create_progress_tracker(mode: str = \"auto\") -&gt; ProgressTracker:\n    \"\"\"\n    Factory function to create appropriate progress tracker.\n\n    Automatically detects the best progress tracker based on environment\n    and available libraries, or uses explicit mode if specified.\n\n    Args:\n        mode: Progress tracker mode\n            - \"auto\": Auto-detect (rich if TTY, else logging)\n            - \"rich\": Use rich.progress (beautiful UI)\n            - \"tqdm\": Use tqdm (simple, compatible)\n            - \"logging\": Use standard logging (fallback)\n            - \"none\": Disable progress tracking\n\n    Returns:\n        ProgressTracker implementation\n\n    Example:\n        ```python\n        # Auto-detect best option\n        tracker = create_progress_tracker(mode=\"auto\")\n\n        # Force rich (will fail if not available)\n        tracker = create_progress_tracker(mode=\"rich\")\n\n        # Force logging (always works)\n        tracker = create_progress_tracker(mode=\"logging\")\n        ```\n    \"\"\"\n    if mode == \"none\":\n        return NoOpProgressTracker()\n\n    if mode == \"auto\":\n        # Auto-detect best option\n        if sys.stdout.isatty():\n            # Running in terminal - try rich first\n            try:\n                from rich.progress import Progress  # noqa: F401\n\n                return RichProgressTracker()\n            except ImportError:\n                # rich not available, fall back to logging\n                return LoggingProgressTracker()\n        else:\n            # Non-TTY environment (CI, logs to file) - use logging\n            return LoggingProgressTracker()\n\n    elif mode == \"rich\":\n        return RichProgressTracker()\n\n    elif mode == \"tqdm\":\n        # Future: implement TqdmProgressTracker\n        raise NotImplementedError(\n            \"tqdm tracker not yet implemented, use 'rich' or 'logging'\"\n        )\n\n    elif mode == \"logging\":\n        return LoggingProgressTracker()\n\n    else:\n        raise ValueError(\n            f\"Invalid progress mode: {mode}. \"\n            f\"Use 'auto', 'rich', 'tqdm', 'logging', or 'none'\"\n        )\n</code></pre>"},{"location":"api/orchestration/state_manager/","title":"state_manager","text":""},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager","title":"state_manager","text":"<p>State management for checkpointing and recovery.</p>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager","title":"StateManager","text":"<pre><code>StateManager(storage: CheckpointStorage, checkpoint_interval: int = 500)\n</code></pre> <p>Manages execution state persistence and recovery.</p> <p>Follows Single Responsibility: only handles state management. Uses Strategy pattern for pluggable storage backends.</p> <p>Initialize state manager.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>CheckpointStorage</code> <p>Checkpoint storage backend</p> required <code>checkpoint_interval</code> <code>int</code> <p>Rows between checkpoints</p> <code>500</code> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def __init__(self, storage: CheckpointStorage, checkpoint_interval: int = 500):\n    \"\"\"\n    Initialize state manager.\n\n    Args:\n        storage: Checkpoint storage backend\n        checkpoint_interval: Rows between checkpoints\n    \"\"\"\n    self.storage = storage\n    self.checkpoint_interval = checkpoint_interval\n    self._last_checkpoint_row = 0\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.should_checkpoint","title":"should_checkpoint","text":"<pre><code>should_checkpoint(current_row: int) -&gt; bool\n</code></pre> <p>Check if checkpoint should be saved.</p> <p>Parameters:</p> Name Type Description Default <code>current_row</code> <code>int</code> <p>Current row index</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint due</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def should_checkpoint(self, current_row: int) -&gt; bool:\n    \"\"\"\n    Check if checkpoint should be saved.\n\n    Args:\n        current_row: Current row index\n\n    Returns:\n        True if checkpoint due\n    \"\"\"\n    return (current_row - self._last_checkpoint_row) &gt;= self.checkpoint_interval\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(context: ExecutionContext) -&gt; bool\n</code></pre> <p>Save checkpoint for execution context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Execution context to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def save_checkpoint(self, context: ExecutionContext) -&gt; bool:\n    \"\"\"\n    Save checkpoint for execution context.\n\n    Args:\n        context: Execution context to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        checkpoint_data = context.to_checkpoint()\n        success = self.storage.save(context.session_id, checkpoint_data)\n\n        if success:\n            self._last_checkpoint_row = context.last_processed_row\n            logger.info(f\"Checkpoint saved at row {context.last_processed_row}\")\n\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to save checkpoint: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(session_id: UUID) -&gt; ExecutionContext | None\n</code></pre> <p>Load checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>ExecutionContext | None</code> <p>Restored execution context or None</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def load_checkpoint(self, session_id: UUID) -&gt; ExecutionContext | None:\n    \"\"\"\n    Load checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Restored execution context or None\n    \"\"\"\n    try:\n        checkpoint_data = self.storage.load(session_id)\n\n        if checkpoint_data is None:\n            return None\n\n        context = ExecutionContext.from_checkpoint(checkpoint_data)\n        logger.info(f\"Checkpoint loaded from row {context.last_processed_row}\")\n\n        return context\n    except Exception as e:\n        logger.error(f\"Failed to load checkpoint: {e}\")\n        return None\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.can_resume","title":"can_resume","text":"<pre><code>can_resume(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if session can be resumed.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint exists</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def can_resume(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if session can be resumed.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if checkpoint exists\n    \"\"\"\n    return self.storage.exists(session_id)\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.cleanup_checkpoints","title":"cleanup_checkpoints","text":"<pre><code>cleanup_checkpoints(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoints for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def cleanup_checkpoints(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoints for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    try:\n        success = self.storage.delete(session_id)\n        if success:\n            logger.info(f\"Checkpoints cleaned up for session {session_id}\")\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to cleanup checkpoints: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    return self.storage.list_checkpoints()\n</code></pre>"},{"location":"api/orchestration/streaming_executor/","title":"streaming_executor","text":""},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor","title":"streaming_executor","text":"<p>Streaming execution strategy.</p> <p>Provides memory-efficient processing for large datasets by processing data in chunks.</p>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor","title":"StreamingExecutor","text":"<pre><code>StreamingExecutor(chunk_size: int = 1000)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Streaming execution strategy.</p> <p>Processes data in chunks to maintain constant memory usage. Ideal for very large datasets (100K+ rows) that don't fit in memory.</p> <p>Benefits: - Constant memory footprint - Can process unlimited dataset sizes - Checkpoints at chunk boundaries - Early results available</p> <p>Initialize streaming executor.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self, chunk_size: int = 1000):\n    \"\"\"\n    Initialize streaming executor.\n\n    Args:\n        chunk_size: Number of rows per chunk\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Execute stages in streaming mode.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrames with processed chunks</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Execute stages in streaming mode.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Yields:\n        DataFrames with processed chunks\n    \"\"\"\n    self.logger.info(f\"Starting streaming execution (chunk_size={self.chunk_size})\")\n\n    # Get data loader stage\n    data_loader = stages[0]\n\n    # Stream data in chunks\n    chunk_index = 0\n    total_rows_processed = 0\n\n    # Read data in chunks\n    for chunk in self._read_chunks(data_loader, context):\n        self.logger.info(f\"Processing chunk {chunk_index} ({len(chunk)} rows)\")\n\n        # Process chunk through remaining stages\n        result_chunk = self._process_chunk(chunk, stages[1:], context)\n\n        # Update context\n        total_rows_processed += len(result_chunk)\n        context.update_row(total_rows_processed - 1)\n\n        # Yield result\n        yield result_chunk\n\n        chunk_index += 1\n\n    self.logger.info(\n        f\"Streaming execution complete: {total_rows_processed} rows, \"\n        f\"{chunk_index} chunks\"\n    )\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Streaming executor doesn't support async.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Streaming executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Streaming executor supports streaming.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Streaming executor supports streaming.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute_stream(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Alias for execute() method.\"\"\"\n    return self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingResult","title":"StreamingResult","text":"<pre><code>StreamingResult()\n</code></pre> <p>Result container for streaming execution.</p> <p>Provides access to metrics after consuming the stream.</p> <p>Initialize streaming result.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize streaming result.\"\"\"\n    self.chunks_processed = 0\n    self.total_rows = 0\n    self.total_cost = Decimal(\"0.0\")\n    self.start_time = datetime.now()\n    self.end_time = None\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingResult.add_chunk","title":"add_chunk","text":"<pre><code>add_chunk(chunk: DataFrame, cost: Decimal)\n</code></pre> <p>Add chunk statistics.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def add_chunk(self, chunk: pd.DataFrame, cost: Decimal):\n    \"\"\"Add chunk statistics.\"\"\"\n    self.chunks_processed += 1\n    self.total_rows += len(chunk)\n    self.total_cost += cost\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingResult.finalize","title":"finalize","text":"<pre><code>finalize() -&gt; ExecutionResult\n</code></pre> <p>Create final ExecutionResult.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def finalize(self) -&gt; ExecutionResult:\n    \"\"\"Create final ExecutionResult.\"\"\"\n    self.end_time = datetime.now()\n    duration = (self.end_time - self.start_time).total_seconds()\n\n    stats = ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=self.total_rows,\n        failed_rows=0,\n        skipped_rows=0,\n        rows_per_second=self.total_rows / duration if duration &gt; 0 else 0,\n        total_duration_seconds=duration,\n    )\n\n    costs = CostEstimate(\n        total_cost=self.total_cost,\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=self.total_rows,\n        confidence=\"actual\",\n    )\n\n    return ExecutionResult(\n        data=pd.DataFrame(),  # Streaming doesn't return full data\n        metrics=stats,\n        costs=costs,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        success=True,\n    )\n</code></pre>"},{"location":"api/orchestration/sync_executor/","title":"sync_executor","text":""},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor","title":"sync_executor","text":"<p>Synchronous execution strategy.</p> <p>Default executor that maintains current behavior using ThreadPoolExecutor for concurrent LLM calls.</p>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor","title":"SyncExecutor","text":"<pre><code>SyncExecutor()\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Synchronous execution strategy.</p> <p>Uses ThreadPoolExecutor for concurrent LLM calls while maintaining sequential stage execution. This is the default strategy that preserves current behavior.</p> <p>Initialize synchronous executor.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize synchronous executor.\"\"\"\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages synchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages sequentially\n        result_data = self._execute_stages(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate (leverage LlamaIndex token counts from intermediate_data)\n        token_tracking = context.intermediate_data.get(\"token_tracking\", {})\n        input_tokens = token_tracking.get(\"input_tokens\", 0)\n        output_tokens = token_tracking.get(\"output_tokens\", 0)\n\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Sync executor doesn't support async.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Sync executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/stages/","title":"stages","text":""},{"location":"api/stages/#ondine.stages","title":"stages","text":"<p>Processing stages for data transformation.</p>"},{"location":"api/stages/#ondine.stages.BatchAggregatorStage","title":"BatchAggregatorStage","text":"<pre><code>BatchAggregatorStage(batch_size: int, strategy: BatchFormattingStrategy | None = None, model: str | None = None, validate_context_window: bool = False, name: str = 'BatchAggregator')\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Aggregate multiple prompts into batch prompts.</p> <p>Responsibility: - Group N prompts into chunks of batch_size - Use strategy to format each chunk as a single batch prompt - Preserve metadata for disaggregation</p> <p>Design Pattern: Strategy Pattern - Delegates formatting logic to BatchFormattingStrategy - Supports multiple formats (JSON, CSV) via strategy injection</p> <p>Initialize batch aggregator stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of prompts to aggregate per batch</p> required <code>strategy</code> <code>BatchFormattingStrategy | None</code> <p>Formatting strategy (defaults to JsonBatchStrategy)</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Model name for context window validation (optional)</p> <code>None</code> <code>validate_context_window</code> <code>bool</code> <p>Whether to validate against context limits</p> <code>False</code> <code>name</code> <code>str</code> <p>Stage name for logging</p> <code>'BatchAggregator'</code> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    strategy: BatchFormattingStrategy | None = None,\n    model: str | None = None,\n    validate_context_window: bool = False,  # Disabled by default (slow for large datasets)\n    name: str = \"BatchAggregator\",\n):\n    \"\"\"Initialize batch aggregator stage.\n\n    Args:\n        batch_size: Number of prompts to aggregate per batch\n        strategy: Formatting strategy (defaults to JsonBatchStrategy)\n        model: Model name for context window validation (optional)\n        validate_context_window: Whether to validate against context limits\n        name: Stage name for logging\n    \"\"\"\n    super().__init__(name=name)\n    self.batch_size = batch_size\n    self.strategy = strategy or JsonBatchStrategy()\n    self.model = model\n    self.validate_context_window = validate_context_window\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchAggregatorStage.process","title":"process","text":"<pre><code>process(batches: list[PromptBatch], context: Any) -&gt; list[PromptBatch]\n</code></pre> <p>Aggregate prompts into batch prompts.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>list[PromptBatch]</code> <p>List of prompt batches (from PromptFormatterStage)</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>list[PromptBatch]</code> <p>List of aggregated prompt batches (1 prompt per batch_size rows)</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def process(self, batches: list[PromptBatch], context: Any) -&gt; list[PromptBatch]:\n    \"\"\"Aggregate prompts into batch prompts.\n\n    Args:\n        batches: List of prompt batches (from PromptFormatterStage)\n        context: Execution context\n\n    Returns:\n        List of aggregated prompt batches (1 prompt per batch_size rows)\n    \"\"\"\n    import time\n\n    aggregated_batches = []\n\n    # Calculate total prompts for progress tracking\n    total_prompts = sum(len(b.prompts) for b in batches)\n    if total_prompts == 0:\n        self.logger.info(\"No prompts to aggregate\")\n        return aggregated_batches\n\n    processed_prompts = 0\n    start_time = time.time()\n    last_log_time = start_time\n    last_log_pct = 0\n\n    expected_batches = (total_prompts + self.batch_size - 1) // self.batch_size\n    self.logger.info(\n        f\"Creating {expected_batches:,} mega-prompts ({self.batch_size} rows each)\"\n    )\n\n    # Process each batch\n    for batch_idx, batch in enumerate(batches):\n        # Group prompts into chunks of batch_size\n        for i in range(0, len(batch.prompts), self.batch_size):\n            chunk_prompts = batch.prompts[i : i + self.batch_size]\n            chunk_metadata = batch.metadata[i : i + self.batch_size]\n\n            # Extract row IDs from metadata\n            row_ids = [m.row_index for m in chunk_metadata]\n\n            # Create metadata for disaggregation\n            metadata = BatchMetadata(\n                original_count=len(chunk_prompts),\n                row_ids=row_ids,\n                prompt_template=None,  # Not available in current structure\n            )\n\n            # Validate batch size against context window (only once, not for every batch)\n            if (\n                self.validate_context_window\n                and self.model\n                and len(aggregated_batches) == 0  # Only validate first batch\n            ):\n                # Skip validation if chunk is empty\n                if not chunk_prompts:\n                    self.logger.warning(\n                        \"Empty chunk encountered, skipping validation\"\n                    )\n                    continue\n\n                # Estimate tokens for this batch\n                import tiktoken\n\n                tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n                avg_tokens = sum(\n                    len(tokenizer.encode(p)) for p in chunk_prompts\n                ) // len(chunk_prompts)\n\n                is_valid, error_msg = validate_batch_size(\n                    self.model, len(chunk_prompts), avg_tokens\n                )\n\n                if not is_valid:\n                    self.logger.warning(\n                        f\"Batch size validation failed: {error_msg}. \"\n                        f\"Consider reducing batch_size.\"\n                    )\n                else:\n                    self.logger.info(\n                        f\"Batch size validation passed: {len(chunk_prompts)} rows, \"\n                        f\"~{avg_tokens} tokens/row\"\n                    )\n\n            # Use strategy to format batch prompt\n            batch_prompt_text = self.strategy.format_batch(\n                chunk_prompts, metadata=metadata.model_dump()\n            )\n\n            # Create new PromptBatch with single mega-prompt\n            # Preserve ALL custom fields from original metadata (especially system_message for caching!)\n            original_custom = chunk_metadata[0].custom or {}\n\n            mega_metadata = RowMetadata(\n                row_index=chunk_metadata[0].row_index,\n                row_id=chunk_metadata[0].row_id,\n                custom={\n                    **original_custom,  # Preserve all custom fields (system_message, etc.)\n                    \"batch_metadata\": metadata.model_dump(),  # Batch-specific fields override\n                    \"is_batch\": True,\n                    \"batch_size\": len(chunk_prompts),\n                },\n            )\n\n            aggregated_batch = PromptBatch(\n                prompts=[batch_prompt_text],\n                metadata=[mega_metadata],\n                batch_id=batch.batch_id,\n            )\n\n            aggregated_batches.append(aggregated_batch)\n\n            # Update progress\n            processed_prompts += len(chunk_prompts)\n\n            # Hybrid progress: Log every 10% OR every 30 seconds (only for slow operations)\n            current_time = time.time()\n            elapsed = current_time - start_time\n\n            if total_prompts &gt; 10000 and elapsed &gt; 5:\n                current_pct = int((processed_prompts / total_prompts) * 100)\n\n                should_log = (\n                    current_pct &gt;= last_log_pct + 10 and current_pct &lt;= 90\n                ) or (current_time - last_log_time &gt;= 30)\n\n                if should_log:\n                    elapsed = current_time - start_time\n                    throughput = processed_prompts / elapsed if elapsed &gt; 0 else 0\n                    eta = (\n                        (total_prompts - processed_prompts) / throughput\n                        if throughput &gt; 0\n                        else 0\n                    )\n\n                    self.logger.info(\n                        f\"Aggregating: {current_pct}% ({processed_prompts:,}/{total_prompts:,}) | \"\n                        f\"ETA: {eta:.0f}s\"\n                    )\n                    last_log_time = current_time\n                    last_log_pct = current_pct\n\n    # Final summary\n    elapsed = time.time() - start_time\n    throughput = (\n        len(aggregated_batches) / elapsed\n        if elapsed &gt; 0\n        else len(aggregated_batches)\n    )\n    self.logger.info(\n        f\"\u2713 Created {len(aggregated_batches):,} mega-prompts in {elapsed:.1f}s \"\n        f\"({throughput:.0f} batches/s)\"\n    )\n\n    return aggregated_batches\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchAggregatorStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: list[PromptBatch]) -&gt; Any\n</code></pre> <p>Validate input batches.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[PromptBatch]</code> <p>List of PromptBatch objects</p> required <p>Returns:</p> Type Description <code>Any</code> <p>ValidationResult</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def validate_input(self, input_data: list[PromptBatch]) -&gt; Any:\n    \"\"\"Validate input batches.\n\n    Args:\n        input_data: List of PromptBatch objects\n\n    Returns:\n        ValidationResult\n    \"\"\"\n    from ondine.core.models import ValidationResult\n\n    if not input_data:\n        return ValidationResult(is_valid=False, errors=[\"No input batches\"])\n\n    return ValidationResult(is_valid=True)\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchAggregatorStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: list[PromptBatch], context: Any) -&gt; Any\n</code></pre> <p>Estimate cost for batch aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[PromptBatch]</code> <p>List of PromptBatch objects</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Any</code> <p>CostEstimate (zero cost - aggregation is free)</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def estimate_cost(self, input_data: list[PromptBatch], context: Any) -&gt; Any:\n    \"\"\"Estimate cost for batch aggregation.\n\n    Args:\n        input_data: List of PromptBatch objects\n        context: Execution context\n\n    Returns:\n        CostEstimate (zero cost - aggregation is free)\n    \"\"\"\n    from ondine.core.models import CostEstimate\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.prompts) for b in input_data),\n        confidence=\"actual\",\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchAggregatorStage.validate","title":"validate","text":"<pre><code>validate(context: Any) -&gt; bool\n</code></pre> <p>Validate stage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def validate(self, context: Any) -&gt; bool:\n    \"\"\"Validate stage configuration.\n\n    Args:\n        context: Execution context\n\n    Returns:\n        True if valid\n\n    Raises:\n        ValueError: If configuration is invalid\n    \"\"\"\n    if self.batch_size &lt; 1:\n        raise ValueError(f\"batch_size must be &gt;= 1, got {self.batch_size}\")\n\n    if self.strategy is None:\n        raise ValueError(\"strategy cannot be None\")\n\n    return True\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchDisaggregatorStage","title":"BatchDisaggregatorStage","text":"<pre><code>BatchDisaggregatorStage(strategy: BatchFormattingStrategy | None = None, retry_failed_individually: bool = True, name: str = 'BatchDisaggregator')\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Disaggregate batch responses into individual responses.</p> <p>Responsibility: - Parse batch response using strategy - Split into individual Response objects - Handle partial failures (retry failed rows) - Preserve row IDs and metadata</p> <p>Design Pattern: Strategy Pattern + Fallback - Delegates parsing to BatchFormattingStrategy - Implements retry logic for partial failures</p> <p>Initialize batch disaggregator stage.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>BatchFormattingStrategy | None</code> <p>Parsing strategy (defaults to JsonBatchStrategy)</p> <code>None</code> <code>retry_failed_individually</code> <code>bool</code> <p>Retry failed rows one-by-one</p> <code>True</code> <code>name</code> <code>str</code> <p>Stage name for logging</p> <code>'BatchDisaggregator'</code> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def __init__(\n    self,\n    strategy: BatchFormattingStrategy | None = None,\n    retry_failed_individually: bool = True,\n    name: str = \"BatchDisaggregator\",\n):\n    \"\"\"Initialize batch disaggregator stage.\n\n    Args:\n        strategy: Parsing strategy (defaults to JsonBatchStrategy)\n        retry_failed_individually: Retry failed rows one-by-one\n        name: Stage name for logging\n    \"\"\"\n    super().__init__(name=name)\n    self.strategy = strategy or JsonBatchStrategy()\n    self.retry_failed_individually = retry_failed_individually\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchDisaggregatorStage.process","title":"process","text":"<pre><code>process(batches: list[ResponseBatch], context: Any) -&gt; list[ResponseBatch]\n</code></pre> <p>Disaggregate batch responses into individual responses.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>list[ResponseBatch]</code> <p>List of response batches (from LLMInvocationStage)</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>list[ResponseBatch]</code> <p>List of disaggregated response batches (N responses per batch)</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def process(\n    self, batches: list[ResponseBatch], context: Any\n) -&gt; list[ResponseBatch]:\n    \"\"\"Disaggregate batch responses into individual responses.\n\n    Args:\n        batches: List of response batches (from LLMInvocationStage)\n        context: Execution context\n\n    Returns:\n        List of disaggregated response batches (N responses per batch)\n    \"\"\"\n    disaggregated_batches = []\n    total_retries = 0\n\n    # Process each batch\n    for batch_idx, batch in enumerate(batches):\n        # Check if this batch contains aggregated responses\n        # In existing structure: batch.responses is list[str], batch.metadata is list[RowMetadata]\n        if (\n            not batch.metadata\n            or not batch.metadata[0].custom\n            or not batch.metadata[0].custom.get(\"is_batch\")\n        ):\n            # Not a batch response, pass through unchanged\n            disaggregated_batches.append(batch)\n            continue\n\n        # Extract batch metadata from first metadata entry\n        batch_metadata_dict = batch.metadata[0].custom.get(\"batch_metadata\", {})\n        batch_metadata = BatchMetadata(**batch_metadata_dict)\n\n        # Get the batch response text (first and only response in aggregated batch)\n        response_text = batch.responses[0]\n\n        # Get cost and latency from batch (will be split evenly)\n        batch_cost = batch.cost\n        batch_tokens = batch.tokens_used\n        batch_latency = batch.latencies_ms[0] if batch.latencies_ms else 0.0\n\n        # Parse batch response\n        try:\n            individual_results = self.strategy.parse_batch_response(\n                response_text,\n                expected_count=batch_metadata.original_count,\n                metadata=batch_metadata_dict,\n            )\n\n            # Create disaggregated batch with individual responses\n            disaggregated_batch = ResponseBatch(\n                responses=individual_results,\n                metadata=[\n                    RowMetadata(\n                        row_index=row_id,\n                        row_id=row_id,\n                        custom={\"from_batch\": True},\n                    )\n                    for row_id in batch_metadata.row_ids\n                ],\n                tokens_used=batch_tokens,\n                cost=batch_cost,\n                batch_id=batch.batch_id,\n                latencies_ms=[batch_latency / len(individual_results)]\n                * len(individual_results),\n            )\n            disaggregated_batches.append(disaggregated_batch)\n\n        except PartialParseError as e:\n            # Partial success - some results parsed, some failed\n            self.logger.warning(\n                f\"Partial parse: {len(e.parsed_results)}/{batch_metadata.original_count} \"\n                f\"results. Failed IDs: {e.failed_ids}\"\n            )\n            total_retries += len(e.failed_ids)\n\n            # Create responses with error markers for failed rows\n            individual_results = []\n            for i, row_id in enumerate(batch_metadata.row_ids):\n                if i + 1 in e.failed_ids:  # failed_ids are 1-based\n                    individual_results.append(\n                        f\"[PARSE_ERROR: Row {i + 1} not found in batch response]\"\n                    )\n                else:\n                    # Find the corresponding parsed result\n                    result_idx = i - sum(1 for fid in e.failed_ids if fid &lt;= i + 1)\n                    individual_results.append(e.parsed_results[result_idx])\n\n            disaggregated_batch = ResponseBatch(\n                responses=individual_results,\n                metadata=[\n                    RowMetadata(\n                        row_index=row_id,\n                        row_id=row_id,\n                        custom={\n                            \"from_batch\": True,\n                            \"parse_error\": i + 1 in e.failed_ids,\n                        },\n                    )\n                    for i, row_id in enumerate(batch_metadata.row_ids)\n                ],\n                tokens_used=batch_tokens,\n                cost=batch_cost,\n                batch_id=batch.batch_id,\n                latencies_ms=[batch_latency / len(individual_results)]\n                * len(individual_results),\n            )\n            disaggregated_batches.append(disaggregated_batch)\n\n        except Exception as e:\n            # Complete failure - couldn't parse batch at all\n            self.logger.error(\n                f\"Failed to parse batch response: {e}. \"\n                f\"Response: {response_text[:200]}\"\n            )\n\n            # Create error responses for all rows\n            error_responses = [\n                f\"[BATCH_PARSE_ERROR: {str(e)}]\" for _ in batch_metadata.row_ids\n            ]\n\n            disaggregated_batch = ResponseBatch(\n                responses=error_responses,\n                metadata=[\n                    RowMetadata(\n                        row_index=row_id,\n                        row_id=row_id,\n                        custom={\"parse_error\": True, \"batch_parse_failed\": True},\n                    )\n                    for row_id in batch_metadata.row_ids\n                ],\n                tokens_used=batch_tokens,\n                cost=batch_cost,\n                batch_id=batch.batch_id,\n                latencies_ms=[batch_latency / len(batch_metadata.row_ids)]\n                * len(batch_metadata.row_ids),\n            )\n            disaggregated_batches.append(disaggregated_batch)\n\n    if total_retries &gt; 0:\n        self.logger.info(f\"Total rows retried individually: {total_retries}\")\n\n    return disaggregated_batches\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchDisaggregatorStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: list[ResponseBatch]) -&gt; Any\n</code></pre> <p>Validate input batches.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[ResponseBatch]</code> <p>List of ResponseBatch objects</p> required <p>Returns:</p> Type Description <code>Any</code> <p>ValidationResult</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def validate_input(self, input_data: list[ResponseBatch]) -&gt; Any:\n    \"\"\"Validate input batches.\n\n    Args:\n        input_data: List of ResponseBatch objects\n\n    Returns:\n        ValidationResult\n    \"\"\"\n    from ondine.core.models import ValidationResult\n\n    if not input_data:\n        return ValidationResult(is_valid=False, errors=[\"No input batches\"])\n\n    return ValidationResult(is_valid=True)\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchDisaggregatorStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: list[ResponseBatch], context: Any) -&gt; Any\n</code></pre> <p>Estimate cost for batch disaggregation.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[ResponseBatch]</code> <p>List of ResponseBatch objects</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Any</code> <p>CostEstimate (zero cost - disaggregation is free)</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def estimate_cost(self, input_data: list[ResponseBatch], context: Any) -&gt; Any:\n    \"\"\"Estimate cost for batch disaggregation.\n\n    Args:\n        input_data: List of ResponseBatch objects\n        context: Execution context\n\n    Returns:\n        CostEstimate (zero cost - disaggregation is free)\n    \"\"\"\n    from decimal import Decimal\n\n    from ondine.core.models import CostEstimate\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.responses) for b in input_data),\n        confidence=\"actual\",\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.BatchDisaggregatorStage.validate","title":"validate","text":"<pre><code>validate(context: Any) -&gt; bool\n</code></pre> <p>Validate stage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def validate(self, context: Any) -&gt; bool:\n    \"\"\"Validate stage configuration.\n\n    Args:\n        context: Execution context\n\n    Returns:\n        True if valid\n\n    Raises:\n        ValueError: If configuration is invalid\n    \"\"\"\n    if self.strategy is None:\n        raise ValueError(\"strategy cannot be None\")\n\n    return True\n</code></pre>"},{"location":"api/stages/#ondine.stages.DataLoaderStage","title":"DataLoaderStage","text":"<pre><code>DataLoaderStage(dataframe: DataFrame | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[DatasetSpec, DataFrame]</code></p> <p>Load data from source and validate schema.</p> <p>Responsibilities: - Read data from configured source - Validate required columns exist - Apply any filters - Update context with row count</p> <p>Initialize data loader stage.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded dataframe (for DataFrame source)</p> <code>None</code> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"\n    Initialize data loader stage.\n\n    Args:\n        dataframe: Optional pre-loaded dataframe (for DataFrame source)\n    \"\"\"\n    super().__init__(\"DataLoader\")\n    self.dataframe = dataframe\n</code></pre>"},{"location":"api/stages/#ondine.stages.DataLoaderStage.process","title":"process","text":"<pre><code>process(spec: DatasetSpec, context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Load data from source.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def process(self, spec: DatasetSpec, context: Any) -&gt; pd.DataFrame:\n    \"\"\"Load data from source.\"\"\"\n    # Create appropriate reader\n    reader = create_data_reader(\n        source_type=spec.source_type,\n        source_path=spec.source_path,\n        dataframe=self.dataframe,\n        delimiter=spec.delimiter,\n        encoding=spec.encoding,\n        sheet_name=spec.sheet_name,\n    )\n\n    # Read data\n    df = reader.read()\n\n    # Validate columns exist\n    missing_cols = set(spec.input_columns) - set(df.columns)\n    if missing_cols:\n        raise ValueError(f\"Missing columns: {missing_cols}\")\n\n    # Apply filters if specified\n    if spec.filters:\n        for column, value in spec.filters.items():\n            if column in df.columns:\n                df = df[df[column] == value]\n\n    # Update context with total rows\n    context.total_rows = len(df)\n\n    self.logger.info(f\"Loaded {len(df)} rows from {spec.source_type}\")\n\n    return df\n</code></pre>"},{"location":"api/stages/#ondine.stages.DataLoaderStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(spec: DatasetSpec) -&gt; ValidationResult\n</code></pre> <p>Validate dataset specification.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def validate_input(self, spec: DatasetSpec) -&gt; ValidationResult:\n    \"\"\"Validate dataset specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Check file exists for file sources\n    if spec.source_path and not spec.source_path.exists():\n        result.add_error(f\"Source file not found: {spec.source_path}\")\n\n    # Check input columns specified\n    if not spec.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    # Check output columns specified\n    if not spec.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.DataLoaderStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(spec: DatasetSpec) -&gt; CostEstimate\n</code></pre> <p>Data loading has no LLM cost.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def estimate_cost(self, spec: DatasetSpec) -&gt; CostEstimate:\n    \"\"\"Data loading has no LLM cost.\"\"\"\n    # Try to determine row count if dataframe is available\n    row_count = 0\n    if self.dataframe is not None:\n        row_count = len(self.dataframe)\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=row_count,\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage","title":"LLMInvocationStage","text":"<pre><code>LLMInvocationStage(llm_client: LLMClient, concurrency: int = 5, rate_limiter: RateLimiter | None = None, retry_handler: RetryHandler | None = None, error_policy: ErrorPolicy = ErrorPolicy.SKIP, max_retries: int = 3, output_cls: type[BaseModel] | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[list[PromptBatch], list[ResponseBatch]]</code></p> <p>Invoke LLM with prompts using concurrency and retries.</p> <p>Responsibilities: - Execute LLM calls with rate limiting - Handle retries for transient failures - Track tokens and costs - Support concurrent processing</p> <p>Initialize LLM invocation stage.</p> <p>Parameters:</p> Name Type Description Default <code>llm_client</code> <code>LLMClient</code> <p>LLM client instance</p> required <code>concurrency</code> <code>int</code> <p>Max concurrent requests</p> <code>5</code> <code>rate_limiter</code> <code>RateLimiter | None</code> <p>Optional rate limiter</p> <code>None</code> <code>retry_handler</code> <code>RetryHandler | None</code> <p>Optional retry handler</p> <code>None</code> <code>error_policy</code> <code>ErrorPolicy</code> <p>Policy for handling errors</p> <code>SKIP</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>output_cls</code> <code>type[BaseModel] | None</code> <p>Optional Pydantic model for structured output</p> <code>None</code> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def __init__(\n    self,\n    llm_client: LLMClient,\n    concurrency: int = 5,\n    rate_limiter: RateLimiter | None = None,\n    retry_handler: RetryHandler | None = None,\n    error_policy: ErrorPolicy = ErrorPolicy.SKIP,\n    max_retries: int = 3,\n    output_cls: type[BaseModel] | None = None,\n):\n    \"\"\"\n    Initialize LLM invocation stage.\n\n    Args:\n        llm_client: LLM client instance\n        concurrency: Max concurrent requests\n        rate_limiter: Optional rate limiter\n        retry_handler: Optional retry handler\n        error_policy: Policy for handling errors\n        max_retries: Maximum retry attempts\n        output_cls: Optional Pydantic model for structured output\n    \"\"\"\n    super().__init__(\"LLMInvocation\")\n    self.llm_client = llm_client\n    self.concurrency = concurrency\n    self.rate_limiter = rate_limiter\n    self.retry_handler = retry_handler or RetryHandler()\n    self.output_cls = output_cls\n    self.error_handler = ErrorHandler(\n        policy=error_policy,\n        max_retries=max_retries,\n        default_value_factory=lambda: LLMResponse(\n            text=\"\",\n            tokens_in=0,\n            tokens_out=0,\n            model=llm_client.model,\n            cost=Decimal(\"0.0\"),\n            latency_ms=0.0,\n        ),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage.process","title":"process","text":"<pre><code>process(batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]\n</code></pre> <p>Execute LLM calls for all prompt batches using flatten-then-concurrent pattern.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def process(self, batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]:\n    \"\"\"Execute LLM calls for all prompt batches using flatten-then-concurrent pattern.\"\"\"\n\n    # Initialize token tracking in context.intermediate_data (leverage existing design)\n    if \"token_tracking\" not in context.intermediate_data:\n        context.intermediate_data[\"token_tracking\"] = {\n            \"input_tokens\": 0,\n            \"output_tokens\": 0,\n        }\n\n    # Start progress tracking if available\n    progress_tracker = getattr(context, \"progress_tracker\", None)\n    progress_task = None\n    if progress_tracker:\n        total_prompts = sum(len(b.prompts) for b in batches)\n        progress_task = progress_tracker.start_stage(\n            f\"{self.name}: {context.total_rows} rows\",\n            total_rows=total_prompts,\n        )\n        # Store for access in concurrent loop\n        self._current_progress_task = progress_task\n\n    # Flatten all prompts from all batches\n    all_prompts, batch_map = self._flatten_batches(batches)\n\n    # Calculate total rows (handle both aggregated and non-aggregated batches)\n    total_rows = 0\n    for batch in batches:\n        if not batch.metadata:\n            continue\n        if (\n            batch.metadata\n            and batch.metadata[0].custom\n            and batch.metadata[0].custom.get(\"is_batch\")\n        ):\n            # Aggregated batch: use batch_size from metadata\n            total_rows += batch.metadata[0].custom.get(\n                \"batch_size\", len(batch.metadata)\n            )\n        else:\n            # Non-aggregated batch: count metadata entries\n            total_rows += len(batch.metadata)\n\n    self.logger.info(\n        f\"Processing {total_rows:,} rows in {len(batches)} API calls \"\n        f\"({self.concurrency} concurrent)\"\n    )\n\n    # Step 2: Process ALL prompts concurrently (ignore batch boundaries)\n    all_responses = self._process_all_prompts_concurrent(\n        all_prompts, context, batches\n    )\n\n    # Step 3: Reconstruct batches from flat responses\n    response_batches = self._reconstruct_batches(all_responses, batches, batch_map)\n\n    # Notify progress after processing\n    if hasattr(context, \"notify_progress\"):\n        context.notify_progress()\n\n    # Finish progress tracking\n    if progress_tracker and progress_task:\n        progress_tracker.finish(progress_task)\n\n    return response_batches\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(batches: list[PromptBatch]) -&gt; ValidationResult\n</code></pre> <p>Validate prompt batches.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def validate_input(self, batches: list[PromptBatch]) -&gt; ValidationResult:\n    \"\"\"Validate prompt batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    if not batches:\n        result.add_error(\"No prompt batches provided\")\n\n    for batch in batches:\n        if not batch.prompts:\n            result.add_error(f\"Batch {batch.batch_id} has no prompts\")\n\n        if len(batch.prompts) != len(batch.metadata):\n            result.add_error(f\"Batch {batch.batch_id} prompt/metadata mismatch\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(batches: list[PromptBatch]) -&gt; CostEstimate\n</code></pre> <p>Estimate LLM invocation cost.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def estimate_cost(self, batches: list[PromptBatch]) -&gt; CostEstimate:\n    \"\"\"Estimate LLM invocation cost.\"\"\"\n    total_input_tokens = 0\n    total_output_tokens = 0\n\n    # Estimate tokens for all prompts\n    for batch in batches:\n        for prompt in batch.prompts:\n            input_tokens = self.llm_client.estimate_tokens(prompt)\n            total_input_tokens += input_tokens\n\n            # Assume average output length (can be made configurable)\n            estimated_output = int(input_tokens * 0.5)\n            total_output_tokens += estimated_output\n\n    total_cost = self.llm_client.calculate_cost(\n        total_input_tokens, total_output_tokens\n    )\n\n    return CostEstimate(\n        total_cost=total_cost,\n        total_tokens=total_input_tokens + total_output_tokens,\n        input_tokens=total_input_tokens,\n        output_tokens=total_output_tokens,\n        rows=sum(len(b.prompts) for b in batches),\n        confidence=\"estimate\",\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.AggregationStrategy","title":"AggregationStrategy","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract base for aggregation strategies.</p> <p>Follows Strategy pattern for different ways to aggregate results.</p>"},{"location":"api/stages/#ondine.stages.AggregationStrategy.aggregate","title":"aggregate  <code>abstractmethod</code>","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Aggregate multiple results into one.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[T]</code> <p>List of results from multiple runs</p> required <p>Returns:</p> Type Description <code>T</code> <p>Aggregated result</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>@abstractmethod\ndef aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"\n    Aggregate multiple results into one.\n\n    Args:\n        results: List of results from multiple runs\n\n    Returns:\n        Aggregated result\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.AllStrategy","title":"AllStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns all results as list (no aggregation).</p>"},{"location":"api/stages/#ondine.stages.AllStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; list[T]\n</code></pre> <p>Return all results.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; list[T]:\n    \"\"\"Return all results.\"\"\"\n    return results\n</code></pre>"},{"location":"api/stages/#ondine.stages.AverageStrategy","title":"AverageStrategy","text":"<p>               Bases: <code>AggregationStrategy[float]</code></p> <p>Returns average of numeric results.</p>"},{"location":"api/stages/#ondine.stages.AverageStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[float]) -&gt; float\n</code></pre> <p>Return average.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[float]) -&gt; float:\n    \"\"\"Return average.\"\"\"\n    if not results:\n        return 0.0\n    return sum(results) / len(results)\n</code></pre>"},{"location":"api/stages/#ondine.stages.ConsensusStrategy","title":"ConsensusStrategy","text":"<p>               Bases: <code>AggregationStrategy[str]</code></p> <p>Returns most common result (consensus voting).</p>"},{"location":"api/stages/#ondine.stages.ConsensusStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[str]) -&gt; str\n</code></pre> <p>Return most frequent result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[str]) -&gt; str:\n    \"\"\"Return most frequent result.\"\"\"\n    if not results:\n        return \"\"\n\n    # Count occurrences\n    counter = Counter(results)\n    return counter.most_common(1)[0][0]\n</code></pre>"},{"location":"api/stages/#ondine.stages.FirstSuccessStrategy","title":"FirstSuccessStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns first successful (non-None) result.</p>"},{"location":"api/stages/#ondine.stages.FirstSuccessStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Return first non-None result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"Return first non-None result.\"\"\"\n    for result in results:\n        if result is not None:\n            return result\n    return results[0] if results else None\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage","title":"MultiRunStage","text":"<pre><code>MultiRunStage(wrapped_stage: PipelineStage[TInput, TOutput], num_runs: int = 3, aggregation_strategy: AggregationStrategy | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[TInput, TOutput]</code></p> <p>Decorator stage that runs wrapped stage multiple times.</p> <p>Use cases: - Run LLM 3 times, take consensus (reduce hallucinations) - Retry until success - Collect multiple responses for analysis</p> Example <p>multi_llm = MultiRunStage(     wrapped=LLMInvocationStage(...),     num_runs=3,     aggregation=ConsensusStrategy() )</p> <p>Initialize multi-run stage.</p> <p>Parameters:</p> Name Type Description Default <code>wrapped_stage</code> <code>PipelineStage[TInput, TOutput]</code> <p>Stage to execute multiple times</p> required <code>num_runs</code> <code>int</code> <p>Number of times to run</p> <code>3</code> <code>aggregation_strategy</code> <code>AggregationStrategy | None</code> <p>Strategy for aggregating results</p> <code>None</code> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def __init__(\n    self,\n    wrapped_stage: PipelineStage[TInput, TOutput],\n    num_runs: int = 3,\n    aggregation_strategy: AggregationStrategy | None = None,\n):\n    \"\"\"\n    Initialize multi-run stage.\n\n    Args:\n        wrapped_stage: Stage to execute multiple times\n        num_runs: Number of times to run\n        aggregation_strategy: Strategy for aggregating results\n    \"\"\"\n    super().__init__(f\"MultiRun({wrapped_stage.name})\")\n    self.wrapped_stage = wrapped_stage\n    self.num_runs = num_runs\n    self.aggregation_strategy = aggregation_strategy or ConsensusStrategy()\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage.process","title":"process","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute wrapped stage multiple times and aggregate.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"Execute wrapped stage multiple times and aggregate.\"\"\"\n    results: list[TOutput] = []\n\n    self.logger.info(f\"Running {self.wrapped_stage.name} {self.num_runs} times\")\n\n    for run_num in range(self.num_runs):\n        try:\n            result = self.wrapped_stage.process(input_data, context)\n            results.append(result)\n        except Exception as e:\n            self.logger.error(f\"Run {run_num + 1}/{self.num_runs} failed: {e}\")\n            # Continue with other runs\n            continue\n\n    if not results:\n        raise RuntimeError(\n            f\"All {self.num_runs} runs failed for {self.wrapped_stage.name}\"\n        )\n\n    # Aggregate results\n    aggregated = self.aggregation_strategy.aggregate(results)\n\n    self.logger.info(\n        f\"Aggregated {len(results)} results using \"\n        f\"{self.aggregation_strategy.__class__.__name__}\"\n    )\n\n    return aggregated\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Delegate validation to wrapped stage.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"Delegate validation to wrapped stage.\"\"\"\n    return self.wrapped_stage.validate_input(input_data)\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate cost as num_runs \u00d7 wrapped stage cost.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"Estimate cost as num_runs \u00d7 wrapped stage cost.\"\"\"\n    single_run_cost = self.wrapped_stage.estimate_cost(input_data)\n\n    return CostEstimate(\n        total_cost=single_run_cost.total_cost * self.num_runs,\n        total_tokens=single_run_cost.total_tokens * self.num_runs,\n        input_tokens=single_run_cost.input_tokens * self.num_runs,\n        output_tokens=single_run_cost.output_tokens * self.num_runs,\n        rows=single_run_cost.rows,\n        confidence=f\"{single_run_cost.confidence} \u00d7 {self.num_runs} runs\",\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage","title":"PipelineStage","text":"<pre><code>PipelineStage(name: str)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[TInput, TOutput]</code></p> <p>Abstract base class for all pipeline stages.</p> <p>Implements Template Method pattern with hooks for extensibility. All stages follow Single Responsibility Principle and are composable via the Chain of Responsibility pattern.</p> <p>Stages in the pipeline: 1. DataLoaderStage - Load data from source 2. PromptFormatterStage - Format prompts with data 3. LLMInvocationStage - Call LLM API 4. ResponseParserStage - Parse LLM responses 5. ResultWriterStage - Write results to output</p> Example <pre><code>class CustomStage(PipelineStage[pd.DataFrame, pd.DataFrame]):\n    def process(self, input_data, context):\n        # Custom processing logic\n        return processed_data\n\n    def validate_input(self, input_data):\n        # Validation logic\n        return ValidationResult(is_valid=True)\n</code></pre> <p>Initialize pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable stage name</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize pipeline stage.\n\n    Args:\n        name: Human-readable stage name\n    \"\"\"\n    self.name = name\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.process","title":"process  <code>abstractmethod</code>","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Core processing logic (must be implemented by subclasses).</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data for this stage</p> required <code>context</code> <code>Any</code> <p>Execution context with shared state</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output data</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Core processing logic (must be implemented by subclasses).\n\n    Args:\n        input_data: Input data for this stage\n        context: Execution context with shared state\n\n    Returns:\n        Processed output data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.validate_input","title":"validate_input  <code>abstractmethod</code>","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Validate input before processing.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input to validate</p> required <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with errors/warnings</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"\n    Validate input before processing.\n\n    Args:\n        input_data: Input to validate\n\n    Returns:\n        ValidationResult with errors/warnings\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.execute","title":"execute","text":"<pre><code>execute(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute stage with pre/post hooks (Template Method).</p> <p>This method orchestrates the execution flow and should not be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input validation fails</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def execute(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Execute stage with pre/post hooks (Template Method).\n\n    This method orchestrates the execution flow and should not\n    be overridden.\n\n    Args:\n        input_data: Input data\n        context: Execution context\n\n    Returns:\n        Processed output\n\n    Raises:\n        ValueError: If input validation fails\n    \"\"\"\n    self.logger.info(f\"Starting stage: {self.name}\")\n\n    # Pre-processing hook\n    self.before_process(context)\n\n    # Validate input\n    validation = self.validate_input(input_data)\n    if not validation.is_valid:\n        error_msg = f\"Input validation failed: {validation.errors}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    if validation.warnings:\n        for warning in validation.warnings:\n            self.logger.warning(warning)\n\n    # Core processing\n    try:\n        result = self.process(input_data, context)\n        self.logger.info(f\"Completed stage: {self.name}\")\n\n        # Post-processing hook\n        self.after_process(result, context)\n\n        return result\n    except Exception as e:\n        self.logger.error(f\"Stage {self.name} failed: {e}\")\n        error_decision = self.on_error(e, context)\n        raise error_decision\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.before_process","title":"before_process","text":"<pre><code>before_process(context: Any) -&gt; None\n</code></pre> <p>Hook called before processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def before_process(self, context: Any) -&gt; None:\n    \"\"\"\n    Hook called before processing (default: no-op).\n\n    Args:\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.after_process","title":"after_process","text":"<pre><code>after_process(result: TOutput, context: Any) -&gt; None\n</code></pre> <p>Hook called after successful processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TOutput</code> <p>Processing result</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def after_process(self, result: TOutput, context: Any) -&gt; None:\n    \"\"\"\n    Hook called after successful processing (default: no-op).\n\n    Args:\n        result: Processing result\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.on_error","title":"on_error","text":"<pre><code>on_error(error: Exception, context: Any) -&gt; Exception\n</code></pre> <p>Hook called on processing error (default: re-raise).</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Exception</code> <p>Exception to raise (can transform error)</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def on_error(self, error: Exception, context: Any) -&gt; Exception:\n    \"\"\"\n    Hook called on processing error (default: re-raise).\n\n    Args:\n        error: The exception that occurred\n        context: Execution context\n\n    Returns:\n        Exception to raise (can transform error)\n    \"\"\"\n    return error\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.estimate_cost","title":"estimate_cost  <code>abstractmethod</code>","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate processing cost for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data to estimate for</p> required <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"\n    Estimate processing cost for this stage.\n\n    Args:\n        input_data: Input data to estimate for\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage","title":"PromptFormatterStage","text":"<pre><code>PromptFormatterStage(batch_size: int = 100, use_jinja2: bool = False)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, PromptSpec], list[PromptBatch]]</code></p> <p>Format prompts using template and row data.</p> <p>Responsibilities: - Extract input columns from rows - Format prompts using template - Batch prompts for efficient processing - Attach metadata for tracking</p> <p>Initialize prompt formatter stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of prompts per batch</p> <code>100</code> <code>use_jinja2</code> <code>bool</code> <p>Use Jinja2 for template rendering</p> <code>False</code> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def __init__(self, batch_size: int = 100, use_jinja2: bool = False):\n    \"\"\"\n    Initialize prompt formatter stage.\n\n    Args:\n        batch_size: Number of prompts per batch\n        use_jinja2: Use Jinja2 for template rendering\n    \"\"\"\n    super().__init__(\"PromptFormatter\")\n    self.batch_size = batch_size\n    self.use_jinja2 = use_jinja2\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, PromptSpec], context: Any) -&gt; list[PromptBatch]\n</code></pre> <p>Format prompts from DataFrame rows.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def process(\n    self, input_data: tuple[pd.DataFrame, PromptSpec], context: Any\n) -&gt; list[PromptBatch]:\n    \"\"\"Format prompts from DataFrame rows.\"\"\"\n    df, prompt_spec = input_data\n\n    prompts: list[str] = []\n    metadata_list: list[RowMetadata] = []\n\n    # Extract template variables and system message\n    template_str = prompt_spec.template\n    system_message = prompt_spec.system_message\n\n    # Create template renderer\n    if self.use_jinja2:\n        # Note: autoescape=False is intentional for LLM prompts (not HTML)\n        # We're generating text prompts, not web content, so HTML escaping\n        # would corrupt the prompt data sent to the LLM\n        template = Jinja2Template(template_str, autoescape=False)  # noqa: S701\n\n    # Format prompt for each row\n    # Performance optimization: Use itertuples() instead of iterrows() for 10\u00d7 speedup\n    # itertuples() returns namedtuples which are much faster than Series objects\n    import time\n\n    total_rows = len(df)\n    start_time = time.time()\n    last_log_time = start_time\n    last_log_pct = 0\n\n    self.logger.info(f\"Formatting {total_rows:,} prompts...\")\n\n    for row_count, row in enumerate(df.itertuples(index=True), 1):\n        # Hybrid progress: Log every 10% OR every 30 seconds (only for slow operations)\n        current_time = time.time()\n        current_pct = int((row_count / total_rows) * 100)\n        elapsed = current_time - start_time\n\n        # Only log progress if operation is taking &gt;5 seconds\n        should_log = elapsed &gt; 5 and (\n            (current_pct &gt;= last_log_pct + 10 and current_pct &lt;= 90)  # Every 10%\n            or (current_time - last_log_time &gt;= 30)  # OR every 30s\n        )\n\n        if should_log:\n            elapsed = current_time - start_time\n            throughput = row_count / elapsed if elapsed &gt; 0 else 0\n            eta = (total_rows - row_count) / throughput if throughput &gt; 0 else 0\n\n            self.logger.info(\n                f\"Formatting: {current_pct}% ({row_count:,}/{total_rows:,}) | \"\n                f\"{throughput:,.0f} rows/s | ETA: {eta:.0f}s\"\n            )\n            last_log_time = current_time\n            last_log_pct = current_pct\n\n        try:\n            # Extract index (first element of namedtuple)\n            idx = row[0]\n\n            # Extract input columns from namedtuple\n            # Build row_data dict from column names and namedtuple attributes\n            row_data = {}\n            for col in df.columns:\n                if col in template_str:\n                    # Get attribute by column name (namedtuples have column names as attributes)\n                    row_data[col] = getattr(row, col)\n\n            # Format prompt (Jinja2 or f-string)\n            if self.use_jinja2:\n                prompt = template.render(**row_data)\n            else:\n                prompt = template_str.format(**row_data)\n\n            # Add few-shot examples if specified (but NOT system message)\n            if prompt_spec.few_shot_examples:\n                examples_text = self._format_few_shot_examples(\n                    prompt_spec.few_shot_examples\n                )\n                prompt = f\"{examples_text}\\n\\n{prompt}\"\n\n            # NOTE: Do NOT add system message to prompt here\n            # It will be passed separately via metadata for caching optimization\n\n            prompts.append(prompt)\n\n            # Create metadata with system message for LLM stage\n            # Get 'id' column if it exists\n            row_id = getattr(row, \"id\", None) if hasattr(row, \"id\") else None\n            metadata = RowMetadata(\n                row_index=idx,\n                row_id=row_id,\n                custom={\"system_message\": system_message}\n                if system_message\n                else None,\n            )\n            metadata_list.append(metadata)\n\n        except (KeyError, AttributeError) as e:\n            self.logger.warning(f\"Missing template variable at row {idx}: {e}\")\n            continue\n        except Exception as e:\n            self.logger.error(f\"Error formatting prompt at row {idx}: {e}\")\n            continue\n\n    # Create batches\n    batches: list[PromptBatch] = []\n    for i in range(0, len(prompts), self.batch_size):\n        batch_prompts = prompts[i : i + self.batch_size]\n        batch_metadata = metadata_list[i : i + self.batch_size]\n\n        batch = PromptBatch(\n            prompts=batch_prompts,\n            metadata=batch_metadata,\n            batch_id=i // self.batch_size,\n        )\n        batches.append(batch)\n\n    # Final summary\n    total_time = time.time() - start_time\n    throughput = len(prompts) / total_time if total_time &gt; 0 else 0\n    self.logger.info(\n        f\"\u2713 Formatted {len(prompts):,} prompts in {total_time:.1f}s ({throughput:,.0f} rows/s)\"\n    )\n\n    return batches\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, PromptSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate DataFrame and prompt specification.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; ValidationResult:\n    \"\"\"Validate DataFrame and prompt specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    df, prompt_spec = input_data\n\n    # Check DataFrame not empty\n    if df.empty:\n        result.add_error(\"DataFrame is empty\")\n\n    # Check template variables exist in DataFrame\n    template = prompt_spec.template\n    import re\n\n    variables = re.findall(r\"\\{(\\w+)\\}\", template)\n    missing_vars = set(variables) - set(df.columns)\n\n    if missing_vars:\n        result.add_error(f\"Template variables not in DataFrame: {missing_vars}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, PromptSpec]) -&gt; CostEstimate\n</code></pre> <p>Prompt formatting has no LLM cost.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; CostEstimate:\n    \"\"\"Prompt formatting has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.JSONParser","title":"JSONParser","text":"<pre><code>JSONParser(strict: bool = False)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts JSON from response.</p> <p>Initialize JSON parser.</p> <p>Parameters:</p> Name Type Description Default <code>strict</code> <code>bool</code> <p>If True, fail on invalid JSON; if False, try to extract</p> <code>False</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, strict: bool = False):\n    \"\"\"\n    Initialize JSON parser.\n\n    Args:\n        strict: If True, fail on invalid JSON; if False, try to extract\n    \"\"\"\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/#ondine.stages.JSONParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse JSON from response.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse JSON from response.\"\"\"\n    try:\n        return json.loads(response.strip())\n    except json.JSONDecodeError:\n        if self.strict:\n            raise\n\n        # Try to extract JSON from markdown code blocks\n        if \"```json\" in response:\n            start = response.find(\"```json\") + 7\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        if \"```\" in response:\n            start = response.find(\"```\") + 3\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        # Return as raw text if can't parse\n        return {\"output\": response.strip()}\n</code></pre>"},{"location":"api/stages/#ondine.stages.PydanticParser","title":"PydanticParser","text":"<pre><code>PydanticParser(model: type[BaseModel], strict: bool = True)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that validates responses against Pydantic models.</p> <p>Provides type-safe extraction with automatic validation.</p> <p>Initialize Pydantic parser.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>type[BaseModel]</code> <p>Pydantic model class for validation</p> required <code>strict</code> <code>bool</code> <p>If True, fail on validation errors</p> <code>True</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, model: type[BaseModel], strict: bool = True):\n    \"\"\"\n    Initialize Pydantic parser.\n\n    Args:\n        model: Pydantic model class for validation\n        strict: If True, fail on validation errors\n    \"\"\"\n    self.model = model\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/#ondine.stages.PydanticParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; BaseModel\n</code></pre> <p>Parse and validate response with Pydantic model.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; BaseModel:\n    \"\"\"Parse and validate response with Pydantic model.\"\"\"\n    try:\n        # Try to parse as JSON first\n        json_parser = JSONParser(strict=False)\n        data = json_parser.parse(response)\n\n        # Validate with Pydantic and return the model instance\n        return self.model(**data)\n\n    except ValidationError as e:\n        if self.strict:\n            raise ValueError(f\"Pydantic validation failed: {e}\")\n        # Return raw data if validation fails\n        return {\"output\": response.strip(), \"validation_error\": str(e)}\n</code></pre>"},{"location":"api/stages/#ondine.stages.RawTextParser","title":"RawTextParser","text":"<p>               Bases: <code>ResponseParser</code></p> <p>Parser that returns raw text.</p>"},{"location":"api/stages/#ondine.stages.RawTextParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Return response as-is, after cleaning chat format artifacts.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Return response as-is, after cleaning chat format artifacts.\"\"\"\n    cleaned = response.strip()\n\n    # Strip common chat format prefixes (assistant:, user:, system:)\n    for prefix in [\"assistant:\", \"user:\", \"system:\"]:\n        if cleaned.lower().startswith(prefix):\n            cleaned = cleaned[len(prefix) :].strip()\n            break\n\n    return {\"output\": cleaned}\n</code></pre>"},{"location":"api/stages/#ondine.stages.RegexParser","title":"RegexParser","text":"<pre><code>RegexParser(patterns: dict[str, str])\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts data using regex patterns.</p> <p>Useful for extracting specific fields from structured text.</p> <p>Initialize regex parser.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>dict[str, str]</code> <p>Dict mapping field names to regex patterns</p> required Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, patterns: dict[str, str]):\n    \"\"\"\n    Initialize regex parser.\n\n    Args:\n        patterns: Dict mapping field names to regex patterns\n    \"\"\"\n    self.patterns = {key: re.compile(pattern) for key, pattern in patterns.items()}\n</code></pre>"},{"location":"api/stages/#ondine.stages.RegexParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Extract fields using regex patterns.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Extract fields using regex patterns.\"\"\"\n    result = {}\n\n    for field_name, pattern in self.patterns.items():\n        match = pattern.search(response)\n        if match:\n            # Use first group if groups exist, else full match\n            if match.groups():\n                result[field_name] = match.group(1)\n            else:\n                result[field_name] = match.group(0)\n        else:\n            result[field_name] = None\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParser","title":"ResponseParser","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for response parsers (Strategy pattern).</p>"},{"location":"api/stages/#ondine.stages.ResponseParser.parse","title":"parse  <code>abstractmethod</code>","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse response into structured data.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>@abstractmethod\ndef parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse response into structured data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage","title":"ResponseParserStage","text":"<pre><code>ResponseParserStage(parser: ResponseParser | None = None, output_columns: list[str] | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[list[ResponseBatch], list[str]], DataFrame]</code></p> <p>Parse LLM responses into structured DataFrame.</p> <p>Responsibilities: - Parse responses using configured parser - Map parsed data to output columns - Handle parse errors gracefully - Return DataFrame with results</p> <p>Initialize response parser stage.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ResponseParser | None</code> <p>Response parser (default: RawTextParser)</p> <code>None</code> <code>output_columns</code> <code>list[str] | None</code> <p>Output column names</p> <code>None</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(\n    self,\n    parser: ResponseParser | None = None,\n    output_columns: list[str] | None = None,\n):\n    \"\"\"\n    Initialize response parser stage.\n\n    Args:\n        parser: Response parser (default: RawTextParser)\n        output_columns: Output column names\n    \"\"\"\n    super().__init__(\"ResponseParser\")\n    self.parser = parser or RawTextParser()\n    self.output_columns = output_columns or [\"output\"]\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage.process","title":"process","text":"<pre><code>process(input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Parse responses into DataFrame.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Parse responses into DataFrame.\"\"\"\n    # Handle both tuple (batches, output_cols) and list [batches] for backward compatibility\n    if isinstance(input_data, tuple):\n        batches, output_cols = input_data\n        # Use output_cols from input_data (overrides self.output_columns if provided)\n        if not output_cols:\n            output_cols = self.output_columns\n    else:\n        # Backward compatibility: input_data is just the list of batches\n        batches = input_data\n        output_cols = self.output_columns\n\n    # Initialize result storage\n    results: dict[int, dict[str, Any]] = {}\n\n    # Parse all responses\n    for batch in batches:\n        for response, metadata in zip(\n            batch.responses, batch.metadata, strict=False\n        ):\n            try:\n                # Parse response text\n                response_text = (\n                    response.text if hasattr(response, \"text\") else str(response)\n                )\n                parsed = self.parser.parse(response_text)\n\n                # Map to output columns\n                row_data = {}\n                if len(output_cols) == 1:\n                    # Single output column\n                    if isinstance(parsed, dict) and \"output\" in parsed:\n                        row_data[output_cols[0]] = parsed[\"output\"]\n                    elif isinstance(parsed, dict):\n                        # Use first value\n                        row_data[output_cols[0]] = next(iter(parsed.values()))\n                    else:\n                        row_data[output_cols[0]] = parsed\n                else:\n                    # Multiple output columns\n                    for col in output_cols:\n                        row_data[col] = parsed.get(col, None)\n\n                results[metadata.row_index] = row_data\n\n            except Exception as e:\n                self.logger.error(\n                    f\"Failed to parse response at row {metadata.row_index}: {e}\"\n                )\n                # Store None for failed parses\n                results[metadata.row_index] = {col: None for col in output_cols}\n\n    # Create DataFrame\n    df = pd.DataFrame.from_dict(results, orient=\"index\")\n    df.index.name = \"row_index\"\n\n    self.logger.info(f\"Parsed {len(results)} responses\")\n\n    return df\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; ValidationResult\n</code></pre> <p>Validate response batches.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; ValidationResult:\n    \"\"\"Validate response batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    batches, output_cols = input_data\n\n    if not batches:\n        result.add_error(\"No response batches provided\")\n\n    if not output_cols:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; CostEstimate\n</code></pre> <p>Response parsing has no LLM cost.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; CostEstimate:\n    \"\"\"Response parsing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.responses) for b in input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage","title":"ResultWriterStage","text":"<pre><code>ResultWriterStage()\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, DataFrame, OutputSpec], DataFrame]</code></p> <p>Write results to destination with merge support.</p> <p>Responsibilities: - Merge results with original data - Write to configured destination - Support atomic writes - Return merged DataFrame</p> <p>Initialize result writer stage.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize result writer stage.\"\"\"\n    super().__init__(\"ResultWriter\")\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, DataFrame, OutputSpec], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Write results to destination and return merged DataFrame.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Write results to destination and return merged DataFrame.\"\"\"\n    original_df, results_df, output_spec = input_data\n\n    # Merge results with original data\n    merged_df = self._merge_results(\n        original_df, results_df, output_spec.merge_strategy\n    )\n\n    # Write to destination\n    if output_spec.destination_path:\n        writer = create_data_writer(output_spec.destination_type)\n\n        if output_spec.atomic_write:\n            confirmation = writer.atomic_write(\n                merged_df, output_spec.destination_path\n            )\n        else:\n            confirmation = writer.write(merged_df, output_spec.destination_path)\n\n        self.logger.info(\n            f\"Wrote {confirmation.rows_written} rows to {confirmation.path}\"\n        )\n\n    # Always return the merged DataFrame (needed for quality validation)\n    return merged_df\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate input data and output specification.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def validate_input(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; ValidationResult:\n    \"\"\"Validate input data and output specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    original_df, results_df, output_spec = input_data\n\n    if original_df.empty:\n        result.add_warning(\"Original DataFrame is empty\")\n\n    if results_df.empty:\n        result.add_error(\"Results DataFrame is empty\")\n\n    # Check destination path if specified\n    if output_spec.destination_path:\n        dest_dir = output_spec.destination_path.parent\n        if not dest_dir.exists():\n            result.add_warning(f\"Destination directory does not exist: {dest_dir}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; CostEstimate\n</code></pre> <p>Result writing has no LLM cost.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def estimate_cost(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; CostEstimate:\n    \"\"\"Result writing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[1]),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry","title":"StageRegistry","text":"<p>Global registry for custom pipeline stages.</p> <p>Enables registration and discovery of custom stages that can be injected into pipelines at specific positions.</p> Example"},{"location":"api/stages/#ondine.stages.StageRegistry--register-custom-stage","title":"Register custom stage","text":"<p>@StageRegistry.register(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     def execute(self, context):         # Custom retrieval logic         ...</p>"},{"location":"api/stages/#ondine.stages.StageRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline = (     PipelineBuilder.create()     .with_stage(\"rag_retrieval\", position=\"before_prompt\", index=\"my-docs\")     .build() )</p>"},{"location":"api/stages/#ondine.stages.StageRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(stage_name: str, stage_class: type) -&gt; type\n</code></pre> <p>Register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")</p> required <code>stage_class</code> <code>type</code> <p>Stage class implementing PipelineStage interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered stage class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name already registered</p> Example <p>@StageRegistry.register(\"fact_checker\") class FactCheckerStage(PipelineStage):     def execute(self, context):         # Verify LLM output against sources         ...</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef register(cls, stage_name: str, stage_class: type) -&gt; type:\n    \"\"\"\n    Register a custom pipeline stage.\n\n    Args:\n        stage_name: Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")\n        stage_class: Stage class implementing PipelineStage interface\n\n    Returns:\n        The registered stage class (enables use as decorator)\n\n    Raises:\n        ValueError: If stage_name already registered\n\n    Example:\n        @StageRegistry.register(\"fact_checker\")\n        class FactCheckerStage(PipelineStage):\n            def execute(self, context):\n                # Verify LLM output against sources\n                ...\n    \"\"\"\n    if stage_name in cls._stages:\n        raise ValueError(\n            f\"Stage '{stage_name}' already registered. \"\n            f\"Use a different stage_name or unregister first.\"\n        )\n\n    cls._stages[stage_name] = stage_class\n    return stage_class\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(stage_name: str) -&gt; type\n</code></pre> <p>Get stage class by name.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>Pipeline stage class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Example <p>stage_class = StageRegistry.get(\"rag_retrieval\") stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef get(cls, stage_name: str) -&gt; type:\n    \"\"\"\n    Get stage class by name.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        Pipeline stage class\n\n    Raises:\n        ValueError: If stage not found\n\n    Example:\n        stage_class = StageRegistry.get(\"rag_retrieval\")\n        stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")\n    \"\"\"\n    if stage_name not in cls._stages:\n        available = \", \".join(sorted(cls._stages.keys()))\n        raise ValueError(\n            f\"Unknown stage: '{stage_name}'. \"\n            f\"Available stages: {available if available else 'none'}\"\n        )\n\n    return cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.list_stages","title":"list_stages  <code>classmethod</code>","text":"<pre><code>list_stages() -&gt; dict[str, type]\n</code></pre> <p>List all registered stages.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping stage names to stage classes</p> Example <p>stages = StageRegistry.list_stages() print(f\"Available custom stages: {list(stages.keys())}\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef list_stages(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered stages.\n\n    Returns:\n        Dictionary mapping stage names to stage classes\n\n    Example:\n        stages = StageRegistry.list_stages()\n        print(f\"Available custom stages: {list(stages.keys())}\")\n    \"\"\"\n    return cls._stages.copy()\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(stage_name: str) -&gt; bool\n</code></pre> <p>Check if stage is registered.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, stage_name: str) -&gt; bool:\n    \"\"\"\n    Check if stage is registered.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    return stage_name in cls._stages\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(stage_name: str) -&gt; None\n</code></pre> <p>Unregister a stage (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, stage_name: str) -&gt; None:\n    \"\"\"\n    Unregister a stage (mainly for testing).\n\n    Args:\n        stage_name: Stage identifier\n\n    Raises:\n        ValueError: If stage not found\n    \"\"\"\n    if stage_name not in cls._stages:\n        raise ValueError(f\"Stage '{stage_name}' not registered\")\n\n    del cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/#ondine.stages.create_response_parser","title":"create_response_parser","text":"<pre><code>create_response_parser(prompt_spec: PromptSpec, output_columns: list[str]) -&gt; ResponseParser\n</code></pre> <p>Create appropriate response parser based on prompt specification.</p> <p>This factory enables configuration-driven parser selection, supporting: - Raw text output (default, backward compatible) - JSON structured output (for multiple fields) - Regex pattern extraction (for formatted text)</p> <p>Parameters:</p> Name Type Description Default <code>prompt_spec</code> <code>PromptSpec</code> <p>Prompt specification with response_format</p> required <code>output_columns</code> <code>list[str]</code> <p>Expected output column names</p> required <p>Returns:</p> Type Description <code>ResponseParser</code> <p>Configured ResponseParser instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If response_format is invalid or required config missing</p> Example Source code in <code>ondine/stages/parser_factory.py</code> <pre><code>def create_response_parser(\n    prompt_spec: PromptSpec,\n    output_columns: list[str],\n) -&gt; ResponseParser:\n    r\"\"\"\n    Create appropriate response parser based on prompt specification.\n\n    This factory enables configuration-driven parser selection, supporting:\n    - Raw text output (default, backward compatible)\n    - JSON structured output (for multiple fields)\n    - Regex pattern extraction (for formatted text)\n\n    Args:\n        prompt_spec: Prompt specification with response_format\n        output_columns: Expected output column names\n\n    Returns:\n        Configured ResponseParser instance\n\n    Raises:\n        ValueError: If response_format is invalid or required config missing\n\n    Example:\n        # JSON mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"json\",\n                json_fields=[\"score\", \"explanation\"]\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n\n        # Regex mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"regex\",\n                regex_patterns={\n                    \"score\": r\"SCORE:\\s*(\\d+)\",\n                    \"explanation\": r\"EXPLANATION:\\s*(.+)\"\n                }\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n    \"\"\"\n    response_format = prompt_spec.response_format.lower()\n\n    if response_format == \"json\":\n        logger.info(\"Creating JSONParser for multi-field extraction\")\n\n        # Validate JSON fields match output columns\n        if prompt_spec.json_fields:\n            json_set = set(prompt_spec.json_fields)\n            output_set = set(output_columns)\n            if json_set != output_set:\n                logger.warning(\n                    f\"JSON fields {json_set} don't match output columns {output_set}. \"\n                    f\"Using output_columns as authoritative.\"\n                )\n\n        return JSONParser(strict=False)\n\n    if response_format == \"regex\":\n        logger.info(\"Creating RegexParser for pattern-based extraction\")\n\n        if not prompt_spec.regex_patterns:\n            raise ValueError(\n                \"response_format='regex' requires regex_patterns to be specified\"\n            )\n\n        # Validate regex patterns cover all output columns\n        pattern_cols = set(prompt_spec.regex_patterns.keys())\n        output_set = set(output_columns)\n        missing = output_set - pattern_cols\n        if missing:\n            raise ValueError(f\"Missing regex patterns for output columns: {missing}\")\n\n        return RegexParser(patterns=prompt_spec.regex_patterns)\n\n    if response_format == \"raw\":\n        logger.info(\"Creating RawTextParser (default, backward compatible)\")\n\n        if len(output_columns) &gt; 1:\n            logger.warning(\n                f\"Using RawTextParser with {len(output_columns)} output columns. \"\n                f\"Only the first column will be populated. \"\n                f\"Consider using response_format='json' or 'regex' for multi-column output.\"\n            )\n\n        return RawTextParser()\n\n    # Should never reach here due to Pydantic validation\n    raise ValueError(\n        f\"Invalid response_format: '{response_format}'. \"\n        f\"Must be 'raw', 'json', or 'regex'.\"\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.create_response_parser--json-mode","title":"JSON mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"json\",         json_fields=[\"score\", \"explanation\"]     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/#ondine.stages.create_response_parser--regex-mode","title":"Regex mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"regex\",         regex_patterns={             \"score\": r\"SCORE:\\s(\\d+)\",             \"explanation\": r\"EXPLANATION:\\s(.+)\"         }     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/#ondine.stages.stage","title":"stage","text":"<pre><code>stage(name: str)\n</code></pre> <p>Decorator to register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique stage identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@stage(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     '''Retrieve context from vector store and enrich data.'''</p> <pre><code>def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n    super().__init__(name=\"rag_retrieval\")\n    self.vector_store = vector_store\n    self.index_name = index_name\n    self.top_k = top_k\n\ndef execute(self, context: ExecutionContext) -&gt; StageResult:\n    # Retrieve context for each row\n    enriched_rows = []\n    for _, row in context.data.iterrows():\n        query = row['text']\n        results = self._retrieve(query)\n        row['retrieved_context'] = self._format_context(results)\n        enriched_rows.append(row)\n\n    context.data = pd.DataFrame(enriched_rows)\n    return StageResult(success=True, data=context.data)\n\ndef _retrieve(self, query: str):\n    # Integration with vector store\n    ...\n\ndef _format_context(self, results):\n    # Format retrieved docs\n    ...\n</code></pre> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>def stage(name: str):\n    \"\"\"\n    Decorator to register a custom pipeline stage.\n\n    Args:\n        name: Unique stage identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @stage(\"rag_retrieval\")\n        class RAGRetrievalStage(PipelineStage):\n            '''Retrieve context from vector store and enrich data.'''\n\n            def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n                super().__init__(name=\"rag_retrieval\")\n                self.vector_store = vector_store\n                self.index_name = index_name\n                self.top_k = top_k\n\n            def execute(self, context: ExecutionContext) -&gt; StageResult:\n                # Retrieve context for each row\n                enriched_rows = []\n                for _, row in context.data.iterrows():\n                    query = row['text']\n                    results = self._retrieve(query)\n                    row['retrieved_context'] = self._format_context(results)\n                    enriched_rows.append(row)\n\n                context.data = pd.DataFrame(enriched_rows)\n                return StageResult(success=True, data=context.data)\n\n            def _retrieve(self, query: str):\n                # Integration with vector store\n                ...\n\n            def _format_context(self, results):\n                # Format retrieved docs\n                ...\n    \"\"\"\n\n    def decorator(cls):\n        StageRegistry.register(name, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/stages/batch_aggregator_stage/","title":"batch_aggregator_stage","text":""},{"location":"api/stages/batch_aggregator_stage/#ondine.stages.batch_aggregator_stage","title":"batch_aggregator_stage","text":"<p>Batch aggregator stage for multi-row processing.</p> <p>This stage aggregates multiple prompts into a single batch prompt, enabling 100\u00d7 reduction in API calls.</p>"},{"location":"api/stages/batch_aggregator_stage/#ondine.stages.batch_aggregator_stage.BatchAggregatorStage","title":"BatchAggregatorStage","text":"<pre><code>BatchAggregatorStage(batch_size: int, strategy: BatchFormattingStrategy | None = None, model: str | None = None, validate_context_window: bool = False, name: str = 'BatchAggregator')\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Aggregate multiple prompts into batch prompts.</p> <p>Responsibility: - Group N prompts into chunks of batch_size - Use strategy to format each chunk as a single batch prompt - Preserve metadata for disaggregation</p> <p>Design Pattern: Strategy Pattern - Delegates formatting logic to BatchFormattingStrategy - Supports multiple formats (JSON, CSV) via strategy injection</p> <p>Initialize batch aggregator stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of prompts to aggregate per batch</p> required <code>strategy</code> <code>BatchFormattingStrategy | None</code> <p>Formatting strategy (defaults to JsonBatchStrategy)</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Model name for context window validation (optional)</p> <code>None</code> <code>validate_context_window</code> <code>bool</code> <p>Whether to validate against context limits</p> <code>False</code> <code>name</code> <code>str</code> <p>Stage name for logging</p> <code>'BatchAggregator'</code> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    strategy: BatchFormattingStrategy | None = None,\n    model: str | None = None,\n    validate_context_window: bool = False,  # Disabled by default (slow for large datasets)\n    name: str = \"BatchAggregator\",\n):\n    \"\"\"Initialize batch aggregator stage.\n\n    Args:\n        batch_size: Number of prompts to aggregate per batch\n        strategy: Formatting strategy (defaults to JsonBatchStrategy)\n        model: Model name for context window validation (optional)\n        validate_context_window: Whether to validate against context limits\n        name: Stage name for logging\n    \"\"\"\n    super().__init__(name=name)\n    self.batch_size = batch_size\n    self.strategy = strategy or JsonBatchStrategy()\n    self.model = model\n    self.validate_context_window = validate_context_window\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/batch_aggregator_stage/#ondine.stages.batch_aggregator_stage.BatchAggregatorStage.process","title":"process","text":"<pre><code>process(batches: list[PromptBatch], context: Any) -&gt; list[PromptBatch]\n</code></pre> <p>Aggregate prompts into batch prompts.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>list[PromptBatch]</code> <p>List of prompt batches (from PromptFormatterStage)</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>list[PromptBatch]</code> <p>List of aggregated prompt batches (1 prompt per batch_size rows)</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def process(self, batches: list[PromptBatch], context: Any) -&gt; list[PromptBatch]:\n    \"\"\"Aggregate prompts into batch prompts.\n\n    Args:\n        batches: List of prompt batches (from PromptFormatterStage)\n        context: Execution context\n\n    Returns:\n        List of aggregated prompt batches (1 prompt per batch_size rows)\n    \"\"\"\n    import time\n\n    aggregated_batches = []\n\n    # Calculate total prompts for progress tracking\n    total_prompts = sum(len(b.prompts) for b in batches)\n    if total_prompts == 0:\n        self.logger.info(\"No prompts to aggregate\")\n        return aggregated_batches\n\n    processed_prompts = 0\n    start_time = time.time()\n    last_log_time = start_time\n    last_log_pct = 0\n\n    expected_batches = (total_prompts + self.batch_size - 1) // self.batch_size\n    self.logger.info(\n        f\"Creating {expected_batches:,} mega-prompts ({self.batch_size} rows each)\"\n    )\n\n    # Process each batch\n    for batch_idx, batch in enumerate(batches):\n        # Group prompts into chunks of batch_size\n        for i in range(0, len(batch.prompts), self.batch_size):\n            chunk_prompts = batch.prompts[i : i + self.batch_size]\n            chunk_metadata = batch.metadata[i : i + self.batch_size]\n\n            # Extract row IDs from metadata\n            row_ids = [m.row_index for m in chunk_metadata]\n\n            # Create metadata for disaggregation\n            metadata = BatchMetadata(\n                original_count=len(chunk_prompts),\n                row_ids=row_ids,\n                prompt_template=None,  # Not available in current structure\n            )\n\n            # Validate batch size against context window (only once, not for every batch)\n            if (\n                self.validate_context_window\n                and self.model\n                and len(aggregated_batches) == 0  # Only validate first batch\n            ):\n                # Skip validation if chunk is empty\n                if not chunk_prompts:\n                    self.logger.warning(\n                        \"Empty chunk encountered, skipping validation\"\n                    )\n                    continue\n\n                # Estimate tokens for this batch\n                import tiktoken\n\n                tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n                avg_tokens = sum(\n                    len(tokenizer.encode(p)) for p in chunk_prompts\n                ) // len(chunk_prompts)\n\n                is_valid, error_msg = validate_batch_size(\n                    self.model, len(chunk_prompts), avg_tokens\n                )\n\n                if not is_valid:\n                    self.logger.warning(\n                        f\"Batch size validation failed: {error_msg}. \"\n                        f\"Consider reducing batch_size.\"\n                    )\n                else:\n                    self.logger.info(\n                        f\"Batch size validation passed: {len(chunk_prompts)} rows, \"\n                        f\"~{avg_tokens} tokens/row\"\n                    )\n\n            # Use strategy to format batch prompt\n            batch_prompt_text = self.strategy.format_batch(\n                chunk_prompts, metadata=metadata.model_dump()\n            )\n\n            # Create new PromptBatch with single mega-prompt\n            # Preserve ALL custom fields from original metadata (especially system_message for caching!)\n            original_custom = chunk_metadata[0].custom or {}\n\n            mega_metadata = RowMetadata(\n                row_index=chunk_metadata[0].row_index,\n                row_id=chunk_metadata[0].row_id,\n                custom={\n                    **original_custom,  # Preserve all custom fields (system_message, etc.)\n                    \"batch_metadata\": metadata.model_dump(),  # Batch-specific fields override\n                    \"is_batch\": True,\n                    \"batch_size\": len(chunk_prompts),\n                },\n            )\n\n            aggregated_batch = PromptBatch(\n                prompts=[batch_prompt_text],\n                metadata=[mega_metadata],\n                batch_id=batch.batch_id,\n            )\n\n            aggregated_batches.append(aggregated_batch)\n\n            # Update progress\n            processed_prompts += len(chunk_prompts)\n\n            # Hybrid progress: Log every 10% OR every 30 seconds (only for slow operations)\n            current_time = time.time()\n            elapsed = current_time - start_time\n\n            if total_prompts &gt; 10000 and elapsed &gt; 5:\n                current_pct = int((processed_prompts / total_prompts) * 100)\n\n                should_log = (\n                    current_pct &gt;= last_log_pct + 10 and current_pct &lt;= 90\n                ) or (current_time - last_log_time &gt;= 30)\n\n                if should_log:\n                    elapsed = current_time - start_time\n                    throughput = processed_prompts / elapsed if elapsed &gt; 0 else 0\n                    eta = (\n                        (total_prompts - processed_prompts) / throughput\n                        if throughput &gt; 0\n                        else 0\n                    )\n\n                    self.logger.info(\n                        f\"Aggregating: {current_pct}% ({processed_prompts:,}/{total_prompts:,}) | \"\n                        f\"ETA: {eta:.0f}s\"\n                    )\n                    last_log_time = current_time\n                    last_log_pct = current_pct\n\n    # Final summary\n    elapsed = time.time() - start_time\n    throughput = (\n        len(aggregated_batches) / elapsed\n        if elapsed &gt; 0\n        else len(aggregated_batches)\n    )\n    self.logger.info(\n        f\"\u2713 Created {len(aggregated_batches):,} mega-prompts in {elapsed:.1f}s \"\n        f\"({throughput:.0f} batches/s)\"\n    )\n\n    return aggregated_batches\n</code></pre>"},{"location":"api/stages/batch_aggregator_stage/#ondine.stages.batch_aggregator_stage.BatchAggregatorStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: list[PromptBatch]) -&gt; Any\n</code></pre> <p>Validate input batches.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[PromptBatch]</code> <p>List of PromptBatch objects</p> required <p>Returns:</p> Type Description <code>Any</code> <p>ValidationResult</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def validate_input(self, input_data: list[PromptBatch]) -&gt; Any:\n    \"\"\"Validate input batches.\n\n    Args:\n        input_data: List of PromptBatch objects\n\n    Returns:\n        ValidationResult\n    \"\"\"\n    from ondine.core.models import ValidationResult\n\n    if not input_data:\n        return ValidationResult(is_valid=False, errors=[\"No input batches\"])\n\n    return ValidationResult(is_valid=True)\n</code></pre>"},{"location":"api/stages/batch_aggregator_stage/#ondine.stages.batch_aggregator_stage.BatchAggregatorStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: list[PromptBatch], context: Any) -&gt; Any\n</code></pre> <p>Estimate cost for batch aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[PromptBatch]</code> <p>List of PromptBatch objects</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Any</code> <p>CostEstimate (zero cost - aggregation is free)</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def estimate_cost(self, input_data: list[PromptBatch], context: Any) -&gt; Any:\n    \"\"\"Estimate cost for batch aggregation.\n\n    Args:\n        input_data: List of PromptBatch objects\n        context: Execution context\n\n    Returns:\n        CostEstimate (zero cost - aggregation is free)\n    \"\"\"\n    from ondine.core.models import CostEstimate\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.prompts) for b in input_data),\n        confidence=\"actual\",\n    )\n</code></pre>"},{"location":"api/stages/batch_aggregator_stage/#ondine.stages.batch_aggregator_stage.BatchAggregatorStage.validate","title":"validate","text":"<pre><code>validate(context: Any) -&gt; bool\n</code></pre> <p>Validate stage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid</p> Source code in <code>ondine/stages/batch_aggregator_stage.py</code> <pre><code>def validate(self, context: Any) -&gt; bool:\n    \"\"\"Validate stage configuration.\n\n    Args:\n        context: Execution context\n\n    Returns:\n        True if valid\n\n    Raises:\n        ValueError: If configuration is invalid\n    \"\"\"\n    if self.batch_size &lt; 1:\n        raise ValueError(f\"batch_size must be &gt;= 1, got {self.batch_size}\")\n\n    if self.strategy is None:\n        raise ValueError(\"strategy cannot be None\")\n\n    return True\n</code></pre>"},{"location":"api/stages/batch_disaggregator_stage/","title":"batch_disaggregator_stage","text":""},{"location":"api/stages/batch_disaggregator_stage/#ondine.stages.batch_disaggregator_stage","title":"batch_disaggregator_stage","text":"<p>Batch disaggregator stage for multi-row processing.</p> <p>This stage splits batch responses back into individual responses, with support for partial extraction and row-by-row retry fallback.</p>"},{"location":"api/stages/batch_disaggregator_stage/#ondine.stages.batch_disaggregator_stage.BatchDisaggregatorStage","title":"BatchDisaggregatorStage","text":"<pre><code>BatchDisaggregatorStage(strategy: BatchFormattingStrategy | None = None, retry_failed_individually: bool = True, name: str = 'BatchDisaggregator')\n</code></pre> <p>               Bases: <code>PipelineStage</code></p> <p>Disaggregate batch responses into individual responses.</p> <p>Responsibility: - Parse batch response using strategy - Split into individual Response objects - Handle partial failures (retry failed rows) - Preserve row IDs and metadata</p> <p>Design Pattern: Strategy Pattern + Fallback - Delegates parsing to BatchFormattingStrategy - Implements retry logic for partial failures</p> <p>Initialize batch disaggregator stage.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>BatchFormattingStrategy | None</code> <p>Parsing strategy (defaults to JsonBatchStrategy)</p> <code>None</code> <code>retry_failed_individually</code> <code>bool</code> <p>Retry failed rows one-by-one</p> <code>True</code> <code>name</code> <code>str</code> <p>Stage name for logging</p> <code>'BatchDisaggregator'</code> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def __init__(\n    self,\n    strategy: BatchFormattingStrategy | None = None,\n    retry_failed_individually: bool = True,\n    name: str = \"BatchDisaggregator\",\n):\n    \"\"\"Initialize batch disaggregator stage.\n\n    Args:\n        strategy: Parsing strategy (defaults to JsonBatchStrategy)\n        retry_failed_individually: Retry failed rows one-by-one\n        name: Stage name for logging\n    \"\"\"\n    super().__init__(name=name)\n    self.strategy = strategy or JsonBatchStrategy()\n    self.retry_failed_individually = retry_failed_individually\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/batch_disaggregator_stage/#ondine.stages.batch_disaggregator_stage.BatchDisaggregatorStage.process","title":"process","text":"<pre><code>process(batches: list[ResponseBatch], context: Any) -&gt; list[ResponseBatch]\n</code></pre> <p>Disaggregate batch responses into individual responses.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>list[ResponseBatch]</code> <p>List of response batches (from LLMInvocationStage)</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>list[ResponseBatch]</code> <p>List of disaggregated response batches (N responses per batch)</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def process(\n    self, batches: list[ResponseBatch], context: Any\n) -&gt; list[ResponseBatch]:\n    \"\"\"Disaggregate batch responses into individual responses.\n\n    Args:\n        batches: List of response batches (from LLMInvocationStage)\n        context: Execution context\n\n    Returns:\n        List of disaggregated response batches (N responses per batch)\n    \"\"\"\n    disaggregated_batches = []\n    total_retries = 0\n\n    # Process each batch\n    for batch_idx, batch in enumerate(batches):\n        # Check if this batch contains aggregated responses\n        # In existing structure: batch.responses is list[str], batch.metadata is list[RowMetadata]\n        if (\n            not batch.metadata\n            or not batch.metadata[0].custom\n            or not batch.metadata[0].custom.get(\"is_batch\")\n        ):\n            # Not a batch response, pass through unchanged\n            disaggregated_batches.append(batch)\n            continue\n\n        # Extract batch metadata from first metadata entry\n        batch_metadata_dict = batch.metadata[0].custom.get(\"batch_metadata\", {})\n        batch_metadata = BatchMetadata(**batch_metadata_dict)\n\n        # Get the batch response text (first and only response in aggregated batch)\n        response_text = batch.responses[0]\n\n        # Get cost and latency from batch (will be split evenly)\n        batch_cost = batch.cost\n        batch_tokens = batch.tokens_used\n        batch_latency = batch.latencies_ms[0] if batch.latencies_ms else 0.0\n\n        # Parse batch response\n        try:\n            individual_results = self.strategy.parse_batch_response(\n                response_text,\n                expected_count=batch_metadata.original_count,\n                metadata=batch_metadata_dict,\n            )\n\n            # Create disaggregated batch with individual responses\n            disaggregated_batch = ResponseBatch(\n                responses=individual_results,\n                metadata=[\n                    RowMetadata(\n                        row_index=row_id,\n                        row_id=row_id,\n                        custom={\"from_batch\": True},\n                    )\n                    for row_id in batch_metadata.row_ids\n                ],\n                tokens_used=batch_tokens,\n                cost=batch_cost,\n                batch_id=batch.batch_id,\n                latencies_ms=[batch_latency / len(individual_results)]\n                * len(individual_results),\n            )\n            disaggregated_batches.append(disaggregated_batch)\n\n        except PartialParseError as e:\n            # Partial success - some results parsed, some failed\n            self.logger.warning(\n                f\"Partial parse: {len(e.parsed_results)}/{batch_metadata.original_count} \"\n                f\"results. Failed IDs: {e.failed_ids}\"\n            )\n            total_retries += len(e.failed_ids)\n\n            # Create responses with error markers for failed rows\n            individual_results = []\n            for i, row_id in enumerate(batch_metadata.row_ids):\n                if i + 1 in e.failed_ids:  # failed_ids are 1-based\n                    individual_results.append(\n                        f\"[PARSE_ERROR: Row {i + 1} not found in batch response]\"\n                    )\n                else:\n                    # Find the corresponding parsed result\n                    result_idx = i - sum(1 for fid in e.failed_ids if fid &lt;= i + 1)\n                    individual_results.append(e.parsed_results[result_idx])\n\n            disaggregated_batch = ResponseBatch(\n                responses=individual_results,\n                metadata=[\n                    RowMetadata(\n                        row_index=row_id,\n                        row_id=row_id,\n                        custom={\n                            \"from_batch\": True,\n                            \"parse_error\": i + 1 in e.failed_ids,\n                        },\n                    )\n                    for i, row_id in enumerate(batch_metadata.row_ids)\n                ],\n                tokens_used=batch_tokens,\n                cost=batch_cost,\n                batch_id=batch.batch_id,\n                latencies_ms=[batch_latency / len(individual_results)]\n                * len(individual_results),\n            )\n            disaggregated_batches.append(disaggregated_batch)\n\n        except Exception as e:\n            # Complete failure - couldn't parse batch at all\n            self.logger.error(\n                f\"Failed to parse batch response: {e}. \"\n                f\"Response: {response_text[:200]}\"\n            )\n\n            # Create error responses for all rows\n            error_responses = [\n                f\"[BATCH_PARSE_ERROR: {str(e)}]\" for _ in batch_metadata.row_ids\n            ]\n\n            disaggregated_batch = ResponseBatch(\n                responses=error_responses,\n                metadata=[\n                    RowMetadata(\n                        row_index=row_id,\n                        row_id=row_id,\n                        custom={\"parse_error\": True, \"batch_parse_failed\": True},\n                    )\n                    for row_id in batch_metadata.row_ids\n                ],\n                tokens_used=batch_tokens,\n                cost=batch_cost,\n                batch_id=batch.batch_id,\n                latencies_ms=[batch_latency / len(batch_metadata.row_ids)]\n                * len(batch_metadata.row_ids),\n            )\n            disaggregated_batches.append(disaggregated_batch)\n\n    if total_retries &gt; 0:\n        self.logger.info(f\"Total rows retried individually: {total_retries}\")\n\n    return disaggregated_batches\n</code></pre>"},{"location":"api/stages/batch_disaggregator_stage/#ondine.stages.batch_disaggregator_stage.BatchDisaggregatorStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: list[ResponseBatch]) -&gt; Any\n</code></pre> <p>Validate input batches.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[ResponseBatch]</code> <p>List of ResponseBatch objects</p> required <p>Returns:</p> Type Description <code>Any</code> <p>ValidationResult</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def validate_input(self, input_data: list[ResponseBatch]) -&gt; Any:\n    \"\"\"Validate input batches.\n\n    Args:\n        input_data: List of ResponseBatch objects\n\n    Returns:\n        ValidationResult\n    \"\"\"\n    from ondine.core.models import ValidationResult\n\n    if not input_data:\n        return ValidationResult(is_valid=False, errors=[\"No input batches\"])\n\n    return ValidationResult(is_valid=True)\n</code></pre>"},{"location":"api/stages/batch_disaggregator_stage/#ondine.stages.batch_disaggregator_stage.BatchDisaggregatorStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: list[ResponseBatch], context: Any) -&gt; Any\n</code></pre> <p>Estimate cost for batch disaggregation.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>list[ResponseBatch]</code> <p>List of ResponseBatch objects</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Any</code> <p>CostEstimate (zero cost - disaggregation is free)</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def estimate_cost(self, input_data: list[ResponseBatch], context: Any) -&gt; Any:\n    \"\"\"Estimate cost for batch disaggregation.\n\n    Args:\n        input_data: List of ResponseBatch objects\n        context: Execution context\n\n    Returns:\n        CostEstimate (zero cost - disaggregation is free)\n    \"\"\"\n    from decimal import Decimal\n\n    from ondine.core.models import CostEstimate\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.responses) for b in input_data),\n        confidence=\"actual\",\n    )\n</code></pre>"},{"location":"api/stages/batch_disaggregator_stage/#ondine.stages.batch_disaggregator_stage.BatchDisaggregatorStage.validate","title":"validate","text":"<pre><code>validate(context: Any) -&gt; bool\n</code></pre> <p>Validate stage configuration.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid</p> Source code in <code>ondine/stages/batch_disaggregator_stage.py</code> <pre><code>def validate(self, context: Any) -&gt; bool:\n    \"\"\"Validate stage configuration.\n\n    Args:\n        context: Execution context\n\n    Returns:\n        True if valid\n\n    Raises:\n        ValueError: If configuration is invalid\n    \"\"\"\n    if self.strategy is None:\n        raise ValueError(\"strategy cannot be None\")\n\n    return True\n</code></pre>"},{"location":"api/stages/data_loader_stage/","title":"data_loader_stage","text":""},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage","title":"data_loader_stage","text":"<p>Data loading stage for reading tabular data.</p>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage","title":"DataLoaderStage","text":"<pre><code>DataLoaderStage(dataframe: DataFrame | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[DatasetSpec, DataFrame]</code></p> <p>Load data from source and validate schema.</p> <p>Responsibilities: - Read data from configured source - Validate required columns exist - Apply any filters - Update context with row count</p> <p>Initialize data loader stage.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded dataframe (for DataFrame source)</p> <code>None</code> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"\n    Initialize data loader stage.\n\n    Args:\n        dataframe: Optional pre-loaded dataframe (for DataFrame source)\n    \"\"\"\n    super().__init__(\"DataLoader\")\n    self.dataframe = dataframe\n</code></pre>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage.process","title":"process","text":"<pre><code>process(spec: DatasetSpec, context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Load data from source.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def process(self, spec: DatasetSpec, context: Any) -&gt; pd.DataFrame:\n    \"\"\"Load data from source.\"\"\"\n    # Create appropriate reader\n    reader = create_data_reader(\n        source_type=spec.source_type,\n        source_path=spec.source_path,\n        dataframe=self.dataframe,\n        delimiter=spec.delimiter,\n        encoding=spec.encoding,\n        sheet_name=spec.sheet_name,\n    )\n\n    # Read data\n    df = reader.read()\n\n    # Validate columns exist\n    missing_cols = set(spec.input_columns) - set(df.columns)\n    if missing_cols:\n        raise ValueError(f\"Missing columns: {missing_cols}\")\n\n    # Apply filters if specified\n    if spec.filters:\n        for column, value in spec.filters.items():\n            if column in df.columns:\n                df = df[df[column] == value]\n\n    # Update context with total rows\n    context.total_rows = len(df)\n\n    self.logger.info(f\"Loaded {len(df)} rows from {spec.source_type}\")\n\n    return df\n</code></pre>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(spec: DatasetSpec) -&gt; ValidationResult\n</code></pre> <p>Validate dataset specification.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def validate_input(self, spec: DatasetSpec) -&gt; ValidationResult:\n    \"\"\"Validate dataset specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Check file exists for file sources\n    if spec.source_path and not spec.source_path.exists():\n        result.add_error(f\"Source file not found: {spec.source_path}\")\n\n    # Check input columns specified\n    if not spec.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    # Check output columns specified\n    if not spec.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(spec: DatasetSpec) -&gt; CostEstimate\n</code></pre> <p>Data loading has no LLM cost.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def estimate_cost(self, spec: DatasetSpec) -&gt; CostEstimate:\n    \"\"\"Data loading has no LLM cost.\"\"\"\n    # Try to determine row count if dataframe is available\n    row_count = 0\n    if self.dataframe is not None:\n        row_count = len(self.dataframe)\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=row_count,\n    )\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/","title":"llm_invocation_stage","text":""},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage","title":"llm_invocation_stage","text":"<p>LLM invocation stage with concurrency and retry logic.</p>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage","title":"LLMInvocationStage","text":"<pre><code>LLMInvocationStage(llm_client: LLMClient, concurrency: int = 5, rate_limiter: RateLimiter | None = None, retry_handler: RetryHandler | None = None, error_policy: ErrorPolicy = ErrorPolicy.SKIP, max_retries: int = 3, output_cls: type[BaseModel] | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[list[PromptBatch], list[ResponseBatch]]</code></p> <p>Invoke LLM with prompts using concurrency and retries.</p> <p>Responsibilities: - Execute LLM calls with rate limiting - Handle retries for transient failures - Track tokens and costs - Support concurrent processing</p> <p>Initialize LLM invocation stage.</p> <p>Parameters:</p> Name Type Description Default <code>llm_client</code> <code>LLMClient</code> <p>LLM client instance</p> required <code>concurrency</code> <code>int</code> <p>Max concurrent requests</p> <code>5</code> <code>rate_limiter</code> <code>RateLimiter | None</code> <p>Optional rate limiter</p> <code>None</code> <code>retry_handler</code> <code>RetryHandler | None</code> <p>Optional retry handler</p> <code>None</code> <code>error_policy</code> <code>ErrorPolicy</code> <p>Policy for handling errors</p> <code>SKIP</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>output_cls</code> <code>type[BaseModel] | None</code> <p>Optional Pydantic model for structured output</p> <code>None</code> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def __init__(\n    self,\n    llm_client: LLMClient,\n    concurrency: int = 5,\n    rate_limiter: RateLimiter | None = None,\n    retry_handler: RetryHandler | None = None,\n    error_policy: ErrorPolicy = ErrorPolicy.SKIP,\n    max_retries: int = 3,\n    output_cls: type[BaseModel] | None = None,\n):\n    \"\"\"\n    Initialize LLM invocation stage.\n\n    Args:\n        llm_client: LLM client instance\n        concurrency: Max concurrent requests\n        rate_limiter: Optional rate limiter\n        retry_handler: Optional retry handler\n        error_policy: Policy for handling errors\n        max_retries: Maximum retry attempts\n        output_cls: Optional Pydantic model for structured output\n    \"\"\"\n    super().__init__(\"LLMInvocation\")\n    self.llm_client = llm_client\n    self.concurrency = concurrency\n    self.rate_limiter = rate_limiter\n    self.retry_handler = retry_handler or RetryHandler()\n    self.output_cls = output_cls\n    self.error_handler = ErrorHandler(\n        policy=error_policy,\n        max_retries=max_retries,\n        default_value_factory=lambda: LLMResponse(\n            text=\"\",\n            tokens_in=0,\n            tokens_out=0,\n            model=llm_client.model,\n            cost=Decimal(\"0.0\"),\n            latency_ms=0.0,\n        ),\n    )\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage.process","title":"process","text":"<pre><code>process(batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]\n</code></pre> <p>Execute LLM calls for all prompt batches using flatten-then-concurrent pattern.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def process(self, batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]:\n    \"\"\"Execute LLM calls for all prompt batches using flatten-then-concurrent pattern.\"\"\"\n\n    # Initialize token tracking in context.intermediate_data (leverage existing design)\n    if \"token_tracking\" not in context.intermediate_data:\n        context.intermediate_data[\"token_tracking\"] = {\n            \"input_tokens\": 0,\n            \"output_tokens\": 0,\n        }\n\n    # Start progress tracking if available\n    progress_tracker = getattr(context, \"progress_tracker\", None)\n    progress_task = None\n    if progress_tracker:\n        total_prompts = sum(len(b.prompts) for b in batches)\n        progress_task = progress_tracker.start_stage(\n            f\"{self.name}: {context.total_rows} rows\",\n            total_rows=total_prompts,\n        )\n        # Store for access in concurrent loop\n        self._current_progress_task = progress_task\n\n    # Flatten all prompts from all batches\n    all_prompts, batch_map = self._flatten_batches(batches)\n\n    # Calculate total rows (handle both aggregated and non-aggregated batches)\n    total_rows = 0\n    for batch in batches:\n        if not batch.metadata:\n            continue\n        if (\n            batch.metadata\n            and batch.metadata[0].custom\n            and batch.metadata[0].custom.get(\"is_batch\")\n        ):\n            # Aggregated batch: use batch_size from metadata\n            total_rows += batch.metadata[0].custom.get(\n                \"batch_size\", len(batch.metadata)\n            )\n        else:\n            # Non-aggregated batch: count metadata entries\n            total_rows += len(batch.metadata)\n\n    self.logger.info(\n        f\"Processing {total_rows:,} rows in {len(batches)} API calls \"\n        f\"({self.concurrency} concurrent)\"\n    )\n\n    # Step 2: Process ALL prompts concurrently (ignore batch boundaries)\n    all_responses = self._process_all_prompts_concurrent(\n        all_prompts, context, batches\n    )\n\n    # Step 3: Reconstruct batches from flat responses\n    response_batches = self._reconstruct_batches(all_responses, batches, batch_map)\n\n    # Notify progress after processing\n    if hasattr(context, \"notify_progress\"):\n        context.notify_progress()\n\n    # Finish progress tracking\n    if progress_tracker and progress_task:\n        progress_tracker.finish(progress_task)\n\n    return response_batches\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(batches: list[PromptBatch]) -&gt; ValidationResult\n</code></pre> <p>Validate prompt batches.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def validate_input(self, batches: list[PromptBatch]) -&gt; ValidationResult:\n    \"\"\"Validate prompt batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    if not batches:\n        result.add_error(\"No prompt batches provided\")\n\n    for batch in batches:\n        if not batch.prompts:\n            result.add_error(f\"Batch {batch.batch_id} has no prompts\")\n\n        if len(batch.prompts) != len(batch.metadata):\n            result.add_error(f\"Batch {batch.batch_id} prompt/metadata mismatch\")\n\n    return result\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(batches: list[PromptBatch]) -&gt; CostEstimate\n</code></pre> <p>Estimate LLM invocation cost.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def estimate_cost(self, batches: list[PromptBatch]) -&gt; CostEstimate:\n    \"\"\"Estimate LLM invocation cost.\"\"\"\n    total_input_tokens = 0\n    total_output_tokens = 0\n\n    # Estimate tokens for all prompts\n    for batch in batches:\n        for prompt in batch.prompts:\n            input_tokens = self.llm_client.estimate_tokens(prompt)\n            total_input_tokens += input_tokens\n\n            # Assume average output length (can be made configurable)\n            estimated_output = int(input_tokens * 0.5)\n            total_output_tokens += estimated_output\n\n    total_cost = self.llm_client.calculate_cost(\n        total_input_tokens, total_output_tokens\n    )\n\n    return CostEstimate(\n        total_cost=total_cost,\n        total_tokens=total_input_tokens + total_output_tokens,\n        input_tokens=total_input_tokens,\n        output_tokens=total_output_tokens,\n        rows=sum(len(b.prompts) for b in batches),\n        confidence=\"estimate\",\n    )\n</code></pre>"},{"location":"api/stages/multi_run_stage/","title":"multi_run_stage","text":""},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage","title":"multi_run_stage","text":"<p>Multi-run stage for executing stages multiple times with aggregation.</p> <p>Implements Decorator pattern to wrap any stage and run it multiple times.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AggregationStrategy","title":"AggregationStrategy","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract base for aggregation strategies.</p> <p>Follows Strategy pattern for different ways to aggregate results.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AggregationStrategy.aggregate","title":"aggregate  <code>abstractmethod</code>","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Aggregate multiple results into one.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[T]</code> <p>List of results from multiple runs</p> required <p>Returns:</p> Type Description <code>T</code> <p>Aggregated result</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>@abstractmethod\ndef aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"\n    Aggregate multiple results into one.\n\n    Args:\n        results: List of results from multiple runs\n\n    Returns:\n        Aggregated result\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.ConsensusStrategy","title":"ConsensusStrategy","text":"<p>               Bases: <code>AggregationStrategy[str]</code></p> <p>Returns most common result (consensus voting).</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.ConsensusStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[str]) -&gt; str\n</code></pre> <p>Return most frequent result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[str]) -&gt; str:\n    \"\"\"Return most frequent result.\"\"\"\n    if not results:\n        return \"\"\n\n    # Count occurrences\n    counter = Counter(results)\n    return counter.most_common(1)[0][0]\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.FirstSuccessStrategy","title":"FirstSuccessStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns first successful (non-None) result.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.FirstSuccessStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Return first non-None result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"Return first non-None result.\"\"\"\n    for result in results:\n        if result is not None:\n            return result\n    return results[0] if results else None\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AllStrategy","title":"AllStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns all results as list (no aggregation).</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AllStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; list[T]\n</code></pre> <p>Return all results.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; list[T]:\n    \"\"\"Return all results.\"\"\"\n    return results\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AverageStrategy","title":"AverageStrategy","text":"<p>               Bases: <code>AggregationStrategy[float]</code></p> <p>Returns average of numeric results.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AverageStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[float]) -&gt; float\n</code></pre> <p>Return average.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[float]) -&gt; float:\n    \"\"\"Return average.\"\"\"\n    if not results:\n        return 0.0\n    return sum(results) / len(results)\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage","title":"MultiRunStage","text":"<pre><code>MultiRunStage(wrapped_stage: PipelineStage[TInput, TOutput], num_runs: int = 3, aggregation_strategy: AggregationStrategy | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[TInput, TOutput]</code></p> <p>Decorator stage that runs wrapped stage multiple times.</p> <p>Use cases: - Run LLM 3 times, take consensus (reduce hallucinations) - Retry until success - Collect multiple responses for analysis</p> Example <p>multi_llm = MultiRunStage(     wrapped=LLMInvocationStage(...),     num_runs=3,     aggregation=ConsensusStrategy() )</p> <p>Initialize multi-run stage.</p> <p>Parameters:</p> Name Type Description Default <code>wrapped_stage</code> <code>PipelineStage[TInput, TOutput]</code> <p>Stage to execute multiple times</p> required <code>num_runs</code> <code>int</code> <p>Number of times to run</p> <code>3</code> <code>aggregation_strategy</code> <code>AggregationStrategy | None</code> <p>Strategy for aggregating results</p> <code>None</code> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def __init__(\n    self,\n    wrapped_stage: PipelineStage[TInput, TOutput],\n    num_runs: int = 3,\n    aggregation_strategy: AggregationStrategy | None = None,\n):\n    \"\"\"\n    Initialize multi-run stage.\n\n    Args:\n        wrapped_stage: Stage to execute multiple times\n        num_runs: Number of times to run\n        aggregation_strategy: Strategy for aggregating results\n    \"\"\"\n    super().__init__(f\"MultiRun({wrapped_stage.name})\")\n    self.wrapped_stage = wrapped_stage\n    self.num_runs = num_runs\n    self.aggregation_strategy = aggregation_strategy or ConsensusStrategy()\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage.process","title":"process","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute wrapped stage multiple times and aggregate.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"Execute wrapped stage multiple times and aggregate.\"\"\"\n    results: list[TOutput] = []\n\n    self.logger.info(f\"Running {self.wrapped_stage.name} {self.num_runs} times\")\n\n    for run_num in range(self.num_runs):\n        try:\n            result = self.wrapped_stage.process(input_data, context)\n            results.append(result)\n        except Exception as e:\n            self.logger.error(f\"Run {run_num + 1}/{self.num_runs} failed: {e}\")\n            # Continue with other runs\n            continue\n\n    if not results:\n        raise RuntimeError(\n            f\"All {self.num_runs} runs failed for {self.wrapped_stage.name}\"\n        )\n\n    # Aggregate results\n    aggregated = self.aggregation_strategy.aggregate(results)\n\n    self.logger.info(\n        f\"Aggregated {len(results)} results using \"\n        f\"{self.aggregation_strategy.__class__.__name__}\"\n    )\n\n    return aggregated\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Delegate validation to wrapped stage.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"Delegate validation to wrapped stage.\"\"\"\n    return self.wrapped_stage.validate_input(input_data)\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate cost as num_runs \u00d7 wrapped stage cost.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"Estimate cost as num_runs \u00d7 wrapped stage cost.\"\"\"\n    single_run_cost = self.wrapped_stage.estimate_cost(input_data)\n\n    return CostEstimate(\n        total_cost=single_run_cost.total_cost * self.num_runs,\n        total_tokens=single_run_cost.total_tokens * self.num_runs,\n        input_tokens=single_run_cost.input_tokens * self.num_runs,\n        output_tokens=single_run_cost.output_tokens * self.num_runs,\n        rows=single_run_cost.rows,\n        confidence=f\"{single_run_cost.confidence} \u00d7 {self.num_runs} runs\",\n    )\n</code></pre>"},{"location":"api/stages/parser_factory/","title":"parser_factory","text":""},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory","title":"parser_factory","text":"<p>Factory for creating response parsers based on configuration.</p>"},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory.create_response_parser","title":"create_response_parser","text":"<pre><code>create_response_parser(prompt_spec: PromptSpec, output_columns: list[str]) -&gt; ResponseParser\n</code></pre> <p>Create appropriate response parser based on prompt specification.</p> <p>This factory enables configuration-driven parser selection, supporting: - Raw text output (default, backward compatible) - JSON structured output (for multiple fields) - Regex pattern extraction (for formatted text)</p> <p>Parameters:</p> Name Type Description Default <code>prompt_spec</code> <code>PromptSpec</code> <p>Prompt specification with response_format</p> required <code>output_columns</code> <code>list[str]</code> <p>Expected output column names</p> required <p>Returns:</p> Type Description <code>ResponseParser</code> <p>Configured ResponseParser instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If response_format is invalid or required config missing</p> Example Source code in <code>ondine/stages/parser_factory.py</code> <pre><code>def create_response_parser(\n    prompt_spec: PromptSpec,\n    output_columns: list[str],\n) -&gt; ResponseParser:\n    r\"\"\"\n    Create appropriate response parser based on prompt specification.\n\n    This factory enables configuration-driven parser selection, supporting:\n    - Raw text output (default, backward compatible)\n    - JSON structured output (for multiple fields)\n    - Regex pattern extraction (for formatted text)\n\n    Args:\n        prompt_spec: Prompt specification with response_format\n        output_columns: Expected output column names\n\n    Returns:\n        Configured ResponseParser instance\n\n    Raises:\n        ValueError: If response_format is invalid or required config missing\n\n    Example:\n        # JSON mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"json\",\n                json_fields=[\"score\", \"explanation\"]\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n\n        # Regex mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"regex\",\n                regex_patterns={\n                    \"score\": r\"SCORE:\\s*(\\d+)\",\n                    \"explanation\": r\"EXPLANATION:\\s*(.+)\"\n                }\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n    \"\"\"\n    response_format = prompt_spec.response_format.lower()\n\n    if response_format == \"json\":\n        logger.info(\"Creating JSONParser for multi-field extraction\")\n\n        # Validate JSON fields match output columns\n        if prompt_spec.json_fields:\n            json_set = set(prompt_spec.json_fields)\n            output_set = set(output_columns)\n            if json_set != output_set:\n                logger.warning(\n                    f\"JSON fields {json_set} don't match output columns {output_set}. \"\n                    f\"Using output_columns as authoritative.\"\n                )\n\n        return JSONParser(strict=False)\n\n    if response_format == \"regex\":\n        logger.info(\"Creating RegexParser for pattern-based extraction\")\n\n        if not prompt_spec.regex_patterns:\n            raise ValueError(\n                \"response_format='regex' requires regex_patterns to be specified\"\n            )\n\n        # Validate regex patterns cover all output columns\n        pattern_cols = set(prompt_spec.regex_patterns.keys())\n        output_set = set(output_columns)\n        missing = output_set - pattern_cols\n        if missing:\n            raise ValueError(f\"Missing regex patterns for output columns: {missing}\")\n\n        return RegexParser(patterns=prompt_spec.regex_patterns)\n\n    if response_format == \"raw\":\n        logger.info(\"Creating RawTextParser (default, backward compatible)\")\n\n        if len(output_columns) &gt; 1:\n            logger.warning(\n                f\"Using RawTextParser with {len(output_columns)} output columns. \"\n                f\"Only the first column will be populated. \"\n                f\"Consider using response_format='json' or 'regex' for multi-column output.\"\n            )\n\n        return RawTextParser()\n\n    # Should never reach here due to Pydantic validation\n    raise ValueError(\n        f\"Invalid response_format: '{response_format}'. \"\n        f\"Must be 'raw', 'json', or 'regex'.\"\n    )\n</code></pre>"},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory.create_response_parser--json-mode","title":"JSON mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"json\",         json_fields=[\"score\", \"explanation\"]     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory.create_response_parser--regex-mode","title":"Regex mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"regex\",         regex_patterns={             \"score\": r\"SCORE:\\s(\\d+)\",             \"explanation\": r\"EXPLANATION:\\s(.+)\"         }     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/pipeline_stage/","title":"pipeline_stage","text":""},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage","title":"pipeline_stage","text":"<p>Base pipeline stage abstraction.</p> <p>Defines the contract for all processing stages using Template Method pattern for execution flow.</p>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage","title":"PipelineStage","text":"<pre><code>PipelineStage(name: str)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[TInput, TOutput]</code></p> <p>Abstract base class for all pipeline stages.</p> <p>Implements Template Method pattern with hooks for extensibility. All stages follow Single Responsibility Principle and are composable via the Chain of Responsibility pattern.</p> <p>Stages in the pipeline: 1. DataLoaderStage - Load data from source 2. PromptFormatterStage - Format prompts with data 3. LLMInvocationStage - Call LLM API 4. ResponseParserStage - Parse LLM responses 5. ResultWriterStage - Write results to output</p> Example <pre><code>class CustomStage(PipelineStage[pd.DataFrame, pd.DataFrame]):\n    def process(self, input_data, context):\n        # Custom processing logic\n        return processed_data\n\n    def validate_input(self, input_data):\n        # Validation logic\n        return ValidationResult(is_valid=True)\n</code></pre> <p>Initialize pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable stage name</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize pipeline stage.\n\n    Args:\n        name: Human-readable stage name\n    \"\"\"\n    self.name = name\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.process","title":"process  <code>abstractmethod</code>","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Core processing logic (must be implemented by subclasses).</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data for this stage</p> required <code>context</code> <code>Any</code> <p>Execution context with shared state</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output data</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Core processing logic (must be implemented by subclasses).\n\n    Args:\n        input_data: Input data for this stage\n        context: Execution context with shared state\n\n    Returns:\n        Processed output data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.validate_input","title":"validate_input  <code>abstractmethod</code>","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Validate input before processing.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input to validate</p> required <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with errors/warnings</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"\n    Validate input before processing.\n\n    Args:\n        input_data: Input to validate\n\n    Returns:\n        ValidationResult with errors/warnings\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.execute","title":"execute","text":"<pre><code>execute(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute stage with pre/post hooks (Template Method).</p> <p>This method orchestrates the execution flow and should not be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input validation fails</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def execute(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Execute stage with pre/post hooks (Template Method).\n\n    This method orchestrates the execution flow and should not\n    be overridden.\n\n    Args:\n        input_data: Input data\n        context: Execution context\n\n    Returns:\n        Processed output\n\n    Raises:\n        ValueError: If input validation fails\n    \"\"\"\n    self.logger.info(f\"Starting stage: {self.name}\")\n\n    # Pre-processing hook\n    self.before_process(context)\n\n    # Validate input\n    validation = self.validate_input(input_data)\n    if not validation.is_valid:\n        error_msg = f\"Input validation failed: {validation.errors}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    if validation.warnings:\n        for warning in validation.warnings:\n            self.logger.warning(warning)\n\n    # Core processing\n    try:\n        result = self.process(input_data, context)\n        self.logger.info(f\"Completed stage: {self.name}\")\n\n        # Post-processing hook\n        self.after_process(result, context)\n\n        return result\n    except Exception as e:\n        self.logger.error(f\"Stage {self.name} failed: {e}\")\n        error_decision = self.on_error(e, context)\n        raise error_decision\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.before_process","title":"before_process","text":"<pre><code>before_process(context: Any) -&gt; None\n</code></pre> <p>Hook called before processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def before_process(self, context: Any) -&gt; None:\n    \"\"\"\n    Hook called before processing (default: no-op).\n\n    Args:\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.after_process","title":"after_process","text":"<pre><code>after_process(result: TOutput, context: Any) -&gt; None\n</code></pre> <p>Hook called after successful processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TOutput</code> <p>Processing result</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def after_process(self, result: TOutput, context: Any) -&gt; None:\n    \"\"\"\n    Hook called after successful processing (default: no-op).\n\n    Args:\n        result: Processing result\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.on_error","title":"on_error","text":"<pre><code>on_error(error: Exception, context: Any) -&gt; Exception\n</code></pre> <p>Hook called on processing error (default: re-raise).</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Exception</code> <p>Exception to raise (can transform error)</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def on_error(self, error: Exception, context: Any) -&gt; Exception:\n    \"\"\"\n    Hook called on processing error (default: re-raise).\n\n    Args:\n        error: The exception that occurred\n        context: Execution context\n\n    Returns:\n        Exception to raise (can transform error)\n    \"\"\"\n    return error\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.estimate_cost","title":"estimate_cost  <code>abstractmethod</code>","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate processing cost for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data to estimate for</p> required <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"\n    Estimate processing cost for this stage.\n\n    Args:\n        input_data: Input data to estimate for\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/","title":"prompt_formatter_stage","text":""},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage","title":"prompt_formatter_stage","text":"<p>Prompt formatting stage for template-based prompt generation.</p>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage","title":"PromptFormatterStage","text":"<pre><code>PromptFormatterStage(batch_size: int = 100, use_jinja2: bool = False)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, PromptSpec], list[PromptBatch]]</code></p> <p>Format prompts using template and row data.</p> <p>Responsibilities: - Extract input columns from rows - Format prompts using template - Batch prompts for efficient processing - Attach metadata for tracking</p> <p>Initialize prompt formatter stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of prompts per batch</p> <code>100</code> <code>use_jinja2</code> <code>bool</code> <p>Use Jinja2 for template rendering</p> <code>False</code> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def __init__(self, batch_size: int = 100, use_jinja2: bool = False):\n    \"\"\"\n    Initialize prompt formatter stage.\n\n    Args:\n        batch_size: Number of prompts per batch\n        use_jinja2: Use Jinja2 for template rendering\n    \"\"\"\n    super().__init__(\"PromptFormatter\")\n    self.batch_size = batch_size\n    self.use_jinja2 = use_jinja2\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, PromptSpec], context: Any) -&gt; list[PromptBatch]\n</code></pre> <p>Format prompts from DataFrame rows.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def process(\n    self, input_data: tuple[pd.DataFrame, PromptSpec], context: Any\n) -&gt; list[PromptBatch]:\n    \"\"\"Format prompts from DataFrame rows.\"\"\"\n    df, prompt_spec = input_data\n\n    prompts: list[str] = []\n    metadata_list: list[RowMetadata] = []\n\n    # Extract template variables and system message\n    template_str = prompt_spec.template\n    system_message = prompt_spec.system_message\n\n    # Create template renderer\n    if self.use_jinja2:\n        # Note: autoescape=False is intentional for LLM prompts (not HTML)\n        # We're generating text prompts, not web content, so HTML escaping\n        # would corrupt the prompt data sent to the LLM\n        template = Jinja2Template(template_str, autoescape=False)  # noqa: S701\n\n    # Format prompt for each row\n    # Performance optimization: Use itertuples() instead of iterrows() for 10\u00d7 speedup\n    # itertuples() returns namedtuples which are much faster than Series objects\n    import time\n\n    total_rows = len(df)\n    start_time = time.time()\n    last_log_time = start_time\n    last_log_pct = 0\n\n    self.logger.info(f\"Formatting {total_rows:,} prompts...\")\n\n    for row_count, row in enumerate(df.itertuples(index=True), 1):\n        # Hybrid progress: Log every 10% OR every 30 seconds (only for slow operations)\n        current_time = time.time()\n        current_pct = int((row_count / total_rows) * 100)\n        elapsed = current_time - start_time\n\n        # Only log progress if operation is taking &gt;5 seconds\n        should_log = elapsed &gt; 5 and (\n            (current_pct &gt;= last_log_pct + 10 and current_pct &lt;= 90)  # Every 10%\n            or (current_time - last_log_time &gt;= 30)  # OR every 30s\n        )\n\n        if should_log:\n            elapsed = current_time - start_time\n            throughput = row_count / elapsed if elapsed &gt; 0 else 0\n            eta = (total_rows - row_count) / throughput if throughput &gt; 0 else 0\n\n            self.logger.info(\n                f\"Formatting: {current_pct}% ({row_count:,}/{total_rows:,}) | \"\n                f\"{throughput:,.0f} rows/s | ETA: {eta:.0f}s\"\n            )\n            last_log_time = current_time\n            last_log_pct = current_pct\n\n        try:\n            # Extract index (first element of namedtuple)\n            idx = row[0]\n\n            # Extract input columns from namedtuple\n            # Build row_data dict from column names and namedtuple attributes\n            row_data = {}\n            for col in df.columns:\n                if col in template_str:\n                    # Get attribute by column name (namedtuples have column names as attributes)\n                    row_data[col] = getattr(row, col)\n\n            # Format prompt (Jinja2 or f-string)\n            if self.use_jinja2:\n                prompt = template.render(**row_data)\n            else:\n                prompt = template_str.format(**row_data)\n\n            # Add few-shot examples if specified (but NOT system message)\n            if prompt_spec.few_shot_examples:\n                examples_text = self._format_few_shot_examples(\n                    prompt_spec.few_shot_examples\n                )\n                prompt = f\"{examples_text}\\n\\n{prompt}\"\n\n            # NOTE: Do NOT add system message to prompt here\n            # It will be passed separately via metadata for caching optimization\n\n            prompts.append(prompt)\n\n            # Create metadata with system message for LLM stage\n            # Get 'id' column if it exists\n            row_id = getattr(row, \"id\", None) if hasattr(row, \"id\") else None\n            metadata = RowMetadata(\n                row_index=idx,\n                row_id=row_id,\n                custom={\"system_message\": system_message}\n                if system_message\n                else None,\n            )\n            metadata_list.append(metadata)\n\n        except (KeyError, AttributeError) as e:\n            self.logger.warning(f\"Missing template variable at row {idx}: {e}\")\n            continue\n        except Exception as e:\n            self.logger.error(f\"Error formatting prompt at row {idx}: {e}\")\n            continue\n\n    # Create batches\n    batches: list[PromptBatch] = []\n    for i in range(0, len(prompts), self.batch_size):\n        batch_prompts = prompts[i : i + self.batch_size]\n        batch_metadata = metadata_list[i : i + self.batch_size]\n\n        batch = PromptBatch(\n            prompts=batch_prompts,\n            metadata=batch_metadata,\n            batch_id=i // self.batch_size,\n        )\n        batches.append(batch)\n\n    # Final summary\n    total_time = time.time() - start_time\n    throughput = len(prompts) / total_time if total_time &gt; 0 else 0\n    self.logger.info(\n        f\"\u2713 Formatted {len(prompts):,} prompts in {total_time:.1f}s ({throughput:,.0f} rows/s)\"\n    )\n\n    return batches\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, PromptSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate DataFrame and prompt specification.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; ValidationResult:\n    \"\"\"Validate DataFrame and prompt specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    df, prompt_spec = input_data\n\n    # Check DataFrame not empty\n    if df.empty:\n        result.add_error(\"DataFrame is empty\")\n\n    # Check template variables exist in DataFrame\n    template = prompt_spec.template\n    import re\n\n    variables = re.findall(r\"\\{(\\w+)\\}\", template)\n    missing_vars = set(variables) - set(df.columns)\n\n    if missing_vars:\n        result.add_error(f\"Template variables not in DataFrame: {missing_vars}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, PromptSpec]) -&gt; CostEstimate\n</code></pre> <p>Prompt formatting has no LLM cost.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; CostEstimate:\n    \"\"\"Prompt formatting has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/response_parser_stage/","title":"response_parser_stage","text":""},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage","title":"response_parser_stage","text":"<p>Response parsing stage for structured output extraction.</p>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParser","title":"ResponseParser","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for response parsers (Strategy pattern).</p>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParser.parse","title":"parse  <code>abstractmethod</code>","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse response into structured data.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>@abstractmethod\ndef parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse response into structured data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RawTextParser","title":"RawTextParser","text":"<p>               Bases: <code>ResponseParser</code></p> <p>Parser that returns raw text.</p>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RawTextParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Return response as-is, after cleaning chat format artifacts.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Return response as-is, after cleaning chat format artifacts.\"\"\"\n    cleaned = response.strip()\n\n    # Strip common chat format prefixes (assistant:, user:, system:)\n    for prefix in [\"assistant:\", \"user:\", \"system:\"]:\n        if cleaned.lower().startswith(prefix):\n            cleaned = cleaned[len(prefix) :].strip()\n            break\n\n    return {\"output\": cleaned}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.JSONParser","title":"JSONParser","text":"<pre><code>JSONParser(strict: bool = False)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts JSON from response.</p> <p>Initialize JSON parser.</p> <p>Parameters:</p> Name Type Description Default <code>strict</code> <code>bool</code> <p>If True, fail on invalid JSON; if False, try to extract</p> <code>False</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, strict: bool = False):\n    \"\"\"\n    Initialize JSON parser.\n\n    Args:\n        strict: If True, fail on invalid JSON; if False, try to extract\n    \"\"\"\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.JSONParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse JSON from response.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse JSON from response.\"\"\"\n    try:\n        return json.loads(response.strip())\n    except json.JSONDecodeError:\n        if self.strict:\n            raise\n\n        # Try to extract JSON from markdown code blocks\n        if \"```json\" in response:\n            start = response.find(\"```json\") + 7\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        if \"```\" in response:\n            start = response.find(\"```\") + 3\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        # Return as raw text if can't parse\n        return {\"output\": response.strip()}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.PydanticParser","title":"PydanticParser","text":"<pre><code>PydanticParser(model: type[BaseModel], strict: bool = True)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that validates responses against Pydantic models.</p> <p>Provides type-safe extraction with automatic validation.</p> <p>Initialize Pydantic parser.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>type[BaseModel]</code> <p>Pydantic model class for validation</p> required <code>strict</code> <code>bool</code> <p>If True, fail on validation errors</p> <code>True</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, model: type[BaseModel], strict: bool = True):\n    \"\"\"\n    Initialize Pydantic parser.\n\n    Args:\n        model: Pydantic model class for validation\n        strict: If True, fail on validation errors\n    \"\"\"\n    self.model = model\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.PydanticParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; BaseModel\n</code></pre> <p>Parse and validate response with Pydantic model.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; BaseModel:\n    \"\"\"Parse and validate response with Pydantic model.\"\"\"\n    try:\n        # Try to parse as JSON first\n        json_parser = JSONParser(strict=False)\n        data = json_parser.parse(response)\n\n        # Validate with Pydantic and return the model instance\n        return self.model(**data)\n\n    except ValidationError as e:\n        if self.strict:\n            raise ValueError(f\"Pydantic validation failed: {e}\")\n        # Return raw data if validation fails\n        return {\"output\": response.strip(), \"validation_error\": str(e)}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RegexParser","title":"RegexParser","text":"<pre><code>RegexParser(patterns: dict[str, str])\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts data using regex patterns.</p> <p>Useful for extracting specific fields from structured text.</p> <p>Initialize regex parser.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>dict[str, str]</code> <p>Dict mapping field names to regex patterns</p> required Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, patterns: dict[str, str]):\n    \"\"\"\n    Initialize regex parser.\n\n    Args:\n        patterns: Dict mapping field names to regex patterns\n    \"\"\"\n    self.patterns = {key: re.compile(pattern) for key, pattern in patterns.items()}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RegexParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Extract fields using regex patterns.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Extract fields using regex patterns.\"\"\"\n    result = {}\n\n    for field_name, pattern in self.patterns.items():\n        match = pattern.search(response)\n        if match:\n            # Use first group if groups exist, else full match\n            if match.groups():\n                result[field_name] = match.group(1)\n            else:\n                result[field_name] = match.group(0)\n        else:\n            result[field_name] = None\n\n    return result\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage","title":"ResponseParserStage","text":"<pre><code>ResponseParserStage(parser: ResponseParser | None = None, output_columns: list[str] | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[list[ResponseBatch], list[str]], DataFrame]</code></p> <p>Parse LLM responses into structured DataFrame.</p> <p>Responsibilities: - Parse responses using configured parser - Map parsed data to output columns - Handle parse errors gracefully - Return DataFrame with results</p> <p>Initialize response parser stage.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ResponseParser | None</code> <p>Response parser (default: RawTextParser)</p> <code>None</code> <code>output_columns</code> <code>list[str] | None</code> <p>Output column names</p> <code>None</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(\n    self,\n    parser: ResponseParser | None = None,\n    output_columns: list[str] | None = None,\n):\n    \"\"\"\n    Initialize response parser stage.\n\n    Args:\n        parser: Response parser (default: RawTextParser)\n        output_columns: Output column names\n    \"\"\"\n    super().__init__(\"ResponseParser\")\n    self.parser = parser or RawTextParser()\n    self.output_columns = output_columns or [\"output\"]\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage.process","title":"process","text":"<pre><code>process(input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Parse responses into DataFrame.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Parse responses into DataFrame.\"\"\"\n    # Handle both tuple (batches, output_cols) and list [batches] for backward compatibility\n    if isinstance(input_data, tuple):\n        batches, output_cols = input_data\n        # Use output_cols from input_data (overrides self.output_columns if provided)\n        if not output_cols:\n            output_cols = self.output_columns\n    else:\n        # Backward compatibility: input_data is just the list of batches\n        batches = input_data\n        output_cols = self.output_columns\n\n    # Initialize result storage\n    results: dict[int, dict[str, Any]] = {}\n\n    # Parse all responses\n    for batch in batches:\n        for response, metadata in zip(\n            batch.responses, batch.metadata, strict=False\n        ):\n            try:\n                # Parse response text\n                response_text = (\n                    response.text if hasattr(response, \"text\") else str(response)\n                )\n                parsed = self.parser.parse(response_text)\n\n                # Map to output columns\n                row_data = {}\n                if len(output_cols) == 1:\n                    # Single output column\n                    if isinstance(parsed, dict) and \"output\" in parsed:\n                        row_data[output_cols[0]] = parsed[\"output\"]\n                    elif isinstance(parsed, dict):\n                        # Use first value\n                        row_data[output_cols[0]] = next(iter(parsed.values()))\n                    else:\n                        row_data[output_cols[0]] = parsed\n                else:\n                    # Multiple output columns\n                    for col in output_cols:\n                        row_data[col] = parsed.get(col, None)\n\n                results[metadata.row_index] = row_data\n\n            except Exception as e:\n                self.logger.error(\n                    f\"Failed to parse response at row {metadata.row_index}: {e}\"\n                )\n                # Store None for failed parses\n                results[metadata.row_index] = {col: None for col in output_cols}\n\n    # Create DataFrame\n    df = pd.DataFrame.from_dict(results, orient=\"index\")\n    df.index.name = \"row_index\"\n\n    self.logger.info(f\"Parsed {len(results)} responses\")\n\n    return df\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; ValidationResult\n</code></pre> <p>Validate response batches.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; ValidationResult:\n    \"\"\"Validate response batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    batches, output_cols = input_data\n\n    if not batches:\n        result.add_error(\"No response batches provided\")\n\n    if not output_cols:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; CostEstimate\n</code></pre> <p>Response parsing has no LLM cost.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; CostEstimate:\n    \"\"\"Response parsing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.responses) for b in input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/result_writer_stage/","title":"result_writer_stage","text":""},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage","title":"result_writer_stage","text":"<p>Result writing stage for persisting output.</p>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage","title":"ResultWriterStage","text":"<pre><code>ResultWriterStage()\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, DataFrame, OutputSpec], DataFrame]</code></p> <p>Write results to destination with merge support.</p> <p>Responsibilities: - Merge results with original data - Write to configured destination - Support atomic writes - Return merged DataFrame</p> <p>Initialize result writer stage.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize result writer stage.\"\"\"\n    super().__init__(\"ResultWriter\")\n</code></pre>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, DataFrame, OutputSpec], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Write results to destination and return merged DataFrame.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Write results to destination and return merged DataFrame.\"\"\"\n    original_df, results_df, output_spec = input_data\n\n    # Merge results with original data\n    merged_df = self._merge_results(\n        original_df, results_df, output_spec.merge_strategy\n    )\n\n    # Write to destination\n    if output_spec.destination_path:\n        writer = create_data_writer(output_spec.destination_type)\n\n        if output_spec.atomic_write:\n            confirmation = writer.atomic_write(\n                merged_df, output_spec.destination_path\n            )\n        else:\n            confirmation = writer.write(merged_df, output_spec.destination_path)\n\n        self.logger.info(\n            f\"Wrote {confirmation.rows_written} rows to {confirmation.path}\"\n        )\n\n    # Always return the merged DataFrame (needed for quality validation)\n    return merged_df\n</code></pre>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate input data and output specification.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def validate_input(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; ValidationResult:\n    \"\"\"Validate input data and output specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    original_df, results_df, output_spec = input_data\n\n    if original_df.empty:\n        result.add_warning(\"Original DataFrame is empty\")\n\n    if results_df.empty:\n        result.add_error(\"Results DataFrame is empty\")\n\n    # Check destination path if specified\n    if output_spec.destination_path:\n        dest_dir = output_spec.destination_path.parent\n        if not dest_dir.exists():\n            result.add_warning(f\"Destination directory does not exist: {dest_dir}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; CostEstimate\n</code></pre> <p>Result writing has no LLM cost.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def estimate_cost(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; CostEstimate:\n    \"\"\"Result writing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[1]),\n    )\n</code></pre>"},{"location":"api/stages/stage_registry/","title":"stage_registry","text":""},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry","title":"stage_registry","text":"<p>Stage registry for extensible pipeline stage plugins.</p> <p>Enables custom pipeline stages to be registered and injected into processing pipelines without modifying core code.</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.PipelineStage","title":"PipelineStage","text":"<p>Protocol for pipeline stage implementations (imported to avoid circular dependency).</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry","title":"StageRegistry","text":"<p>Global registry for custom pipeline stages.</p> <p>Enables registration and discovery of custom stages that can be injected into pipelines at specific positions.</p> Example"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry--register-custom-stage","title":"Register custom stage","text":"<p>@StageRegistry.register(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     def execute(self, context):         # Custom retrieval logic         ...</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline = (     PipelineBuilder.create()     .with_stage(\"rag_retrieval\", position=\"before_prompt\", index=\"my-docs\")     .build() )</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(stage_name: str, stage_class: type) -&gt; type\n</code></pre> <p>Register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")</p> required <code>stage_class</code> <code>type</code> <p>Stage class implementing PipelineStage interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered stage class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name already registered</p> Example <p>@StageRegistry.register(\"fact_checker\") class FactCheckerStage(PipelineStage):     def execute(self, context):         # Verify LLM output against sources         ...</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef register(cls, stage_name: str, stage_class: type) -&gt; type:\n    \"\"\"\n    Register a custom pipeline stage.\n\n    Args:\n        stage_name: Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")\n        stage_class: Stage class implementing PipelineStage interface\n\n    Returns:\n        The registered stage class (enables use as decorator)\n\n    Raises:\n        ValueError: If stage_name already registered\n\n    Example:\n        @StageRegistry.register(\"fact_checker\")\n        class FactCheckerStage(PipelineStage):\n            def execute(self, context):\n                # Verify LLM output against sources\n                ...\n    \"\"\"\n    if stage_name in cls._stages:\n        raise ValueError(\n            f\"Stage '{stage_name}' already registered. \"\n            f\"Use a different stage_name or unregister first.\"\n        )\n\n    cls._stages[stage_name] = stage_class\n    return stage_class\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(stage_name: str) -&gt; type\n</code></pre> <p>Get stage class by name.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>Pipeline stage class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Example <p>stage_class = StageRegistry.get(\"rag_retrieval\") stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef get(cls, stage_name: str) -&gt; type:\n    \"\"\"\n    Get stage class by name.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        Pipeline stage class\n\n    Raises:\n        ValueError: If stage not found\n\n    Example:\n        stage_class = StageRegistry.get(\"rag_retrieval\")\n        stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")\n    \"\"\"\n    if stage_name not in cls._stages:\n        available = \", \".join(sorted(cls._stages.keys()))\n        raise ValueError(\n            f\"Unknown stage: '{stage_name}'. \"\n            f\"Available stages: {available if available else 'none'}\"\n        )\n\n    return cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.list_stages","title":"list_stages  <code>classmethod</code>","text":"<pre><code>list_stages() -&gt; dict[str, type]\n</code></pre> <p>List all registered stages.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping stage names to stage classes</p> Example <p>stages = StageRegistry.list_stages() print(f\"Available custom stages: {list(stages.keys())}\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef list_stages(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered stages.\n\n    Returns:\n        Dictionary mapping stage names to stage classes\n\n    Example:\n        stages = StageRegistry.list_stages()\n        print(f\"Available custom stages: {list(stages.keys())}\")\n    \"\"\"\n    return cls._stages.copy()\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(stage_name: str) -&gt; bool\n</code></pre> <p>Check if stage is registered.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, stage_name: str) -&gt; bool:\n    \"\"\"\n    Check if stage is registered.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    return stage_name in cls._stages\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(stage_name: str) -&gt; None\n</code></pre> <p>Unregister a stage (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, stage_name: str) -&gt; None:\n    \"\"\"\n    Unregister a stage (mainly for testing).\n\n    Args:\n        stage_name: Stage identifier\n\n    Raises:\n        ValueError: If stage not found\n    \"\"\"\n    if stage_name not in cls._stages:\n        raise ValueError(f\"Stage '{stage_name}' not registered\")\n\n    del cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.stage","title":"stage","text":"<pre><code>stage(name: str)\n</code></pre> <p>Decorator to register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique stage identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@stage(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     '''Retrieve context from vector store and enrich data.'''</p> <pre><code>def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n    super().__init__(name=\"rag_retrieval\")\n    self.vector_store = vector_store\n    self.index_name = index_name\n    self.top_k = top_k\n\ndef execute(self, context: ExecutionContext) -&gt; StageResult:\n    # Retrieve context for each row\n    enriched_rows = []\n    for _, row in context.data.iterrows():\n        query = row['text']\n        results = self._retrieve(query)\n        row['retrieved_context'] = self._format_context(results)\n        enriched_rows.append(row)\n\n    context.data = pd.DataFrame(enriched_rows)\n    return StageResult(success=True, data=context.data)\n\ndef _retrieve(self, query: str):\n    # Integration with vector store\n    ...\n\ndef _format_context(self, results):\n    # Format retrieved docs\n    ...\n</code></pre> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>def stage(name: str):\n    \"\"\"\n    Decorator to register a custom pipeline stage.\n\n    Args:\n        name: Unique stage identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @stage(\"rag_retrieval\")\n        class RAGRetrievalStage(PipelineStage):\n            '''Retrieve context from vector store and enrich data.'''\n\n            def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n                super().__init__(name=\"rag_retrieval\")\n                self.vector_store = vector_store\n                self.index_name = index_name\n                self.top_k = top_k\n\n            def execute(self, context: ExecutionContext) -&gt; StageResult:\n                # Retrieve context for each row\n                enriched_rows = []\n                for _, row in context.data.iterrows():\n                    query = row['text']\n                    results = self._retrieve(query)\n                    row['retrieved_context'] = self._format_context(results)\n                    enriched_rows.append(row)\n\n                context.data = pd.DataFrame(enriched_rows)\n                return StageResult(success=True, data=context.data)\n\n            def _retrieve(self, query: str):\n                # Integration with vector store\n                ...\n\n            def _format_context(self, results):\n                # Format retrieved docs\n                ...\n    \"\"\"\n\n    def decorator(cls):\n        StageRegistry.register(name, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/","title":"streaming_loader_stage","text":""},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage","title":"streaming_loader_stage","text":"<p>Streaming data loader for memory-efficient processing of large files.</p> <p>Implements streaming pattern for datasets that don't fit in memory.</p>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage","title":"StreamingDataLoaderStage","text":"<pre><code>StreamingDataLoaderStage(chunk_size: int = 1000)\n</code></pre> <p>               Bases: <code>PipelineStage[DatasetSpec, Iterator[DataFrame]]</code></p> <p>Load data in chunks for memory-efficient processing.</p> <p>Use this for very large datasets (100K+ rows) that don't fit in memory.</p> <p>Initialize streaming data loader.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def __init__(self, chunk_size: int = 1000):\n    \"\"\"\n    Initialize streaming data loader.\n\n    Args:\n        chunk_size: Number of rows per chunk\n    \"\"\"\n    super().__init__(\"StreamingDataLoader\")\n    self.chunk_size = chunk_size\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage.process","title":"process","text":"<pre><code>process(spec: DatasetSpec, context: Any) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Load data as iterator of chunks.</p> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def process(self, spec: DatasetSpec, context: Any) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Load data as iterator of chunks.\"\"\"\n    # Create appropriate reader\n    reader = create_data_reader(\n        source_type=spec.source_type,\n        source_path=spec.source_path,\n        delimiter=spec.delimiter,\n        encoding=spec.encoding,\n        sheet_name=spec.sheet_name,\n    )\n\n    self.logger.info(f\"Streaming data in chunks of {self.chunk_size} rows\")\n\n    # Return chunked iterator\n    return reader.read_chunked(self.chunk_size)\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(spec: DatasetSpec) -&gt; ValidationResult\n</code></pre> <p>Validate dataset specification.</p> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def validate_input(self, spec: DatasetSpec) -&gt; ValidationResult:\n    \"\"\"Validate dataset specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Check file exists for file sources\n    if spec.source_path and not spec.source_path.exists():\n        result.add_error(f\"Source file not found: {spec.source_path}\")\n\n    # Check columns specified\n    if not spec.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(spec: DatasetSpec) -&gt; CostEstimate\n</code></pre> <p>Streaming has no direct LLM cost.</p> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def estimate_cost(self, spec: DatasetSpec) -&gt; CostEstimate:\n    \"\"\"Streaming has no direct LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=0,  # Unknown until streaming starts\n    )\n</code></pre>"},{"location":"api/strategies/","title":"strategies","text":""},{"location":"api/strategies/#ondine.strategies","title":"strategies","text":"<p>Batch formatting strategies for multi-row processing.</p>"},{"location":"api/strategies/#ondine.strategies.BatchFormattingStrategy","title":"BatchFormattingStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract strategy for batch prompt formatting and response parsing.</p> <p>This interface defines the contract for batch processing strategies. Implementations must handle: - Formatting N prompts into 1 batch prompt - Parsing 1 batch response into N individual results - Partial failure handling (some results parsed, some failed)</p> <p>Design Pattern: Strategy Pattern - Allows different formatting approaches (JSON, CSV, XML) - Stages depend on this abstraction (Dependency Inversion) - New strategies can be added without modifying stages (Open/Closed)</p>"},{"location":"api/strategies/#ondine.strategies.BatchFormattingStrategy.format_batch","title":"format_batch  <code>abstractmethod</code>","text":"<pre><code>format_batch(prompts: list[str], metadata: dict[str, Any] | None = None) -&gt; str\n</code></pre> <p>Format multiple prompts into a single batch prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of individual prompts to batch together</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata (e.g., row IDs, column names)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Single batch prompt containing all inputs</p> Example <p>prompts = [\"Classify: Product A\", \"Classify: Product B\"] result = strategy.format_batch(prompts)</p> Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>@abstractmethod\ndef format_batch(\n    self,\n    prompts: list[str],\n    metadata: dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"Format multiple prompts into a single batch prompt.\n\n    Args:\n        prompts: List of individual prompts to batch together\n        metadata: Optional metadata (e.g., row IDs, column names)\n\n    Returns:\n        Single batch prompt containing all inputs\n\n    Example:\n        prompts = [\"Classify: Product A\", \"Classify: Product B\"]\n        result = strategy.format_batch(prompts)\n        # Returns: \"Classify these 2 items: 1. Product A, 2. Product B...\"\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchFormattingStrategy.format_batch--returns-classify-these-2-items-1-product-a-2-product-b","title":"Returns: \"Classify these 2 items: 1. Product A, 2. Product B...\"","text":""},{"location":"api/strategies/#ondine.strategies.BatchFormattingStrategy.parse_batch_response","title":"parse_batch_response  <code>abstractmethod</code>","text":"<pre><code>parse_batch_response(response: str, expected_count: int, metadata: dict[str, Any] | None = None) -&gt; list[str]\n</code></pre> <p>Parse batch response into individual results.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>Single batch response from LLM</p> required <code>expected_count</code> <code>int</code> <p>Expected number of results</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata from format_batch</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of individual results (length must equal expected_count)</p> <p>Raises:</p> Type Description <code>PartialParseError</code> <p>If some results parsed but not all</p> <code>ValueError</code> <p>If response format is completely invalid</p> Example <p>response = '[{\"id\": 1, \"result\": \"positive\"}, {\"id\": 2, \"result\": \"negative\"}]' results = strategy.parse_batch_response(response, expected_count=2)</p> Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>@abstractmethod\ndef parse_batch_response(\n    self,\n    response: str,\n    expected_count: int,\n    metadata: dict[str, Any] | None = None,\n) -&gt; list[str]:\n    \"\"\"Parse batch response into individual results.\n\n    Args:\n        response: Single batch response from LLM\n        expected_count: Expected number of results\n        metadata: Optional metadata from format_batch\n\n    Returns:\n        List of individual results (length must equal expected_count)\n\n    Raises:\n        PartialParseError: If some results parsed but not all\n        ValueError: If response format is completely invalid\n\n    Example:\n        response = '[{\"id\": 1, \"result\": \"positive\"}, {\"id\": 2, \"result\": \"negative\"}]'\n        results = strategy.parse_batch_response(response, expected_count=2)\n        # Returns: [\"positive\", \"negative\"]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchFormattingStrategy.parse_batch_response--returns-positive-negative","title":"Returns: [\"positive\", \"negative\"]","text":""},{"location":"api/strategies/#ondine.strategies.BatchFormattingStrategy.estimate_batch_tokens","title":"estimate_batch_tokens","text":"<pre><code>estimate_batch_tokens(prompts: list[str], tokenizer: Any) -&gt; int\n</code></pre> <p>Estimate total tokens for batch prompt (optional override).</p> <p>Default implementation: Sum individual prompt tokens + overhead.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of prompts to batch</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer for counting (e.g., tiktoken)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count for batch prompt</p> Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>def estimate_batch_tokens(\n    self,\n    prompts: list[str],\n    tokenizer: Any,\n) -&gt; int:\n    \"\"\"Estimate total tokens for batch prompt (optional override).\n\n    Default implementation: Sum individual prompt tokens + overhead.\n\n    Args:\n        prompts: List of prompts to batch\n        tokenizer: Tokenizer for counting (e.g., tiktoken)\n\n    Returns:\n        Estimated token count for batch prompt\n    \"\"\"\n    # Default: sum of individual prompts + 10% overhead for formatting\n    total = sum(len(tokenizer.encode(p)) for p in prompts)\n    return int(total * 1.1)\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.PartialParseError","title":"PartialParseError","text":"<pre><code>PartialParseError(message: str, parsed_results: list[str], failed_ids: list[int], original_response: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Raised when batch response is partially parsed.</p> <p>Attributes:</p> Name Type Description <code>parsed_results</code> <p>Successfully parsed results</p> <code>failed_ids</code> <p>IDs of rows that failed to parse</p> <code>original_response</code> <p>The original response text</p> <p>Initialize partial parse error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description</p> required <code>parsed_results</code> <code>list[str]</code> <p>Successfully parsed results</p> required <code>failed_ids</code> <code>list[int]</code> <p>IDs of rows that failed to parse</p> required <code>original_response</code> <code>str</code> <p>The original response text</p> required Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    parsed_results: list[str],\n    failed_ids: list[int],\n    original_response: str,\n):\n    \"\"\"Initialize partial parse error.\n\n    Args:\n        message: Error description\n        parsed_results: Successfully parsed results\n        failed_ids: IDs of rows that failed to parse\n        original_response: The original response text\n    \"\"\"\n    super().__init__(message)\n    self.parsed_results = parsed_results\n    self.failed_ids = failed_ids\n    self.original_response = original_response\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.JsonBatchStrategy","title":"JsonBatchStrategy","text":"<pre><code>JsonBatchStrategy()\n</code></pre> <p>               Bases: <code>BatchFormattingStrategy</code></p> <p>JSON-based batch formatting strategy.</p> <p>Formats prompts as JSON array and parses responses using LlamaIndex's structured_predict() for reliable Pydantic-validated parsing.</p> <p>Design: - Uses structured output (Pydantic models) for type safety - Leverages LlamaIndex for automatic retry on malformed JSON - Supports partial extraction (parse what works, report failures)</p> <p>Initialize JSON batch strategy.</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize JSON batch strategy.\"\"\"\n    self.logger = get_logger(f\"{__name__}.JsonBatchStrategy\")\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.JsonBatchStrategy.format_batch","title":"format_batch","text":"<pre><code>format_batch(prompts: list[str], metadata: dict[str, Any] | None = None) -&gt; str\n</code></pre> <p>Format multiple prompts as JSON array.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of prompts to batch</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata (row_ids, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON-formatted batch prompt with instructions</p> Example <p>Input: [\"Classify: Product A\", \"Classify: Product B\"] Output: ''' Process these 2 items and return a JSON array:</p> <p>INPUT: [   {\"id\": 1, \"input\": \"Classify: Product A\"},   {\"id\": 2, \"input\": \"Classify: Product B\"} ]</p> <p>OUTPUT FORMAT (return ONLY this JSON, nothing else): [   {\"id\": 1, \"result\": \"positive\"},   {\"id\": 2, \"result\": \"negative\"} ] '''</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>    def format_batch(\n        self,\n        prompts: list[str],\n        metadata: dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"Format multiple prompts as JSON array.\n\n        Args:\n            prompts: List of prompts to batch\n            metadata: Optional metadata (row_ids, etc.)\n\n        Returns:\n            JSON-formatted batch prompt with instructions\n\n        Example:\n            Input: [\"Classify: Product A\", \"Classify: Product B\"]\n            Output:\n            '''\n            Process these 2 items and return a JSON array:\n\n            INPUT:\n            [\n              {\"id\": 1, \"input\": \"Classify: Product A\"},\n              {\"id\": 2, \"input\": \"Classify: Product B\"}\n            ]\n\n            OUTPUT FORMAT (return ONLY this JSON, nothing else):\n            [\n              {\"id\": 1, \"result\": \"positive\"},\n              {\"id\": 2, \"result\": \"negative\"}\n            ]\n            '''\n        \"\"\"\n        # Create BatchItem objects\n        items = [\n            BatchItem(id=i + 1, input=prompt, result=None)\n            for i, prompt in enumerate(prompts)\n        ]\n\n        # Format as prompt\n        items_json = json.dumps(\n            [{\"id\": item.id, \"input\": item.input} for item in items], indent=2\n        )\n\n        return f\"\"\"Process these {len(prompts)} items and return a JSON array.\n\nINPUT:\n{items_json}\n\nCRITICAL OUTPUT REQUIREMENTS:\n1. Return a JSON array with {len(prompts)} objects\n2. Each object must have \"id\" (number) and \"result\" (string, object, or number) fields\n3. IDs must match the input IDs (1 to {len(prompts)})\n4. Return ONLY the JSON array, no explanations or markdown\n\nOUTPUT FORMAT:\n[\n  {{\"id\": 1, \"result\": \"your result here\"}},\n  {{\"id\": 2, \"result\": {{\"nested\": \"json object\"}}}},\n  ...\n  {{\"id\": {len(prompts)}, \"result\": \"your result here\"}}\n]\n\nJSON Array:\"\"\"\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.JsonBatchStrategy.parse_batch_response","title":"parse_batch_response","text":"<pre><code>parse_batch_response(response: str, expected_count: int, metadata: dict[str, Any] | None = None) -&gt; list[str]\n</code></pre> <p>Parse JSON batch response into individual results.</p> <p>Uses manual JSON parsing with fallback to partial extraction. LlamaIndex structured_predict() is used by the LLM client, not here.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>LLM response containing JSON array</p> required <code>expected_count</code> <code>int</code> <p>Expected number of results</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of result strings, sorted by ID</p> <p>Raises:</p> Type Description <code>PartialParseError</code> <p>If some results parsed but not all</p> <code>ValueError</code> <p>If response is completely invalid</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>def parse_batch_response(\n    self,\n    response: str,\n    expected_count: int,\n    metadata: dict[str, Any] | None = None,\n) -&gt; list[str]:\n    \"\"\"Parse JSON batch response into individual results.\n\n    Uses manual JSON parsing with fallback to partial extraction.\n    LlamaIndex structured_predict() is used by the LLM client, not here.\n\n    Args:\n        response: LLM response containing JSON array\n        expected_count: Expected number of results\n        metadata: Optional metadata\n\n    Returns:\n        List of result strings, sorted by ID\n\n    Raises:\n        PartialParseError: If some results parsed but not all\n        ValueError: If response is completely invalid\n    \"\"\"\n    # Extract JSON array from response (handle markdown code blocks)\n    json_text = self._extract_json(response)\n\n    if not json_text:\n        raise ValueError(\n            f\"No JSON array found in response. Response: {response[:200]}\"\n        )\n\n    # Parse JSON\n    try:\n        data = json.loads(json_text)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON in response: {e}\") from e\n\n    # Validate it's a list\n    if not isinstance(data, list):\n        raise ValueError(f\"Expected JSON array, got {type(data)}\")\n\n    # Parse into BatchResult for validation\n    try:\n        items = [BatchItem(**item) for item in data]\n        batch_result = BatchResult(results=items)\n    except Exception as e:\n        raise ValueError(f\"Invalid batch result format: {e}\") from e\n\n    # Check if we got all expected results\n    missing_ids = batch_result.get_missing_ids(expected_count)\n\n    if missing_ids:\n        # Partial success - raise PartialParseError\n        self.logger.warning(\n            f\"Partial parse: got {len(batch_result.results)}/{expected_count} results. \"\n            f\"Missing IDs: {missing_ids}\"\n        )\n        raise PartialParseError(\n            message=f\"Missing {len(missing_ids)} results: IDs {missing_ids}\",\n            parsed_results=batch_result.to_list(),\n            failed_ids=missing_ids,\n            original_response=response,\n        )\n\n    # Full success - return sorted results\n    return batch_result.to_list()\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.JsonBatchStrategy.estimate_batch_tokens","title":"estimate_batch_tokens","text":"<pre><code>estimate_batch_tokens(prompts: list[str], tokenizer: Any) -&gt; int\n</code></pre> <p>Estimate tokens for JSON batch prompt.</p> <p>JSON adds overhead: - Array structure: ~50 tokens - Per-item overhead: ~20 tokens (id, input, result fields) - Instructions: ~150 tokens</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of prompts</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer (e.g., tiktoken)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>def estimate_batch_tokens(\n    self,\n    prompts: list[str],\n    tokenizer: Any,\n) -&gt; int:\n    \"\"\"Estimate tokens for JSON batch prompt.\n\n    JSON adds overhead:\n    - Array structure: ~50 tokens\n    - Per-item overhead: ~20 tokens (id, input, result fields)\n    - Instructions: ~150 tokens\n\n    Args:\n        prompts: List of prompts\n        tokenizer: Tokenizer (e.g., tiktoken)\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    # Base instruction overhead\n    overhead = 200\n\n    # Per-item overhead (JSON structure)\n    per_item_overhead = 25\n\n    # Sum prompt tokens\n    prompt_tokens = sum(len(tokenizer.encode(p)) for p in prompts)\n\n    # Total\n    return overhead + (len(prompts) * per_item_overhead) + prompt_tokens\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchItem","title":"BatchItem","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single item in a batch request or response.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Unique identifier for the item (1-based index)</p> <code>input</code> <code>str | None</code> <p>Input text for this item (request only)</p> <code>result</code> <code>Any | None</code> <p>Output result for this item (response only)</p>"},{"location":"api/strategies/#ondine.strategies.BatchItem.validate_id","title":"validate_id  <code>classmethod</code>","text":"<pre><code>validate_id(v: int) -&gt; int\n</code></pre> <p>Validate ID is positive.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"id\")\n@classmethod\ndef validate_id(cls, v: int) -&gt; int:\n    \"\"\"Validate ID is positive.\"\"\"\n    if v &lt; 1:\n        raise ValueError(\"ID must be &gt;= 1\")\n    return v\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchMetadata","title":"BatchMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for batch processing.</p> <p>Tracks information needed for disaggregation and error handling.</p> <p>Attributes:</p> Name Type Description <code>original_count</code> <code>int</code> <p>Number of items in the batch</p> <code>row_ids</code> <code>list[int]</code> <p>Original row IDs from the dataset</p> <code>prompt_template</code> <code>str | None</code> <p>Original prompt template used</p>"},{"location":"api/strategies/#ondine.strategies.BatchMetadata.validate_row_ids","title":"validate_row_ids  <code>classmethod</code>","text":"<pre><code>validate_row_ids(v: list[int], info) -&gt; list[int]\n</code></pre> <p>Validate row_ids matches original_count.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"row_ids\")\n@classmethod\ndef validate_row_ids(cls, v: list[int], info) -&gt; list[int]:\n    \"\"\"Validate row_ids matches original_count.\"\"\"\n    if \"original_count\" in info.data and len(v) != info.data[\"original_count\"]:\n        raise ValueError(\n            f\"row_ids length ({len(v)}) must match original_count ({info.data['original_count']})\"\n        )\n    return v\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchRequest","title":"BatchRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Batch request containing multiple items.</p> <p>Used for formatting batch prompts with structured data.</p> <p>Attributes:</p> Name Type Description <code>items</code> <code>list[BatchItem]</code> <p>List of items to process</p> <code>task</code> <code>str | None</code> <p>Task description (e.g., \"Classify sentiment\")</p> <code>output_format</code> <code>str | None</code> <p>Expected output format description</p>"},{"location":"api/strategies/#ondine.strategies.BatchRequest.validate_items","title":"validate_items  <code>classmethod</code>","text":"<pre><code>validate_items(v: list[BatchItem]) -&gt; list[BatchItem]\n</code></pre> <p>Validate items list is not empty.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"items\")\n@classmethod\ndef validate_items(cls, v: list[BatchItem]) -&gt; list[BatchItem]:\n    \"\"\"Validate items list is not empty.\"\"\"\n    if not v:\n        raise ValueError(\"Batch must contain at least 1 item\")\n    return v\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchResult","title":"BatchResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Batch response containing multiple results.</p> <p>Used for parsing LLM responses with structured_predict().</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[BatchItem]</code> <p>List of results (one per input item)</p>"},{"location":"api/strategies/#ondine.strategies.BatchResult.validate_results","title":"validate_results  <code>classmethod</code>","text":"<pre><code>validate_results(v: list[BatchItem]) -&gt; list[BatchItem]\n</code></pre> <p>Validate results list is not empty and all have IDs.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"results\")\n@classmethod\ndef validate_results(cls, v: list[BatchItem]) -&gt; list[BatchItem]:\n    \"\"\"Validate results list is not empty and all have IDs.\"\"\"\n    if not v:\n        raise ValueError(\"Batch result must contain at least 1 result\")\n\n    # Validate all items have IDs\n    for item in v:\n        if item.id is None:\n            raise ValueError(\"All result items must have an ID\")\n\n    return v\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchResult.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; list[str]\n</code></pre> <p>Convert to list of result strings, sorted by ID.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of result strings in ID order.</p> <code>list[str]</code> <ul> <li>None results are converted to empty string \"\"</li> </ul> <code>list[str]</code> <ul> <li>Dict/list results are JSON-serialized</li> </ul> <code>list[str]</code> <ul> <li>Other types are converted to string</li> </ul> Source code in <code>ondine/strategies/models.py</code> <pre><code>def to_list(self) -&gt; list[str]:\n    \"\"\"Convert to list of result strings, sorted by ID.\n\n    Returns:\n        List of result strings in ID order.\n        - None results are converted to empty string \"\"\n        - Dict/list results are JSON-serialized\n        - Other types are converted to string\n    \"\"\"\n    # Sort by ID\n    sorted_results = sorted(self.results, key=lambda x: x.id)\n\n    # Extract result strings\n    output = []\n    for item in sorted_results:\n        if item.result is None:\n            output.append(\"\")\n        elif isinstance(item.result, (dict, list)):\n            output.append(json.dumps(item.result))\n        else:\n            output.append(str(item.result))\n    return output\n</code></pre>"},{"location":"api/strategies/#ondine.strategies.BatchResult.get_missing_ids","title":"get_missing_ids","text":"<pre><code>get_missing_ids(expected_count: int) -&gt; list[int]\n</code></pre> <p>Get IDs that are missing from results.</p> <p>Parameters:</p> Name Type Description Default <code>expected_count</code> <code>int</code> <p>Expected number of results</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>List of missing IDs (1-based)</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>def get_missing_ids(self, expected_count: int) -&gt; list[int]:\n    \"\"\"Get IDs that are missing from results.\n\n    Args:\n        expected_count: Expected number of results\n\n    Returns:\n        List of missing IDs (1-based)\n    \"\"\"\n    present_ids = {item.id for item in self.results}\n    expected_ids = set(range(1, expected_count + 1))\n    return sorted(expected_ids - present_ids)\n</code></pre>"},{"location":"api/strategies/batch_formatting/","title":"batch_formatting","text":""},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting","title":"batch_formatting","text":"<p>Batch formatting strategy interface for multi-row processing.</p> <p>This module defines the abstract interface for batch formatting strategies, which are responsible for: 1. Formatting multiple prompts into a single batch prompt 2. Parsing batch responses back into individual results</p> <p>The Strategy pattern allows different formatting approaches (JSON, CSV, XML) without modifying the core batch processing stages.</p>"},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting.PartialParseError","title":"PartialParseError","text":"<pre><code>PartialParseError(message: str, parsed_results: list[str], failed_ids: list[int], original_response: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Raised when batch response is partially parsed.</p> <p>Attributes:</p> Name Type Description <code>parsed_results</code> <p>Successfully parsed results</p> <code>failed_ids</code> <p>IDs of rows that failed to parse</p> <code>original_response</code> <p>The original response text</p> <p>Initialize partial parse error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error description</p> required <code>parsed_results</code> <code>list[str]</code> <p>Successfully parsed results</p> required <code>failed_ids</code> <code>list[int]</code> <p>IDs of rows that failed to parse</p> required <code>original_response</code> <code>str</code> <p>The original response text</p> required Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    parsed_results: list[str],\n    failed_ids: list[int],\n    original_response: str,\n):\n    \"\"\"Initialize partial parse error.\n\n    Args:\n        message: Error description\n        parsed_results: Successfully parsed results\n        failed_ids: IDs of rows that failed to parse\n        original_response: The original response text\n    \"\"\"\n    super().__init__(message)\n    self.parsed_results = parsed_results\n    self.failed_ids = failed_ids\n    self.original_response = original_response\n</code></pre>"},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting.BatchFormattingStrategy","title":"BatchFormattingStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract strategy for batch prompt formatting and response parsing.</p> <p>This interface defines the contract for batch processing strategies. Implementations must handle: - Formatting N prompts into 1 batch prompt - Parsing 1 batch response into N individual results - Partial failure handling (some results parsed, some failed)</p> <p>Design Pattern: Strategy Pattern - Allows different formatting approaches (JSON, CSV, XML) - Stages depend on this abstraction (Dependency Inversion) - New strategies can be added without modifying stages (Open/Closed)</p>"},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting.BatchFormattingStrategy.format_batch","title":"format_batch  <code>abstractmethod</code>","text":"<pre><code>format_batch(prompts: list[str], metadata: dict[str, Any] | None = None) -&gt; str\n</code></pre> <p>Format multiple prompts into a single batch prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of individual prompts to batch together</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata (e.g., row IDs, column names)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Single batch prompt containing all inputs</p> Example <p>prompts = [\"Classify: Product A\", \"Classify: Product B\"] result = strategy.format_batch(prompts)</p> Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>@abstractmethod\ndef format_batch(\n    self,\n    prompts: list[str],\n    metadata: dict[str, Any] | None = None,\n) -&gt; str:\n    \"\"\"Format multiple prompts into a single batch prompt.\n\n    Args:\n        prompts: List of individual prompts to batch together\n        metadata: Optional metadata (e.g., row IDs, column names)\n\n    Returns:\n        Single batch prompt containing all inputs\n\n    Example:\n        prompts = [\"Classify: Product A\", \"Classify: Product B\"]\n        result = strategy.format_batch(prompts)\n        # Returns: \"Classify these 2 items: 1. Product A, 2. Product B...\"\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting.BatchFormattingStrategy.format_batch--returns-classify-these-2-items-1-product-a-2-product-b","title":"Returns: \"Classify these 2 items: 1. Product A, 2. Product B...\"","text":""},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting.BatchFormattingStrategy.parse_batch_response","title":"parse_batch_response  <code>abstractmethod</code>","text":"<pre><code>parse_batch_response(response: str, expected_count: int, metadata: dict[str, Any] | None = None) -&gt; list[str]\n</code></pre> <p>Parse batch response into individual results.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>Single batch response from LLM</p> required <code>expected_count</code> <code>int</code> <p>Expected number of results</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata from format_batch</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of individual results (length must equal expected_count)</p> <p>Raises:</p> Type Description <code>PartialParseError</code> <p>If some results parsed but not all</p> <code>ValueError</code> <p>If response format is completely invalid</p> Example <p>response = '[{\"id\": 1, \"result\": \"positive\"}, {\"id\": 2, \"result\": \"negative\"}]' results = strategy.parse_batch_response(response, expected_count=2)</p> Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>@abstractmethod\ndef parse_batch_response(\n    self,\n    response: str,\n    expected_count: int,\n    metadata: dict[str, Any] | None = None,\n) -&gt; list[str]:\n    \"\"\"Parse batch response into individual results.\n\n    Args:\n        response: Single batch response from LLM\n        expected_count: Expected number of results\n        metadata: Optional metadata from format_batch\n\n    Returns:\n        List of individual results (length must equal expected_count)\n\n    Raises:\n        PartialParseError: If some results parsed but not all\n        ValueError: If response format is completely invalid\n\n    Example:\n        response = '[{\"id\": 1, \"result\": \"positive\"}, {\"id\": 2, \"result\": \"negative\"}]'\n        results = strategy.parse_batch_response(response, expected_count=2)\n        # Returns: [\"positive\", \"negative\"]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting.BatchFormattingStrategy.parse_batch_response--returns-positive-negative","title":"Returns: [\"positive\", \"negative\"]","text":""},{"location":"api/strategies/batch_formatting/#ondine.strategies.batch_formatting.BatchFormattingStrategy.estimate_batch_tokens","title":"estimate_batch_tokens","text":"<pre><code>estimate_batch_tokens(prompts: list[str], tokenizer: Any) -&gt; int\n</code></pre> <p>Estimate total tokens for batch prompt (optional override).</p> <p>Default implementation: Sum individual prompt tokens + overhead.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of prompts to batch</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer for counting (e.g., tiktoken)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count for batch prompt</p> Source code in <code>ondine/strategies/batch_formatting.py</code> <pre><code>def estimate_batch_tokens(\n    self,\n    prompts: list[str],\n    tokenizer: Any,\n) -&gt; int:\n    \"\"\"Estimate total tokens for batch prompt (optional override).\n\n    Default implementation: Sum individual prompt tokens + overhead.\n\n    Args:\n        prompts: List of prompts to batch\n        tokenizer: Tokenizer for counting (e.g., tiktoken)\n\n    Returns:\n        Estimated token count for batch prompt\n    \"\"\"\n    # Default: sum of individual prompts + 10% overhead for formatting\n    total = sum(len(tokenizer.encode(p)) for p in prompts)\n    return int(total * 1.1)\n</code></pre>"},{"location":"api/strategies/json_batch_strategy/","title":"json_batch_strategy","text":""},{"location":"api/strategies/json_batch_strategy/#ondine.strategies.json_batch_strategy","title":"json_batch_strategy","text":"<p>JSON-based batch formatting strategy using LlamaIndex structured output.</p> <p>This strategy formats multiple prompts as a JSON array and uses LlamaIndex's structured_predict() for reliable parsing with Pydantic validation.</p>"},{"location":"api/strategies/json_batch_strategy/#ondine.strategies.json_batch_strategy.JsonBatchStrategy","title":"JsonBatchStrategy","text":"<pre><code>JsonBatchStrategy()\n</code></pre> <p>               Bases: <code>BatchFormattingStrategy</code></p> <p>JSON-based batch formatting strategy.</p> <p>Formats prompts as JSON array and parses responses using LlamaIndex's structured_predict() for reliable Pydantic-validated parsing.</p> <p>Design: - Uses structured output (Pydantic models) for type safety - Leverages LlamaIndex for automatic retry on malformed JSON - Supports partial extraction (parse what works, report failures)</p> <p>Initialize JSON batch strategy.</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize JSON batch strategy.\"\"\"\n    self.logger = get_logger(f\"{__name__}.JsonBatchStrategy\")\n</code></pre>"},{"location":"api/strategies/json_batch_strategy/#ondine.strategies.json_batch_strategy.JsonBatchStrategy.format_batch","title":"format_batch","text":"<pre><code>format_batch(prompts: list[str], metadata: dict[str, Any] | None = None) -&gt; str\n</code></pre> <p>Format multiple prompts as JSON array.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of prompts to batch</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata (row_ids, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON-formatted batch prompt with instructions</p> Example <p>Input: [\"Classify: Product A\", \"Classify: Product B\"] Output: ''' Process these 2 items and return a JSON array:</p> <p>INPUT: [   {\"id\": 1, \"input\": \"Classify: Product A\"},   {\"id\": 2, \"input\": \"Classify: Product B\"} ]</p> <p>OUTPUT FORMAT (return ONLY this JSON, nothing else): [   {\"id\": 1, \"result\": \"positive\"},   {\"id\": 2, \"result\": \"negative\"} ] '''</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>    def format_batch(\n        self,\n        prompts: list[str],\n        metadata: dict[str, Any] | None = None,\n    ) -&gt; str:\n        \"\"\"Format multiple prompts as JSON array.\n\n        Args:\n            prompts: List of prompts to batch\n            metadata: Optional metadata (row_ids, etc.)\n\n        Returns:\n            JSON-formatted batch prompt with instructions\n\n        Example:\n            Input: [\"Classify: Product A\", \"Classify: Product B\"]\n            Output:\n            '''\n            Process these 2 items and return a JSON array:\n\n            INPUT:\n            [\n              {\"id\": 1, \"input\": \"Classify: Product A\"},\n              {\"id\": 2, \"input\": \"Classify: Product B\"}\n            ]\n\n            OUTPUT FORMAT (return ONLY this JSON, nothing else):\n            [\n              {\"id\": 1, \"result\": \"positive\"},\n              {\"id\": 2, \"result\": \"negative\"}\n            ]\n            '''\n        \"\"\"\n        # Create BatchItem objects\n        items = [\n            BatchItem(id=i + 1, input=prompt, result=None)\n            for i, prompt in enumerate(prompts)\n        ]\n\n        # Format as prompt\n        items_json = json.dumps(\n            [{\"id\": item.id, \"input\": item.input} for item in items], indent=2\n        )\n\n        return f\"\"\"Process these {len(prompts)} items and return a JSON array.\n\nINPUT:\n{items_json}\n\nCRITICAL OUTPUT REQUIREMENTS:\n1. Return a JSON array with {len(prompts)} objects\n2. Each object must have \"id\" (number) and \"result\" (string, object, or number) fields\n3. IDs must match the input IDs (1 to {len(prompts)})\n4. Return ONLY the JSON array, no explanations or markdown\n\nOUTPUT FORMAT:\n[\n  {{\"id\": 1, \"result\": \"your result here\"}},\n  {{\"id\": 2, \"result\": {{\"nested\": \"json object\"}}}},\n  ...\n  {{\"id\": {len(prompts)}, \"result\": \"your result here\"}}\n]\n\nJSON Array:\"\"\"\n</code></pre>"},{"location":"api/strategies/json_batch_strategy/#ondine.strategies.json_batch_strategy.JsonBatchStrategy.parse_batch_response","title":"parse_batch_response","text":"<pre><code>parse_batch_response(response: str, expected_count: int, metadata: dict[str, Any] | None = None) -&gt; list[str]\n</code></pre> <p>Parse JSON batch response into individual results.</p> <p>Uses manual JSON parsing with fallback to partial extraction. LlamaIndex structured_predict() is used by the LLM client, not here.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>LLM response containing JSON array</p> required <code>expected_count</code> <code>int</code> <p>Expected number of results</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of result strings, sorted by ID</p> <p>Raises:</p> Type Description <code>PartialParseError</code> <p>If some results parsed but not all</p> <code>ValueError</code> <p>If response is completely invalid</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>def parse_batch_response(\n    self,\n    response: str,\n    expected_count: int,\n    metadata: dict[str, Any] | None = None,\n) -&gt; list[str]:\n    \"\"\"Parse JSON batch response into individual results.\n\n    Uses manual JSON parsing with fallback to partial extraction.\n    LlamaIndex structured_predict() is used by the LLM client, not here.\n\n    Args:\n        response: LLM response containing JSON array\n        expected_count: Expected number of results\n        metadata: Optional metadata\n\n    Returns:\n        List of result strings, sorted by ID\n\n    Raises:\n        PartialParseError: If some results parsed but not all\n        ValueError: If response is completely invalid\n    \"\"\"\n    # Extract JSON array from response (handle markdown code blocks)\n    json_text = self._extract_json(response)\n\n    if not json_text:\n        raise ValueError(\n            f\"No JSON array found in response. Response: {response[:200]}\"\n        )\n\n    # Parse JSON\n    try:\n        data = json.loads(json_text)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON in response: {e}\") from e\n\n    # Validate it's a list\n    if not isinstance(data, list):\n        raise ValueError(f\"Expected JSON array, got {type(data)}\")\n\n    # Parse into BatchResult for validation\n    try:\n        items = [BatchItem(**item) for item in data]\n        batch_result = BatchResult(results=items)\n    except Exception as e:\n        raise ValueError(f\"Invalid batch result format: {e}\") from e\n\n    # Check if we got all expected results\n    missing_ids = batch_result.get_missing_ids(expected_count)\n\n    if missing_ids:\n        # Partial success - raise PartialParseError\n        self.logger.warning(\n            f\"Partial parse: got {len(batch_result.results)}/{expected_count} results. \"\n            f\"Missing IDs: {missing_ids}\"\n        )\n        raise PartialParseError(\n            message=f\"Missing {len(missing_ids)} results: IDs {missing_ids}\",\n            parsed_results=batch_result.to_list(),\n            failed_ids=missing_ids,\n            original_response=response,\n        )\n\n    # Full success - return sorted results\n    return batch_result.to_list()\n</code></pre>"},{"location":"api/strategies/json_batch_strategy/#ondine.strategies.json_batch_strategy.JsonBatchStrategy.estimate_batch_tokens","title":"estimate_batch_tokens","text":"<pre><code>estimate_batch_tokens(prompts: list[str], tokenizer: Any) -&gt; int\n</code></pre> <p>Estimate tokens for JSON batch prompt.</p> <p>JSON adds overhead: - Array structure: ~50 tokens - Per-item overhead: ~20 tokens (id, input, result fields) - Instructions: ~150 tokens</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of prompts</p> required <code>tokenizer</code> <code>Any</code> <p>Tokenizer (e.g., tiktoken)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/strategies/json_batch_strategy.py</code> <pre><code>def estimate_batch_tokens(\n    self,\n    prompts: list[str],\n    tokenizer: Any,\n) -&gt; int:\n    \"\"\"Estimate tokens for JSON batch prompt.\n\n    JSON adds overhead:\n    - Array structure: ~50 tokens\n    - Per-item overhead: ~20 tokens (id, input, result fields)\n    - Instructions: ~150 tokens\n\n    Args:\n        prompts: List of prompts\n        tokenizer: Tokenizer (e.g., tiktoken)\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    # Base instruction overhead\n    overhead = 200\n\n    # Per-item overhead (JSON structure)\n    per_item_overhead = 25\n\n    # Sum prompt tokens\n    prompt_tokens = sum(len(tokenizer.encode(p)) for p in prompts)\n\n    # Total\n    return overhead + (len(prompts) * per_item_overhead) + prompt_tokens\n</code></pre>"},{"location":"api/strategies/models/","title":"models","text":""},{"location":"api/strategies/models/#ondine.strategies.models","title":"models","text":"<p>Pydantic models for batch processing.</p> <p>These models define the structure for batch requests and responses, enabling type-safe batch processing with validation.</p>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchItem","title":"BatchItem","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single item in a batch request or response.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>int</code> <p>Unique identifier for the item (1-based index)</p> <code>input</code> <code>str | None</code> <p>Input text for this item (request only)</p> <code>result</code> <code>Any | None</code> <p>Output result for this item (response only)</p>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchItem.validate_id","title":"validate_id  <code>classmethod</code>","text":"<pre><code>validate_id(v: int) -&gt; int\n</code></pre> <p>Validate ID is positive.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"id\")\n@classmethod\ndef validate_id(cls, v: int) -&gt; int:\n    \"\"\"Validate ID is positive.\"\"\"\n    if v &lt; 1:\n        raise ValueError(\"ID must be &gt;= 1\")\n    return v\n</code></pre>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchRequest","title":"BatchRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Batch request containing multiple items.</p> <p>Used for formatting batch prompts with structured data.</p> <p>Attributes:</p> Name Type Description <code>items</code> <code>list[BatchItem]</code> <p>List of items to process</p> <code>task</code> <code>str | None</code> <p>Task description (e.g., \"Classify sentiment\")</p> <code>output_format</code> <code>str | None</code> <p>Expected output format description</p>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchRequest.validate_items","title":"validate_items  <code>classmethod</code>","text":"<pre><code>validate_items(v: list[BatchItem]) -&gt; list[BatchItem]\n</code></pre> <p>Validate items list is not empty.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"items\")\n@classmethod\ndef validate_items(cls, v: list[BatchItem]) -&gt; list[BatchItem]:\n    \"\"\"Validate items list is not empty.\"\"\"\n    if not v:\n        raise ValueError(\"Batch must contain at least 1 item\")\n    return v\n</code></pre>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchResult","title":"BatchResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Batch response containing multiple results.</p> <p>Used for parsing LLM responses with structured_predict().</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[BatchItem]</code> <p>List of results (one per input item)</p>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchResult.validate_results","title":"validate_results  <code>classmethod</code>","text":"<pre><code>validate_results(v: list[BatchItem]) -&gt; list[BatchItem]\n</code></pre> <p>Validate results list is not empty and all have IDs.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"results\")\n@classmethod\ndef validate_results(cls, v: list[BatchItem]) -&gt; list[BatchItem]:\n    \"\"\"Validate results list is not empty and all have IDs.\"\"\"\n    if not v:\n        raise ValueError(\"Batch result must contain at least 1 result\")\n\n    # Validate all items have IDs\n    for item in v:\n        if item.id is None:\n            raise ValueError(\"All result items must have an ID\")\n\n    return v\n</code></pre>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchResult.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; list[str]\n</code></pre> <p>Convert to list of result strings, sorted by ID.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of result strings in ID order.</p> <code>list[str]</code> <ul> <li>None results are converted to empty string \"\"</li> </ul> <code>list[str]</code> <ul> <li>Dict/list results are JSON-serialized</li> </ul> <code>list[str]</code> <ul> <li>Other types are converted to string</li> </ul> Source code in <code>ondine/strategies/models.py</code> <pre><code>def to_list(self) -&gt; list[str]:\n    \"\"\"Convert to list of result strings, sorted by ID.\n\n    Returns:\n        List of result strings in ID order.\n        - None results are converted to empty string \"\"\n        - Dict/list results are JSON-serialized\n        - Other types are converted to string\n    \"\"\"\n    # Sort by ID\n    sorted_results = sorted(self.results, key=lambda x: x.id)\n\n    # Extract result strings\n    output = []\n    for item in sorted_results:\n        if item.result is None:\n            output.append(\"\")\n        elif isinstance(item.result, (dict, list)):\n            output.append(json.dumps(item.result))\n        else:\n            output.append(str(item.result))\n    return output\n</code></pre>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchResult.get_missing_ids","title":"get_missing_ids","text":"<pre><code>get_missing_ids(expected_count: int) -&gt; list[int]\n</code></pre> <p>Get IDs that are missing from results.</p> <p>Parameters:</p> Name Type Description Default <code>expected_count</code> <code>int</code> <p>Expected number of results</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>List of missing IDs (1-based)</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>def get_missing_ids(self, expected_count: int) -&gt; list[int]:\n    \"\"\"Get IDs that are missing from results.\n\n    Args:\n        expected_count: Expected number of results\n\n    Returns:\n        List of missing IDs (1-based)\n    \"\"\"\n    present_ids = {item.id for item in self.results}\n    expected_ids = set(range(1, expected_count + 1))\n    return sorted(expected_ids - present_ids)\n</code></pre>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchMetadata","title":"BatchMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for batch processing.</p> <p>Tracks information needed for disaggregation and error handling.</p> <p>Attributes:</p> Name Type Description <code>original_count</code> <code>int</code> <p>Number of items in the batch</p> <code>row_ids</code> <code>list[int]</code> <p>Original row IDs from the dataset</p> <code>prompt_template</code> <code>str | None</code> <p>Original prompt template used</p>"},{"location":"api/strategies/models/#ondine.strategies.models.BatchMetadata.validate_row_ids","title":"validate_row_ids  <code>classmethod</code>","text":"<pre><code>validate_row_ids(v: list[int], info) -&gt; list[int]\n</code></pre> <p>Validate row_ids matches original_count.</p> Source code in <code>ondine/strategies/models.py</code> <pre><code>@field_validator(\"row_ids\")\n@classmethod\ndef validate_row_ids(cls, v: list[int], info) -&gt; list[int]:\n    \"\"\"Validate row_ids matches original_count.\"\"\"\n    if \"original_count\" in info.data and len(v) != info.data[\"original_count\"]:\n        raise ValueError(\n            f\"row_ids length ({len(v)}) must match original_count ({info.data['original_count']})\"\n        )\n    return v\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#ondine.utils","title":"utils","text":"<p>Utility modules for cross-cutting concerns.</p>"},{"location":"api/utils/#ondine.utils.BudgetController","title":"BudgetController","text":"<pre><code>BudgetController(max_budget: Decimal | None = None, warn_at_75: bool = True, warn_at_90: bool = True, fail_on_exceed: bool = True)\n</code></pre> <p>Controls and enforces budget limits during execution.</p> <p>Follows Single Responsibility: only handles budget management.</p> <p>Initialize budget controller.</p> <p>Parameters:</p> Name Type Description Default <code>max_budget</code> <code>Decimal | None</code> <p>Maximum allowed budget in USD</p> <code>None</code> <code>warn_at_75</code> <code>bool</code> <p>Warn at 75% of budget</p> <code>True</code> <code>warn_at_90</code> <code>bool</code> <p>Warn at 90% of budget</p> <code>True</code> <code>fail_on_exceed</code> <code>bool</code> <p>Raise error if budget exceeded</p> <code>True</code> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def __init__(\n    self,\n    max_budget: Decimal | None = None,\n    warn_at_75: bool = True,\n    warn_at_90: bool = True,\n    fail_on_exceed: bool = True,\n):\n    \"\"\"\n    Initialize budget controller.\n\n    Args:\n        max_budget: Maximum allowed budget in USD\n        warn_at_75: Warn at 75% of budget\n        warn_at_90: Warn at 90% of budget\n        fail_on_exceed: Raise error if budget exceeded\n    \"\"\"\n    self.max_budget = max_budget\n    self.warn_at_75 = warn_at_75\n    self.warn_at_90 = warn_at_90\n    self.fail_on_exceed = fail_on_exceed\n\n    self._warned_75 = False\n    self._warned_90 = False\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.check_budget","title":"check_budget","text":"<pre><code>check_budget(current_cost: Decimal) -&gt; None\n</code></pre> <p>Check if budget is within limits.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Raises:</p> Type Description <code>BudgetExceededError</code> <p>If budget exceeded and fail_on_exceed=True</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def check_budget(self, current_cost: Decimal) -&gt; None:\n    \"\"\"\n    Check if budget is within limits.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Raises:\n        BudgetExceededError: If budget exceeded and fail_on_exceed=True\n    \"\"\"\n    if self.max_budget is None:\n        return\n\n    usage_ratio = float(current_cost / self.max_budget)\n\n    # 75% warning\n    if self.warn_at_75 and not self._warned_75 and usage_ratio &gt;= 0.75:\n        logger.warning(\n            f\"Budget warning: 75% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_75 = True\n\n    # 90% warning\n    if self.warn_at_90 and not self._warned_90 and usage_ratio &gt;= 0.90:\n        logger.warning(\n            f\"Budget warning: 90% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_90 = True\n\n    # Budget exceeded\n    if current_cost &gt; self.max_budget:\n        error_msg = f\"Budget exceeded: ${current_cost:.4f} &gt; ${self.max_budget:.2f}\"\n        logger.error(error_msg)\n\n        if self.fail_on_exceed:\n            raise BudgetExceededError(error_msg)\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.get_remaining","title":"get_remaining","text":"<pre><code>get_remaining(current_cost: Decimal) -&gt; Decimal | None\n</code></pre> <p>Get remaining budget.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>Decimal | None</code> <p>Remaining budget or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_remaining(self, current_cost: Decimal) -&gt; Decimal | None:\n    \"\"\"\n    Get remaining budget.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Remaining budget or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return self.max_budget - current_cost\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.get_usage_percentage","title":"get_usage_percentage","text":"<pre><code>get_usage_percentage(current_cost: Decimal) -&gt; float | None\n</code></pre> <p>Get budget usage as percentage.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>Usage percentage or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_usage_percentage(self, current_cost: Decimal) -&gt; float | None:\n    \"\"\"\n    Get budget usage as percentage.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Usage percentage or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return float(current_cost / self.max_budget) * 100\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.can_afford","title":"can_afford","text":"<pre><code>can_afford(estimated_cost: Decimal, current_cost: Decimal) -&gt; bool\n</code></pre> <p>Check if estimated additional cost is within budget.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_cost</code> <code>Decimal</code> <p>Estimated cost for next operation</p> required <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if within budget</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def can_afford(self, estimated_cost: Decimal, current_cost: Decimal) -&gt; bool:\n    \"\"\"\n    Check if estimated additional cost is within budget.\n\n    Args:\n        estimated_cost: Estimated cost for next operation\n        current_cost: Current accumulated cost\n\n    Returns:\n        True if within budget\n    \"\"\"\n    if self.max_budget is None:\n        return True\n    return (current_cost + estimated_cost) &lt;= self.max_budget\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetExceededError","title":"BudgetExceededError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when budget limit is exceeded.</p>"},{"location":"api/utils/#ondine.utils.CostCalculator","title":"CostCalculator","text":"<p>Centralized cost calculation for LLM API usage.</p> <p>Single Responsibility: Calculate cost from token counts and pricing. Used by: LLMClient, CostTracker, and any component needing cost calculation.</p> <p>Design Decision: Centralize the cost formula in one place to ensure consistency and make future changes (e.g., tiered pricing) easier.</p>"},{"location":"api/utils/#ondine.utils.CostCalculator.calculate","title":"calculate  <code>staticmethod</code>","text":"<pre><code>calculate(tokens_in: int, tokens_out: int, input_cost_per_1k: Decimal, output_cost_per_1k: Decimal) -&gt; Decimal\n</code></pre> <p>Calculate cost from token counts and pricing.</p> Formula <p>cost = (tokens_in / 1000) * input_cost_per_1k +        (tokens_out / 1000) * output_cost_per_1k</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Number of input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Number of output tokens</p> required <code>input_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 input tokens</p> required <code>output_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost as Decimal (exact precision for financial calculations)</p> Example <p>from decimal import Decimal cost = CostCalculator.calculate( ...     tokens_in=1000, ...     tokens_out=500, ...     input_cost_per_1k=Decimal(\"0.00005\"), ...     output_cost_per_1k=Decimal(\"0.00008\") ... ) cost Decimal('0.00009')</p> Source code in <code>ondine/utils/cost_calculator.py</code> <pre><code>@staticmethod\ndef calculate(\n    tokens_in: int,\n    tokens_out: int,\n    input_cost_per_1k: Decimal,\n    output_cost_per_1k: Decimal,\n) -&gt; Decimal:\n    \"\"\"\n    Calculate cost from token counts and pricing.\n\n    Formula:\n        cost = (tokens_in / 1000) * input_cost_per_1k +\n               (tokens_out / 1000) * output_cost_per_1k\n\n    Args:\n        tokens_in: Number of input tokens\n        tokens_out: Number of output tokens\n        input_cost_per_1k: Cost per 1000 input tokens\n        output_cost_per_1k: Cost per 1000 output tokens\n\n    Returns:\n        Total cost as Decimal (exact precision for financial calculations)\n\n    Example:\n        &gt;&gt;&gt; from decimal import Decimal\n        &gt;&gt;&gt; cost = CostCalculator.calculate(\n        ...     tokens_in=1000,\n        ...     tokens_out=500,\n        ...     input_cost_per_1k=Decimal(\"0.00005\"),\n        ...     output_cost_per_1k=Decimal(\"0.00008\")\n        ... )\n        &gt;&gt;&gt; cost\n        Decimal('0.00009')\n    \"\"\"\n    input_cost = (Decimal(tokens_in) / 1000) * input_cost_per_1k\n    output_cost = (Decimal(tokens_out) / 1000) * output_cost_per_1k\n    return input_cost + output_cost\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker","title":"CostTracker","text":"<pre><code>CostTracker(input_cost_per_1k: Decimal | None = None, output_cost_per_1k: Decimal | None = None)\n</code></pre> <p>Detailed cost accounting with thread-safety and per-stage breakdowns.</p> <p>Scope: Detailed financial tracking and reporting Pattern: Accumulator with thread-safe operations</p> <p>Use CostTracker for: - Stage-by-stage cost breakdowns - Detailed entry logging (timestamp, model, tokens) - Thread-safe accumulation in concurrent execution - Cost reporting and analytics - Budget enforcement (via BudgetController)</p> <p>NOT for: - Simple orchestration state (use ExecutionContext for that)</p> <p>Why separate from ExecutionContext? - CostTracker = detailed accounting system (entries, breakdowns, thread-safety) - ExecutionContext = orchestration state (progress, session, timing) - Different concerns: accounting vs execution control</p> <p>Thread Safety: - All methods protected by threading.Lock - Safe for concurrent LLM invocations</p> Example <p>tracker = CostTracker(     input_cost_per_1k=Decimal(\"0.00015\"),     output_cost_per_1k=Decimal(\"0.0006\") ) cost = tracker.add(tokens_in=1000, tokens_out=500, model=\"gpt-4o-mini\") breakdown = tracker.get_stage_costs()  # {\"llm_invocation\": Decimal(\"0.00045\")}</p> <p>See Also: - ExecutionContext: For orchestration-level state - BudgetController: For cost limit enforcement - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Initialize cost tracker.</p> <p>Parameters:</p> Name Type Description Default <code>input_cost_per_1k</code> <code>Decimal | None</code> <p>Input token cost per 1K tokens</p> <code>None</code> <code>output_cost_per_1k</code> <code>Decimal | None</code> <p>Output token cost per 1K tokens</p> <code>None</code> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def __init__(\n    self,\n    input_cost_per_1k: Decimal | None = None,\n    output_cost_per_1k: Decimal | None = None,\n):\n    \"\"\"\n    Initialize cost tracker.\n\n    Args:\n        input_cost_per_1k: Input token cost per 1K tokens\n        output_cost_per_1k: Output token cost per 1K tokens\n    \"\"\"\n    self.input_cost_per_1k = input_cost_per_1k or Decimal(\"0.0\")\n    self.output_cost_per_1k = output_cost_per_1k or Decimal(\"0.0\")\n\n    self._total_input_tokens = 0\n    self._total_output_tokens = 0\n    self._total_cost = Decimal(\"0.0\")\n    self._entries: list[CostEntry] = []\n    self._stage_costs: dict[str, Decimal] = {}\n    self._lock = threading.Lock()\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.total_cost","title":"total_cost  <code>property</code>","text":"<pre><code>total_cost: Decimal\n</code></pre> <p>Get total accumulated cost.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.total_tokens","title":"total_tokens  <code>property</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Get total token count.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Get total input tokens.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Get total output tokens.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.add","title":"add","text":"<pre><code>add(tokens_in: int, tokens_out: int, model: str, timestamp: float, stage: str | None = None) -&gt; Decimal\n</code></pre> <p>Add cost entry.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens used</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens used</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>timestamp</code> <code>float</code> <p>Timestamp of request</p> required <code>stage</code> <code>str | None</code> <p>Optional stage name</p> <code>None</code> <p>Returns:</p> Type Description <code>Decimal</code> <p>Cost for this entry</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def add(\n    self,\n    tokens_in: int,\n    tokens_out: int,\n    model: str,\n    timestamp: float,\n    stage: str | None = None,\n) -&gt; Decimal:\n    \"\"\"\n    Add cost entry.\n\n    Args:\n        tokens_in: Input tokens used\n        tokens_out: Output tokens used\n        model: Model identifier\n        timestamp: Timestamp of request\n        stage: Optional stage name\n\n    Returns:\n        Cost for this entry\n    \"\"\"\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    with self._lock:\n        entry = CostEntry(\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            cost=cost,\n            model=model,\n            timestamp=timestamp,\n        )\n        self._entries.append(entry)\n\n        self._total_input_tokens += tokens_in\n        self._total_output_tokens += tokens_out\n        self._total_cost += cost\n\n        if stage:\n            self._stage_costs[stage] = (\n                self._stage_costs.get(stage, Decimal(\"0.0\")) + cost\n            )\n\n    return cost\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for given token counts.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for given token counts.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.input_cost_per_1k,\n        output_cost_per_1k=self.output_cost_per_1k,\n    )\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.get_estimate","title":"get_estimate","text":"<pre><code>get_estimate(rows: int = 0) -&gt; CostEstimate\n</code></pre> <p>Get cost estimate.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Number of rows processed</p> <code>0</code> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>CostEstimate object</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_estimate(self, rows: int = 0) -&gt; CostEstimate:\n    \"\"\"\n    Get cost estimate.\n\n    Args:\n        rows: Number of rows processed\n\n    Returns:\n        CostEstimate object\n    \"\"\"\n    with self._lock:\n        total_tokens = self._total_input_tokens + self._total_output_tokens\n        return CostEstimate(\n            total_cost=self._total_cost,\n            total_tokens=total_tokens,\n            input_tokens=self._total_input_tokens,\n            output_tokens=self._total_output_tokens,\n            rows=rows,\n            breakdown_by_stage=dict(self._stage_costs),\n            confidence=\"actual\",\n        )\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset all tracking.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all tracking.\"\"\"\n    with self._lock:\n        self._total_input_tokens = 0\n        self._total_output_tokens = 0\n        self._total_cost = Decimal(\"0.0\")\n        self._entries.clear()\n        self._stage_costs.clear()\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.get_stage_costs","title":"get_stage_costs","text":"<pre><code>get_stage_costs() -&gt; dict[str, Decimal]\n</code></pre> <p>Get costs breakdown by stage.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_stage_costs(self) -&gt; dict[str, Decimal]:\n    \"\"\"Get costs breakdown by stage.\"\"\"\n    with self._lock:\n        return dict(self._stage_costs)\n</code></pre>"},{"location":"api/utils/#ondine.utils.PreprocessingStats","title":"PreprocessingStats  <code>dataclass</code>","text":"<pre><code>PreprocessingStats(rows_processed: int, chars_before: int, chars_after: int, truncated_count: int, null_count: int)\n</code></pre> <p>Statistics from preprocessing operation.</p>"},{"location":"api/utils/#ondine.utils.PreprocessingStats.reduction_pct","title":"reduction_pct  <code>property</code>","text":"<pre><code>reduction_pct: float\n</code></pre> <p>Calculate character reduction percentage.</p>"},{"location":"api/utils/#ondine.utils.TextPreprocessor","title":"TextPreprocessor","text":"<pre><code>TextPreprocessor(max_length: int = 500)\n</code></pre> <p>Composable text preprocessor following Chain of Responsibility.</p> <p>Single Responsibility: Orchestrate cleaning steps. Open/Closed: Extensible via cleaners list. Dependency Inversion: Depends on Protocol, not concrete classes.</p> <p>Initialize with default cleaning pipeline.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, max_length: int = 500):\n    \"\"\"Initialize with default cleaning pipeline.\"\"\"\n    self.cleaners: list[TextCleaner] = [\n        UnicodeNormalizer(),\n        ControlCharRemover(),\n        SpecialCharCleaner(),\n        WhitespaceNormalizer(),\n        TextTruncator(max_length),\n    ]\n</code></pre>"},{"location":"api/utils/#ondine.utils.TextPreprocessor.process","title":"process","text":"<pre><code>process(text: str) -&gt; str\n</code></pre> <p>Apply all cleaners in sequence.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"Apply all cleaners in sequence.\"\"\"\n    if pd.isna(text) or not isinstance(text, str):\n        return \"\"\n\n    for cleaner in self.cleaners:\n        text = cleaner.clean(text)\n\n    return text\n</code></pre>"},{"location":"api/utils/#ondine.utils.TextPreprocessor.add_cleaner","title":"add_cleaner","text":"<pre><code>add_cleaner(cleaner: TextCleaner) -&gt; None\n</code></pre> <p>Extend pipeline with custom cleaner.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def add_cleaner(self, cleaner: TextCleaner) -&gt; None:\n    \"\"\"Extend pipeline with custom cleaner.\"\"\"\n    self.cleaners.append(cleaner)\n</code></pre>"},{"location":"api/utils/#ondine.utils.RateLimiter","title":"RateLimiter","text":"<pre><code>RateLimiter(requests_per_minute: int, burst_size: int | None = None)\n</code></pre> <p>Token bucket rate limiter for controlling API request rates.</p> <p>Thread-safe implementation.</p> <p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_minute</code> <code>int</code> <p>Maximum requests per minute</p> required <code>burst_size</code> <code>int | None</code> <p>Maximum burst size (default: requests_per_minute)</p> <code>None</code> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def __init__(self, requests_per_minute: int, burst_size: int | None = None):\n    \"\"\"\n    Initialize rate limiter.\n\n    Args:\n        requests_per_minute: Maximum requests per minute\n        burst_size: Maximum burst size (default: requests_per_minute)\n    \"\"\"\n    self.rpm = requests_per_minute\n    self.capacity = burst_size or requests_per_minute\n    self.tokens = float(self.capacity)\n    self.last_update = time.time()\n    self.lock = threading.Lock()\n\n    # Calculate refill rate (tokens per second)\n    self.refill_rate = requests_per_minute / 60.0\n</code></pre>"},{"location":"api/utils/#ondine.utils.RateLimiter.available_tokens","title":"available_tokens  <code>property</code>","text":"<pre><code>available_tokens: float\n</code></pre> <p>Get current available tokens.</p>"},{"location":"api/utils/#ondine.utils.RateLimiter.acquire","title":"acquire","text":"<pre><code>acquire(tokens: int = 1, timeout: float | None = None) -&gt; bool\n</code></pre> <p>Acquire tokens for making requests.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>int</code> <p>Number of tokens to acquire</p> <code>1</code> <code>timeout</code> <code>float | None</code> <p>Maximum wait time in seconds (None = wait forever)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if tokens acquired, False if timeout</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokens &gt; capacity</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def acquire(self, tokens: int = 1, timeout: float | None = None) -&gt; bool:\n    \"\"\"\n    Acquire tokens for making requests.\n\n    Args:\n        tokens: Number of tokens to acquire\n        timeout: Maximum wait time in seconds (None = wait forever)\n\n    Returns:\n        True if tokens acquired, False if timeout\n\n    Raises:\n        ValueError: If tokens &gt; capacity\n    \"\"\"\n    if tokens &gt; self.capacity:\n        raise ValueError(\n            f\"Requested {tokens} tokens exceeds capacity {self.capacity}\"\n        )\n\n    deadline = None if timeout is None else time.time() + timeout\n\n    while True:\n        with self.lock:\n            self._refill()\n\n            if self.tokens &gt;= tokens:\n                self.tokens -= tokens\n                return True\n\n        # Check timeout\n        if deadline is not None and time.time() &gt;= deadline:\n            return False\n\n        # Sleep before retry\n        time.sleep(0.1)\n</code></pre>"},{"location":"api/utils/#ondine.utils.RateLimiter.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset rate limiter to full capacity.</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset rate limiter to full capacity.\"\"\"\n    with self.lock:\n        self.tokens = float(self.capacity)\n        self.last_update = time.time()\n</code></pre>"},{"location":"api/utils/#ondine.utils.NetworkError","title":"NetworkError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Network-related error.</p>"},{"location":"api/utils/#ondine.utils.RateLimitError","title":"RateLimitError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Rate limit exceeded error.</p>"},{"location":"api/utils/#ondine.utils.RetryableError","title":"RetryableError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors that should be retried.</p>"},{"location":"api/utils/#ondine.utils.RetryHandler","title":"RetryHandler","text":"<pre><code>RetryHandler(max_attempts: int = 3, initial_delay: float = 1.0, max_delay: float = 60.0, exponential_base: int = 2, retryable_exceptions: tuple[type[Exception], ...] | None = None)\n</code></pre> <p>Request-level retry with exponential backoff (for transient errors).</p> <p>Scope: Single LLM API call or operation Use when: Transient errors (rate limits, network timeouts, API hiccups) NOT for: Row-level quality issues (use Pipeline.auto_retry_failed for that)</p> <p>Retry Strategy: - Exponential backoff (1s, 2s, 4s, 8s, ...) - Configurable max attempts (default: 3) - Only retries specific exception types</p> Example <p>handler = RetryHandler(max_attempts=3, initial_delay=1.0) result = handler.execute(lambda: call_llm_api())</p> <p>See Also: - ErrorHandler: Orchestrates retry decisions based on policy - Pipeline._auto_retry_failed_rows(): Row-level quality retry - docs/architecture/decisions/ADR-006-retry-levels.md</p> <p>Initialize retry handler.</p> <p>Parameters:</p> Name Type Description Default <code>max_attempts</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay in seconds</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds</p> <code>60.0</code> <code>exponential_base</code> <code>int</code> <p>Base for exponential backoff</p> <code>2</code> <code>retryable_exceptions</code> <code>tuple[type[Exception], ...] | None</code> <p>Exception types to retry</p> <code>None</code> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def __init__(\n    self,\n    max_attempts: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: int = 2,\n    retryable_exceptions: tuple[type[Exception], ...] | None = None,\n):\n    \"\"\"\n    Initialize retry handler.\n\n    Args:\n        max_attempts: Maximum retry attempts\n        initial_delay: Initial delay in seconds\n        max_delay: Maximum delay in seconds\n        exponential_base: Base for exponential backoff\n        retryable_exceptions: Exception types to retry\n    \"\"\"\n    self.max_attempts = max_attempts\n    self.initial_delay = initial_delay\n    self.max_delay = max_delay\n    self.exponential_base = exponential_base\n\n    if retryable_exceptions is None:\n        self.retryable_exceptions = (\n            RetryableError,\n            RateLimitError,\n            NetworkError,\n        )\n    else:\n        self.retryable_exceptions = retryable_exceptions\n</code></pre>"},{"location":"api/utils/#ondine.utils.RetryHandler.execute","title":"execute","text":"<pre><code>execute(func: Callable[[], T]) -&gt; T\n</code></pre> <p>Execute function with retry logic.</p> <p>Only retries exceptions in self.retryable_exceptions tuple. NonRetryableError and its subclasses are re-raised immediately.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[], T]</code> <p>Function to execute</p> required <p>Returns:</p> Type Description <code>T</code> <p>Result from function</p> <p>Raises:</p> Type Description <code>NonRetryableError</code> <p>Fatal errors (model not found, invalid API key, etc.)</p> <code>Exception</code> <p>If all retries exhausted for retryable errors</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def execute(self, func: Callable[[], T]) -&gt; T:\n    \"\"\"\n    Execute function with retry logic.\n\n    Only retries exceptions in self.retryable_exceptions tuple.\n    NonRetryableError and its subclasses are re-raised immediately.\n\n    Args:\n        func: Function to execute\n\n    Returns:\n        Result from function\n\n    Raises:\n        NonRetryableError: Fatal errors (model not found, invalid API key, etc.)\n        Exception: If all retries exhausted for retryable errors\n    \"\"\"\n    retryer = Retrying(\n        stop=stop_after_attempt(self.max_attempts),\n        wait=wait_exponential(\n            multiplier=self.initial_delay,\n            max=self.max_delay,\n            exp_base=self.exponential_base,\n        ),\n        retry=retry_if_exception_type(self.retryable_exceptions),\n        reraise=True,\n    )\n\n    return retryer(func)\n</code></pre>"},{"location":"api/utils/#ondine.utils.RetryHandler.calculate_delay","title":"calculate_delay","text":"<pre><code>calculate_delay(attempt: int) -&gt; float\n</code></pre> <p>Calculate delay for given attempt number.</p> <p>Parameters:</p> Name Type Description Default <code>attempt</code> <code>int</code> <p>Attempt number (1-based)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def calculate_delay(self, attempt: int) -&gt; float:\n    \"\"\"\n    Calculate delay for given attempt number.\n\n    Args:\n        attempt: Attempt number (1-based)\n\n    Returns:\n        Delay in seconds\n    \"\"\"\n    delay = self.initial_delay * (self.exponential_base ** (attempt - 1))\n    return min(delay, self.max_delay)\n</code></pre>"},{"location":"api/utils/#ondine.utils.preprocess_dataframe","title":"preprocess_dataframe","text":"<pre><code>preprocess_dataframe(df: DataFrame, input_columns: list[str], max_length: int = 500) -&gt; tuple[pd.DataFrame, PreprocessingStats]\n</code></pre> <p>Preprocess input columns in dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe</p> required <code>input_columns</code> <code>list[str]</code> <p>Columns to clean</p> required <code>max_length</code> <code>int</code> <p>Max chars per field</p> <code>500</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, PreprocessingStats]</code> <p>(cleaned_df, stats)</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def preprocess_dataframe(\n    df: pd.DataFrame,\n    input_columns: list[str],\n    max_length: int = 500,\n) -&gt; tuple[pd.DataFrame, PreprocessingStats]:\n    \"\"\"\n    Preprocess input columns in dataframe.\n\n    Args:\n        df: Input dataframe\n        input_columns: Columns to clean\n        max_length: Max chars per field\n\n    Returns:\n        (cleaned_df, stats)\n    \"\"\"\n    result = df.copy()\n    preprocessor = TextPreprocessor(max_length)\n\n    chars_before = 0\n    chars_after = 0\n    truncated = 0\n    nulls = 0\n\n    for col in input_columns:\n        if col not in result.columns:\n            continue\n\n        for idx in result.index:\n            original = result.at[idx, col]\n\n            if pd.isna(original):\n                nulls += 1\n                continue\n\n            original_str = str(original)\n            chars_before += len(original_str)\n\n            cleaned = preprocessor.process(original_str)\n            chars_after += len(cleaned)\n\n            if len(original_str) &gt; max_length:\n                truncated += 1\n\n            result.at[idx, col] = cleaned\n\n    stats = PreprocessingStats(\n        rows_processed=len(result),\n        chars_before=chars_before,\n        chars_after=chars_after,\n        truncated_count=truncated,\n        null_count=nulls,\n    )\n\n    return result, stats\n</code></pre>"},{"location":"api/utils/#ondine.utils.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(level: str = 'INFO', json_format: bool = False, include_timestamp: bool = True) -&gt; None\n</code></pre> <p>Configure structured logging for the SDK.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> <code>json_format</code> <code>bool</code> <p>Use JSON output format</p> <code>False</code> <code>include_timestamp</code> <code>bool</code> <p>Include timestamps in logs</p> <code>True</code> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def configure_logging(\n    level: str = \"INFO\",\n    json_format: bool = False,\n    include_timestamp: bool = True,\n) -&gt; None:\n    \"\"\"\n    Configure structured logging for the SDK.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        json_format: Use JSON output format\n        include_timestamp: Include timestamps in logs\n    \"\"\"\n    global _logging_configured\n\n    # Set stdlib logging level\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=getattr(logging, level.upper()),\n    )\n\n    # Configure structlog processors\n    processors = [\n        structlog.contextvars.merge_contextvars,\n        structlog.processors.add_log_level,\n        structlog.processors.StackInfoRenderer(),\n    ]\n\n    if include_timestamp:\n        processors.append(structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"))\n\n    if json_format:\n        processors.append(structlog.processors.JSONRenderer())\n    else:\n        # Use custom compact console renderer (no padding)\n        processors.append(_compact_console_renderer)\n\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.make_filtering_bound_logger(\n            getattr(logging, level.upper())\n        ),\n        context_class=dict,\n        logger_factory=structlog.PrintLoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    _logging_configured = True\n</code></pre>"},{"location":"api/utils/#ondine.utils.get_logger","title":"get_logger","text":"<pre><code>get_logger(name: str) -&gt; structlog.BoundLogger\n</code></pre> <p>Get a structured logger instance.</p> <p>Auto-configures logging on first use if not already configured.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically name)</p> required <p>Returns:</p> Type Description <code>BoundLogger</code> <p>Configured structlog logger</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def get_logger(name: str) -&gt; structlog.BoundLogger:\n    \"\"\"\n    Get a structured logger instance.\n\n    Auto-configures logging on first use if not already configured.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured structlog logger\n    \"\"\"\n    global _logging_configured\n\n    # Auto-configure logging on first use\n    if not _logging_configured:\n        configure_logging()\n\n    return structlog.get_logger(name)\n</code></pre>"},{"location":"api/utils/#ondine.utils.sanitize_for_logging","title":"sanitize_for_logging","text":"<pre><code>sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Sanitize sensitive data for logging.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary potentially containing sensitive data</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Sanitized dictionary</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Sanitize sensitive data for logging.\n\n    Args:\n        data: Dictionary potentially containing sensitive data\n\n    Returns:\n        Sanitized dictionary\n    \"\"\"\n    sensitive_keys = {\n        \"api_key\",\n        \"password\",\n        \"secret\",\n        \"token\",\n        \"authorization\",\n        \"credential\",\n    }\n\n    sanitized = {}\n    for key, value in data.items():\n        key_lower = key.lower()\n        if any(sensitive in key_lower for sensitive in sensitive_keys):\n            sanitized[key] = \"***REDACTED***\"\n        elif isinstance(value, dict):\n            sanitized[key] = sanitize_for_logging(value)\n        else:\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"api/utils/budget_controller/","title":"budget_controller","text":""},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller","title":"budget_controller","text":"<p>Budget control and enforcement for LLM costs.</p> <p>Implements cost monitoring with threshold warnings and hard limits.</p>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetExceededError","title":"BudgetExceededError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when budget limit is exceeded.</p>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController","title":"BudgetController","text":"<pre><code>BudgetController(max_budget: Decimal | None = None, warn_at_75: bool = True, warn_at_90: bool = True, fail_on_exceed: bool = True)\n</code></pre> <p>Controls and enforces budget limits during execution.</p> <p>Follows Single Responsibility: only handles budget management.</p> <p>Initialize budget controller.</p> <p>Parameters:</p> Name Type Description Default <code>max_budget</code> <code>Decimal | None</code> <p>Maximum allowed budget in USD</p> <code>None</code> <code>warn_at_75</code> <code>bool</code> <p>Warn at 75% of budget</p> <code>True</code> <code>warn_at_90</code> <code>bool</code> <p>Warn at 90% of budget</p> <code>True</code> <code>fail_on_exceed</code> <code>bool</code> <p>Raise error if budget exceeded</p> <code>True</code> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def __init__(\n    self,\n    max_budget: Decimal | None = None,\n    warn_at_75: bool = True,\n    warn_at_90: bool = True,\n    fail_on_exceed: bool = True,\n):\n    \"\"\"\n    Initialize budget controller.\n\n    Args:\n        max_budget: Maximum allowed budget in USD\n        warn_at_75: Warn at 75% of budget\n        warn_at_90: Warn at 90% of budget\n        fail_on_exceed: Raise error if budget exceeded\n    \"\"\"\n    self.max_budget = max_budget\n    self.warn_at_75 = warn_at_75\n    self.warn_at_90 = warn_at_90\n    self.fail_on_exceed = fail_on_exceed\n\n    self._warned_75 = False\n    self._warned_90 = False\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.check_budget","title":"check_budget","text":"<pre><code>check_budget(current_cost: Decimal) -&gt; None\n</code></pre> <p>Check if budget is within limits.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Raises:</p> Type Description <code>BudgetExceededError</code> <p>If budget exceeded and fail_on_exceed=True</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def check_budget(self, current_cost: Decimal) -&gt; None:\n    \"\"\"\n    Check if budget is within limits.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Raises:\n        BudgetExceededError: If budget exceeded and fail_on_exceed=True\n    \"\"\"\n    if self.max_budget is None:\n        return\n\n    usage_ratio = float(current_cost / self.max_budget)\n\n    # 75% warning\n    if self.warn_at_75 and not self._warned_75 and usage_ratio &gt;= 0.75:\n        logger.warning(\n            f\"Budget warning: 75% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_75 = True\n\n    # 90% warning\n    if self.warn_at_90 and not self._warned_90 and usage_ratio &gt;= 0.90:\n        logger.warning(\n            f\"Budget warning: 90% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_90 = True\n\n    # Budget exceeded\n    if current_cost &gt; self.max_budget:\n        error_msg = f\"Budget exceeded: ${current_cost:.4f} &gt; ${self.max_budget:.2f}\"\n        logger.error(error_msg)\n\n        if self.fail_on_exceed:\n            raise BudgetExceededError(error_msg)\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.get_remaining","title":"get_remaining","text":"<pre><code>get_remaining(current_cost: Decimal) -&gt; Decimal | None\n</code></pre> <p>Get remaining budget.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>Decimal | None</code> <p>Remaining budget or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_remaining(self, current_cost: Decimal) -&gt; Decimal | None:\n    \"\"\"\n    Get remaining budget.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Remaining budget or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return self.max_budget - current_cost\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.get_usage_percentage","title":"get_usage_percentage","text":"<pre><code>get_usage_percentage(current_cost: Decimal) -&gt; float | None\n</code></pre> <p>Get budget usage as percentage.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>Usage percentage or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_usage_percentage(self, current_cost: Decimal) -&gt; float | None:\n    \"\"\"\n    Get budget usage as percentage.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Usage percentage or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return float(current_cost / self.max_budget) * 100\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.can_afford","title":"can_afford","text":"<pre><code>can_afford(estimated_cost: Decimal, current_cost: Decimal) -&gt; bool\n</code></pre> <p>Check if estimated additional cost is within budget.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_cost</code> <code>Decimal</code> <p>Estimated cost for next operation</p> required <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if within budget</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def can_afford(self, estimated_cost: Decimal, current_cost: Decimal) -&gt; bool:\n    \"\"\"\n    Check if estimated additional cost is within budget.\n\n    Args:\n        estimated_cost: Estimated cost for next operation\n        current_cost: Current accumulated cost\n\n    Returns:\n        True if within budget\n    \"\"\"\n    if self.max_budget is None:\n        return True\n    return (current_cost + estimated_cost) &lt;= self.max_budget\n</code></pre>"},{"location":"api/utils/cost_calculator/","title":"cost_calculator","text":""},{"location":"api/utils/cost_calculator/#ondine.utils.cost_calculator","title":"cost_calculator","text":"<p>Centralized cost calculation utilities.</p> <p>Provides single source of truth for LLM cost calculation formula, eliminating duplication across LLMClient and CostTracker.</p>"},{"location":"api/utils/cost_calculator/#ondine.utils.cost_calculator.CostCalculator","title":"CostCalculator","text":"<p>Centralized cost calculation for LLM API usage.</p> <p>Single Responsibility: Calculate cost from token counts and pricing. Used by: LLMClient, CostTracker, and any component needing cost calculation.</p> <p>Design Decision: Centralize the cost formula in one place to ensure consistency and make future changes (e.g., tiered pricing) easier.</p>"},{"location":"api/utils/cost_calculator/#ondine.utils.cost_calculator.CostCalculator.calculate","title":"calculate  <code>staticmethod</code>","text":"<pre><code>calculate(tokens_in: int, tokens_out: int, input_cost_per_1k: Decimal, output_cost_per_1k: Decimal) -&gt; Decimal\n</code></pre> <p>Calculate cost from token counts and pricing.</p> Formula <p>cost = (tokens_in / 1000) * input_cost_per_1k +        (tokens_out / 1000) * output_cost_per_1k</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Number of input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Number of output tokens</p> required <code>input_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 input tokens</p> required <code>output_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost as Decimal (exact precision for financial calculations)</p> Example <p>from decimal import Decimal cost = CostCalculator.calculate( ...     tokens_in=1000, ...     tokens_out=500, ...     input_cost_per_1k=Decimal(\"0.00005\"), ...     output_cost_per_1k=Decimal(\"0.00008\") ... ) cost Decimal('0.00009')</p> Source code in <code>ondine/utils/cost_calculator.py</code> <pre><code>@staticmethod\ndef calculate(\n    tokens_in: int,\n    tokens_out: int,\n    input_cost_per_1k: Decimal,\n    output_cost_per_1k: Decimal,\n) -&gt; Decimal:\n    \"\"\"\n    Calculate cost from token counts and pricing.\n\n    Formula:\n        cost = (tokens_in / 1000) * input_cost_per_1k +\n               (tokens_out / 1000) * output_cost_per_1k\n\n    Args:\n        tokens_in: Number of input tokens\n        tokens_out: Number of output tokens\n        input_cost_per_1k: Cost per 1000 input tokens\n        output_cost_per_1k: Cost per 1000 output tokens\n\n    Returns:\n        Total cost as Decimal (exact precision for financial calculations)\n\n    Example:\n        &gt;&gt;&gt; from decimal import Decimal\n        &gt;&gt;&gt; cost = CostCalculator.calculate(\n        ...     tokens_in=1000,\n        ...     tokens_out=500,\n        ...     input_cost_per_1k=Decimal(\"0.00005\"),\n        ...     output_cost_per_1k=Decimal(\"0.00008\")\n        ... )\n        &gt;&gt;&gt; cost\n        Decimal('0.00009')\n    \"\"\"\n    input_cost = (Decimal(tokens_in) / 1000) * input_cost_per_1k\n    output_cost = (Decimal(tokens_out) / 1000) * output_cost_per_1k\n    return input_cost + output_cost\n</code></pre>"},{"location":"api/utils/cost_tracker/","title":"cost_tracker","text":""},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker","title":"cost_tracker","text":"<p>Cost tracking for LLM API calls.</p> <p>Provides accurate cost tracking with thread safety and detailed breakdowns.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostEntry","title":"CostEntry  <code>dataclass</code>","text":"<pre><code>CostEntry(tokens_in: int, tokens_out: int, cost: Decimal, model: str, timestamp: float)\n</code></pre> <p>Single cost tracking entry.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker","title":"CostTracker","text":"<pre><code>CostTracker(input_cost_per_1k: Decimal | None = None, output_cost_per_1k: Decimal | None = None)\n</code></pre> <p>Detailed cost accounting with thread-safety and per-stage breakdowns.</p> <p>Scope: Detailed financial tracking and reporting Pattern: Accumulator with thread-safe operations</p> <p>Use CostTracker for: - Stage-by-stage cost breakdowns - Detailed entry logging (timestamp, model, tokens) - Thread-safe accumulation in concurrent execution - Cost reporting and analytics - Budget enforcement (via BudgetController)</p> <p>NOT for: - Simple orchestration state (use ExecutionContext for that)</p> <p>Why separate from ExecutionContext? - CostTracker = detailed accounting system (entries, breakdowns, thread-safety) - ExecutionContext = orchestration state (progress, session, timing) - Different concerns: accounting vs execution control</p> <p>Thread Safety: - All methods protected by threading.Lock - Safe for concurrent LLM invocations</p> Example <p>tracker = CostTracker(     input_cost_per_1k=Decimal(\"0.00015\"),     output_cost_per_1k=Decimal(\"0.0006\") ) cost = tracker.add(tokens_in=1000, tokens_out=500, model=\"gpt-4o-mini\") breakdown = tracker.get_stage_costs()  # {\"llm_invocation\": Decimal(\"0.00045\")}</p> <p>See Also: - ExecutionContext: For orchestration-level state - BudgetController: For cost limit enforcement - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Initialize cost tracker.</p> <p>Parameters:</p> Name Type Description Default <code>input_cost_per_1k</code> <code>Decimal | None</code> <p>Input token cost per 1K tokens</p> <code>None</code> <code>output_cost_per_1k</code> <code>Decimal | None</code> <p>Output token cost per 1K tokens</p> <code>None</code> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def __init__(\n    self,\n    input_cost_per_1k: Decimal | None = None,\n    output_cost_per_1k: Decimal | None = None,\n):\n    \"\"\"\n    Initialize cost tracker.\n\n    Args:\n        input_cost_per_1k: Input token cost per 1K tokens\n        output_cost_per_1k: Output token cost per 1K tokens\n    \"\"\"\n    self.input_cost_per_1k = input_cost_per_1k or Decimal(\"0.0\")\n    self.output_cost_per_1k = output_cost_per_1k or Decimal(\"0.0\")\n\n    self._total_input_tokens = 0\n    self._total_output_tokens = 0\n    self._total_cost = Decimal(\"0.0\")\n    self._entries: list[CostEntry] = []\n    self._stage_costs: dict[str, Decimal] = {}\n    self._lock = threading.Lock()\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.total_cost","title":"total_cost  <code>property</code>","text":"<pre><code>total_cost: Decimal\n</code></pre> <p>Get total accumulated cost.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.total_tokens","title":"total_tokens  <code>property</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Get total token count.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Get total input tokens.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Get total output tokens.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.add","title":"add","text":"<pre><code>add(tokens_in: int, tokens_out: int, model: str, timestamp: float, stage: str | None = None) -&gt; Decimal\n</code></pre> <p>Add cost entry.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens used</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens used</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>timestamp</code> <code>float</code> <p>Timestamp of request</p> required <code>stage</code> <code>str | None</code> <p>Optional stage name</p> <code>None</code> <p>Returns:</p> Type Description <code>Decimal</code> <p>Cost for this entry</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def add(\n    self,\n    tokens_in: int,\n    tokens_out: int,\n    model: str,\n    timestamp: float,\n    stage: str | None = None,\n) -&gt; Decimal:\n    \"\"\"\n    Add cost entry.\n\n    Args:\n        tokens_in: Input tokens used\n        tokens_out: Output tokens used\n        model: Model identifier\n        timestamp: Timestamp of request\n        stage: Optional stage name\n\n    Returns:\n        Cost for this entry\n    \"\"\"\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    with self._lock:\n        entry = CostEntry(\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            cost=cost,\n            model=model,\n            timestamp=timestamp,\n        )\n        self._entries.append(entry)\n\n        self._total_input_tokens += tokens_in\n        self._total_output_tokens += tokens_out\n        self._total_cost += cost\n\n        if stage:\n            self._stage_costs[stage] = (\n                self._stage_costs.get(stage, Decimal(\"0.0\")) + cost\n            )\n\n    return cost\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for given token counts.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for given token counts.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.input_cost_per_1k,\n        output_cost_per_1k=self.output_cost_per_1k,\n    )\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.get_estimate","title":"get_estimate","text":"<pre><code>get_estimate(rows: int = 0) -&gt; CostEstimate\n</code></pre> <p>Get cost estimate.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Number of rows processed</p> <code>0</code> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>CostEstimate object</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_estimate(self, rows: int = 0) -&gt; CostEstimate:\n    \"\"\"\n    Get cost estimate.\n\n    Args:\n        rows: Number of rows processed\n\n    Returns:\n        CostEstimate object\n    \"\"\"\n    with self._lock:\n        total_tokens = self._total_input_tokens + self._total_output_tokens\n        return CostEstimate(\n            total_cost=self._total_cost,\n            total_tokens=total_tokens,\n            input_tokens=self._total_input_tokens,\n            output_tokens=self._total_output_tokens,\n            rows=rows,\n            breakdown_by_stage=dict(self._stage_costs),\n            confidence=\"actual\",\n        )\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset all tracking.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all tracking.\"\"\"\n    with self._lock:\n        self._total_input_tokens = 0\n        self._total_output_tokens = 0\n        self._total_cost = Decimal(\"0.0\")\n        self._entries.clear()\n        self._stage_costs.clear()\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.get_stage_costs","title":"get_stage_costs","text":"<pre><code>get_stage_costs() -&gt; dict[str, Decimal]\n</code></pre> <p>Get costs breakdown by stage.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_stage_costs(self) -&gt; dict[str, Decimal]:\n    \"\"\"Get costs breakdown by stage.\"\"\"\n    with self._lock:\n        return dict(self._stage_costs)\n</code></pre>"},{"location":"api/utils/input_preprocessing/","title":"input_preprocessing","text":""},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing","title":"input_preprocessing","text":"<p>Input preprocessing for LLM prompts.</p> <p>Best practices: Remove noise, normalize whitespace, control length.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.PreprocessingStats","title":"PreprocessingStats  <code>dataclass</code>","text":"<pre><code>PreprocessingStats(rows_processed: int, chars_before: int, chars_after: int, truncated_count: int, null_count: int)\n</code></pre> <p>Statistics from preprocessing operation.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.PreprocessingStats.reduction_pct","title":"reduction_pct  <code>property</code>","text":"<pre><code>reduction_pct: float\n</code></pre> <p>Calculate character reduction percentage.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextCleaner","title":"TextCleaner","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for text cleaning strategies.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextCleaner.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Clean text according to strategy.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Clean text according to strategy.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.UnicodeNormalizer","title":"UnicodeNormalizer","text":"<p>Normalize Unicode to canonical form (NFC).</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.UnicodeNormalizer.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Normalize Unicode: \u00e9 vs e + \u00b4 \u2192 consistent form.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Normalize Unicode: \u00e9 vs e + \u00b4 \u2192 consistent form.\"\"\"\n    return unicodedata.normalize(\"NFC\", text)\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.ControlCharRemover","title":"ControlCharRemover","text":"<p>Remove control characters that confuse tokenizers.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.ControlCharRemover.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Replace control chars with space (preserves word boundaries).</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Replace control chars with space (preserves word boundaries).\"\"\"\n    return \"\".join(\n        char if unicodedata.category(char)[0] != \"C\" else \" \" for char in text\n    )\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.SpecialCharCleaner","title":"SpecialCharCleaner","text":"<pre><code>SpecialCharCleaner(preserve: str = ',\\\\-/\\\\.\\\\(\\\\)&amp;')\n</code></pre> <p>Remove noise characters while preserving semantic punctuation.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, preserve: str = r\",\\-/\\.\\(\\)&amp;\"):\n    self.preserve = preserve\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.SpecialCharCleaner.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Remove \u00ae\u2122\u00a9 and excessive special chars.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Remove \u00ae\u2122\u00a9 and excessive special chars.\"\"\"\n    # Remove trademark symbols\n    text = re.sub(r\"[\u00ae\u2122\u00a9\u2117\u2120]\", \"\", text)\n\n    # Normalize quotes\n    text = text.replace('\"', '\"').replace('\"', '\"')\n    text = text.replace(\"\"\", \"'\").replace(\"\"\", \"'\")\n\n    # Remove zero-width characters\n    text = re.sub(r\"[\\u200b-\\u200f\\ufeff]\", \"\", text)\n\n    # Keep only: alphanumeric, whitespace, specified punctuation\n    pattern = f\"[^\\\\w\\\\s{self.preserve}]\"\n    return re.sub(pattern, \" \", text)\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.WhitespaceNormalizer","title":"WhitespaceNormalizer","text":"<p>Collapse multiple spaces/tabs/newlines.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.WhitespaceNormalizer.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Replace tabs/newlines with spaces, collapse multiples.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Replace tabs/newlines with spaces, collapse multiples.\"\"\"\n    text = text.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    return re.sub(r\"\\s+\", \" \", text).strip()\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextTruncator","title":"TextTruncator","text":"<pre><code>TextTruncator(max_length: int = 500)\n</code></pre> <p>Intelligently truncate at word boundaries.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, max_length: int = 500):\n    self.max_length = max_length\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextTruncator.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Truncate respecting word boundaries.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Truncate respecting word boundaries.\"\"\"\n    if len(text) &lt;= self.max_length:\n        return text\n\n    # Reserve space for ellipsis\n    limit = self.max_length - 3\n    min_keep = int(self.max_length * 0.7)\n\n    # Try delimiters first: |, ;, ' - '\n    for delim in [\"|\", \";\", \" - \", \"  \"]:\n        pos = text.rfind(delim, 0, limit)\n        if pos &gt; min_keep:\n            return text[:pos].strip() + \"...\"\n\n    # Fall back to last space\n    pos = text.rfind(\" \", 0, limit)\n    if pos &gt; min_keep:\n        return text[:pos].strip() + \"...\"\n\n    # Hard truncate if no boundary found\n    return text[:limit].strip() + \"...\"\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextPreprocessor","title":"TextPreprocessor","text":"<pre><code>TextPreprocessor(max_length: int = 500)\n</code></pre> <p>Composable text preprocessor following Chain of Responsibility.</p> <p>Single Responsibility: Orchestrate cleaning steps. Open/Closed: Extensible via cleaners list. Dependency Inversion: Depends on Protocol, not concrete classes.</p> <p>Initialize with default cleaning pipeline.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, max_length: int = 500):\n    \"\"\"Initialize with default cleaning pipeline.\"\"\"\n    self.cleaners: list[TextCleaner] = [\n        UnicodeNormalizer(),\n        ControlCharRemover(),\n        SpecialCharCleaner(),\n        WhitespaceNormalizer(),\n        TextTruncator(max_length),\n    ]\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextPreprocessor.process","title":"process","text":"<pre><code>process(text: str) -&gt; str\n</code></pre> <p>Apply all cleaners in sequence.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"Apply all cleaners in sequence.\"\"\"\n    if pd.isna(text) or not isinstance(text, str):\n        return \"\"\n\n    for cleaner in self.cleaners:\n        text = cleaner.clean(text)\n\n    return text\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextPreprocessor.add_cleaner","title":"add_cleaner","text":"<pre><code>add_cleaner(cleaner: TextCleaner) -&gt; None\n</code></pre> <p>Extend pipeline with custom cleaner.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def add_cleaner(self, cleaner: TextCleaner) -&gt; None:\n    \"\"\"Extend pipeline with custom cleaner.\"\"\"\n    self.cleaners.append(cleaner)\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.preprocess_dataframe","title":"preprocess_dataframe","text":"<pre><code>preprocess_dataframe(df: DataFrame, input_columns: list[str], max_length: int = 500) -&gt; tuple[pd.DataFrame, PreprocessingStats]\n</code></pre> <p>Preprocess input columns in dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe</p> required <code>input_columns</code> <code>list[str]</code> <p>Columns to clean</p> required <code>max_length</code> <code>int</code> <p>Max chars per field</p> <code>500</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, PreprocessingStats]</code> <p>(cleaned_df, stats)</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def preprocess_dataframe(\n    df: pd.DataFrame,\n    input_columns: list[str],\n    max_length: int = 500,\n) -&gt; tuple[pd.DataFrame, PreprocessingStats]:\n    \"\"\"\n    Preprocess input columns in dataframe.\n\n    Args:\n        df: Input dataframe\n        input_columns: Columns to clean\n        max_length: Max chars per field\n\n    Returns:\n        (cleaned_df, stats)\n    \"\"\"\n    result = df.copy()\n    preprocessor = TextPreprocessor(max_length)\n\n    chars_before = 0\n    chars_after = 0\n    truncated = 0\n    nulls = 0\n\n    for col in input_columns:\n        if col not in result.columns:\n            continue\n\n        for idx in result.index:\n            original = result.at[idx, col]\n\n            if pd.isna(original):\n                nulls += 1\n                continue\n\n            original_str = str(original)\n            chars_before += len(original_str)\n\n            cleaned = preprocessor.process(original_str)\n            chars_after += len(cleaned)\n\n            if len(original_str) &gt; max_length:\n                truncated += 1\n\n            result.at[idx, col] = cleaned\n\n    stats = PreprocessingStats(\n        rows_processed=len(result),\n        chars_before=chars_before,\n        chars_after=chars_after,\n        truncated_count=truncated,\n        null_count=nulls,\n    )\n\n    return result, stats\n</code></pre>"},{"location":"api/utils/logging_utils/","title":"logging_utils","text":""},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils","title":"logging_utils","text":"<p>Structured logging utilities.</p> <p>Provides consistent logging configuration across the SDK using structlog.</p>"},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(level: str = 'INFO', json_format: bool = False, include_timestamp: bool = True) -&gt; None\n</code></pre> <p>Configure structured logging for the SDK.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> <code>json_format</code> <code>bool</code> <p>Use JSON output format</p> <code>False</code> <code>include_timestamp</code> <code>bool</code> <p>Include timestamps in logs</p> <code>True</code> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def configure_logging(\n    level: str = \"INFO\",\n    json_format: bool = False,\n    include_timestamp: bool = True,\n) -&gt; None:\n    \"\"\"\n    Configure structured logging for the SDK.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        json_format: Use JSON output format\n        include_timestamp: Include timestamps in logs\n    \"\"\"\n    global _logging_configured\n\n    # Set stdlib logging level\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=getattr(logging, level.upper()),\n    )\n\n    # Configure structlog processors\n    processors = [\n        structlog.contextvars.merge_contextvars,\n        structlog.processors.add_log_level,\n        structlog.processors.StackInfoRenderer(),\n    ]\n\n    if include_timestamp:\n        processors.append(structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"))\n\n    if json_format:\n        processors.append(structlog.processors.JSONRenderer())\n    else:\n        # Use custom compact console renderer (no padding)\n        processors.append(_compact_console_renderer)\n\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.make_filtering_bound_logger(\n            getattr(logging, level.upper())\n        ),\n        context_class=dict,\n        logger_factory=structlog.PrintLoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    _logging_configured = True\n</code></pre>"},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils.get_logger","title":"get_logger","text":"<pre><code>get_logger(name: str) -&gt; structlog.BoundLogger\n</code></pre> <p>Get a structured logger instance.</p> <p>Auto-configures logging on first use if not already configured.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically name)</p> required <p>Returns:</p> Type Description <code>BoundLogger</code> <p>Configured structlog logger</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def get_logger(name: str) -&gt; structlog.BoundLogger:\n    \"\"\"\n    Get a structured logger instance.\n\n    Auto-configures logging on first use if not already configured.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured structlog logger\n    \"\"\"\n    global _logging_configured\n\n    # Auto-configure logging on first use\n    if not _logging_configured:\n        configure_logging()\n\n    return structlog.get_logger(name)\n</code></pre>"},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils.sanitize_for_logging","title":"sanitize_for_logging","text":"<pre><code>sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Sanitize sensitive data for logging.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary potentially containing sensitive data</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Sanitized dictionary</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Sanitize sensitive data for logging.\n\n    Args:\n        data: Dictionary potentially containing sensitive data\n\n    Returns:\n        Sanitized dictionary\n    \"\"\"\n    sensitive_keys = {\n        \"api_key\",\n        \"password\",\n        \"secret\",\n        \"token\",\n        \"authorization\",\n        \"credential\",\n    }\n\n    sanitized = {}\n    for key, value in data.items():\n        key_lower = key.lower()\n        if any(sensitive in key_lower for sensitive in sensitive_keys):\n            sanitized[key] = \"***REDACTED***\"\n        elif isinstance(value, dict):\n            sanitized[key] = sanitize_for_logging(value)\n        else:\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"api/utils/metrics_exporter/","title":"metrics_exporter","text":""},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter","title":"metrics_exporter","text":"<p>Prometheus metrics export for monitoring.</p> <p>Provides instrumentation for external monitoring systems.</p>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics","title":"PrometheusMetrics","text":"<pre><code>PrometheusMetrics(port: int = 9090)\n</code></pre> <p>Prometheus metrics exporter.</p> <p>Follows Single Responsibility: only handles metrics export.</p> <p>Initialize Prometheus metrics.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Port for metrics HTTP server</p> <code>9090</code> Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def __init__(self, port: int = 9090):\n    \"\"\"\n    Initialize Prometheus metrics.\n\n    Args:\n        port: Port for metrics HTTP server\n    \"\"\"\n    self.port = port\n    self._server_started = False\n\n    # Define metrics\n    self.requests_total = Counter(\n        \"llm_requests_total\",\n        \"Total LLM requests\",\n        [\"provider\", \"model\", \"stage\"],\n    )\n\n    self.request_duration = Histogram(\n        \"llm_request_duration_seconds\",\n        \"LLM request duration in seconds\",\n        [\"provider\", \"stage\"],\n    )\n\n    self.cost_total = Gauge(\n        \"llm_cost_total_usd\",\n        \"Total cost in USD\",\n        [\"provider\"],\n    )\n\n    self.errors_total = Counter(\n        \"llm_errors_total\",\n        \"Total errors\",\n        [\"stage\", \"error_type\"],\n    )\n\n    self.rows_processed = Gauge(\n        \"llm_rows_processed_total\",\n        \"Total rows processed\",\n        [\"stage\"],\n    )\n\n    self.rows_per_second = Gauge(\n        \"llm_rows_per_second\",\n        \"Processing throughput\",\n    )\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.start_server","title":"start_server","text":"<pre><code>start_server() -&gt; None\n</code></pre> <p>Start HTTP server for metrics endpoint.</p> Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def start_server(self) -&gt; None:\n    \"\"\"Start HTTP server for metrics endpoint.\"\"\"\n    if not self._server_started:\n        try:\n            start_http_server(self.port)\n            self._server_started = True\n            logger.info(f\"Prometheus metrics server started on port {self.port}\")\n        except Exception as e:\n            logger.error(f\"Failed to start metrics server: {e}\")\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_request","title":"record_request","text":"<pre><code>record_request(provider: str, model: str, stage: str, duration: float) -&gt; None\n</code></pre> <p>Record LLM request metrics.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>model</code> <code>str</code> <p>Model name</p> required <code>stage</code> <code>str</code> <p>Stage name</p> required <code>duration</code> <code>float</code> <p>Request duration in seconds</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_request(\n    self, provider: str, model: str, stage: str, duration: float\n) -&gt; None:\n    \"\"\"\n    Record LLM request metrics.\n\n    Args:\n        provider: Provider name\n        model: Model name\n        stage: Stage name\n        duration: Request duration in seconds\n    \"\"\"\n    self.requests_total.labels(provider=provider, model=model, stage=stage).inc()\n\n    self.request_duration.labels(provider=provider, stage=stage).observe(duration)\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_cost","title":"record_cost","text":"<pre><code>record_cost(provider: str, cost: float) -&gt; None\n</code></pre> <p>Record cost metric.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>cost</code> <code>float</code> <p>Cost in USD</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_cost(self, provider: str, cost: float) -&gt; None:\n    \"\"\"\n    Record cost metric.\n\n    Args:\n        provider: Provider name\n        cost: Cost in USD\n    \"\"\"\n    self.cost_total.labels(provider=provider).set(cost)\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_error","title":"record_error","text":"<pre><code>record_error(stage: str, error_type: str) -&gt; None\n</code></pre> <p>Record error metric.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Stage name</p> required <code>error_type</code> <code>str</code> <p>Error type</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_error(self, stage: str, error_type: str) -&gt; None:\n    \"\"\"\n    Record error metric.\n\n    Args:\n        stage: Stage name\n        error_type: Error type\n    \"\"\"\n    self.errors_total.labels(stage=stage, error_type=error_type).inc()\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_rows_processed","title":"record_rows_processed","text":"<pre><code>record_rows_processed(stage: str, count: int) -&gt; None\n</code></pre> <p>Record rows processed.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Stage name</p> required <code>count</code> <code>int</code> <p>Number of rows</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_rows_processed(self, stage: str, count: int) -&gt; None:\n    \"\"\"\n    Record rows processed.\n\n    Args:\n        stage: Stage name\n        count: Number of rows\n    \"\"\"\n    self.rows_processed.labels(stage=stage).set(count)\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_throughput","title":"record_throughput","text":"<pre><code>record_throughput(rows_per_second: float) -&gt; None\n</code></pre> <p>Record processing throughput.</p> <p>Parameters:</p> Name Type Description Default <code>rows_per_second</code> <code>float</code> <p>Throughput metric</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_throughput(self, rows_per_second: float) -&gt; None:\n    \"\"\"\n    Record processing throughput.\n\n    Args:\n        rows_per_second: Throughput metric\n    \"\"\"\n    self.rows_per_second.set(rows_per_second)\n</code></pre>"},{"location":"api/utils/model_context_limits/","title":"model_context_limits","text":""},{"location":"api/utils/model_context_limits/#ondine.utils.model_context_limits","title":"model_context_limits","text":"<p>Model context window limits registry.</p> <p>Provides context window limits for various LLM models to enable intelligent batch size validation and optimization.</p>"},{"location":"api/utils/model_context_limits/#ondine.utils.model_context_limits.get_context_limit","title":"get_context_limit","text":"<pre><code>get_context_limit(model: str) -&gt; int\n</code></pre> <p>Get context window limit for a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier</p> required <p>Returns:</p> Type Description <code>int</code> <p>Context window limit in tokens</p> Example <p>get_context_limit(\"gpt-4o-mini\") 128000 get_context_limit(\"unknown-model\") 4096  # Default fallback</p> Source code in <code>ondine/utils/model_context_limits.py</code> <pre><code>def get_context_limit(model: str) -&gt; int:\n    \"\"\"Get context window limit for a model.\n\n    Args:\n        model: Model identifier\n\n    Returns:\n        Context window limit in tokens\n\n    Example:\n        &gt;&gt;&gt; get_context_limit(\"gpt-4o-mini\")\n        128000\n        &gt;&gt;&gt; get_context_limit(\"unknown-model\")\n        4096  # Default fallback\n    \"\"\"\n    # Direct lookup\n    if model in MODEL_CONTEXT_LIMITS:\n        return MODEL_CONTEXT_LIMITS[model]\n\n    # Fuzzy matching for model variants\n    model_lower = model.lower()\n\n    # Check for partial matches\n    for known_model, limit in MODEL_CONTEXT_LIMITS.items():\n        if known_model.lower() in model_lower or model_lower in known_model.lower():\n            logger.debug(f\"Fuzzy matched '{model}' to '{known_model}' (limit: {limit})\")\n            return limit\n\n    # Fallback to default\n    logger.warning(\n        f\"Unknown model '{model}', using default context limit: \"\n        f\"{MODEL_CONTEXT_LIMITS['default']} tokens\"\n    )\n    return MODEL_CONTEXT_LIMITS[\"default\"]\n</code></pre>"},{"location":"api/utils/model_context_limits/#ondine.utils.model_context_limits.validate_batch_size","title":"validate_batch_size","text":"<pre><code>validate_batch_size(model: str, batch_size: int, avg_prompt_tokens: int, safety_margin: float = 0.8) -&gt; tuple[bool, str | None]\n</code></pre> <p>Validate batch size against model context window.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier</p> required <code>batch_size</code> <code>int</code> <p>Proposed batch size</p> required <code>avg_prompt_tokens</code> <code>int</code> <p>Average tokens per prompt</p> required <code>safety_margin</code> <code>float</code> <p>Use only this fraction of context window (default: 0.8)</p> <code>0.8</code> <p>Returns:</p> Type Description <code>bool</code> <p>Tuple of (is_valid, error_message)</p> <code>str | None</code> <ul> <li>(True, None) if valid</li> </ul> <code>tuple[bool, str | None]</code> <ul> <li>(False, error_message) if invalid</li> </ul> Example <p>validate_batch_size(\"gpt-4o-mini\", 100, 500) (True, None)  # 100 * 500 = 50K &lt; 128K * 0.8 validate_batch_size(\"gpt-4o-mini\", 1000, 500) (False, \"Batch too large: ...\")  # 1000 * 500 = 500K &gt; 128K * 0.8</p> Source code in <code>ondine/utils/model_context_limits.py</code> <pre><code>def validate_batch_size(\n    model: str,\n    batch_size: int,\n    avg_prompt_tokens: int,\n    safety_margin: float = 0.8,\n) -&gt; tuple[bool, str | None]:\n    \"\"\"Validate batch size against model context window.\n\n    Args:\n        model: Model identifier\n        batch_size: Proposed batch size\n        avg_prompt_tokens: Average tokens per prompt\n        safety_margin: Use only this fraction of context window (default: 0.8)\n\n    Returns:\n        Tuple of (is_valid, error_message)\n        - (True, None) if valid\n        - (False, error_message) if invalid\n\n    Example:\n        &gt;&gt;&gt; validate_batch_size(\"gpt-4o-mini\", 100, 500)\n        (True, None)  # 100 * 500 = 50K &lt; 128K * 0.8\n        &gt;&gt;&gt; validate_batch_size(\"gpt-4o-mini\", 1000, 500)\n        (False, \"Batch too large: ...\")  # 1000 * 500 = 500K &gt; 128K * 0.8\n    \"\"\"\n    context_limit = get_context_limit(model)\n    safe_limit = int(context_limit * safety_margin)\n\n    estimated_tokens = batch_size * avg_prompt_tokens\n\n    if estimated_tokens &gt; safe_limit:\n        error = (\n            f\"Batch size too large: {batch_size} rows \u00d7 {avg_prompt_tokens} tokens/row \"\n            f\"= {estimated_tokens} tokens, exceeds {safety_margin * 100:.0f}% of \"\n            f\"context window ({safe_limit} tokens). \"\n            f\"Reduce batch_size to {safe_limit // avg_prompt_tokens} or less.\"\n        )\n        return False, error\n\n    return True, None\n</code></pre>"},{"location":"api/utils/model_context_limits/#ondine.utils.model_context_limits.suggest_optimal_batch_size","title":"suggest_optimal_batch_size","text":"<pre><code>suggest_optimal_batch_size(model: str, avg_prompt_tokens: int, safety_margin: float = 0.8) -&gt; int\n</code></pre> <p>Suggest optimal batch size for a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier</p> required <code>avg_prompt_tokens</code> <code>int</code> <p>Average tokens per prompt</p> required <code>safety_margin</code> <code>float</code> <p>Use only this fraction of context window</p> <code>0.8</code> <p>Returns:</p> Type Description <code>int</code> <p>Suggested batch size</p> Example <p>suggest_optimal_batch_size(\"gpt-4o-mini\", 500) 204  # (128000 * 0.8) / 500</p> Source code in <code>ondine/utils/model_context_limits.py</code> <pre><code>def suggest_optimal_batch_size(\n    model: str,\n    avg_prompt_tokens: int,\n    safety_margin: float = 0.8,\n) -&gt; int:\n    \"\"\"Suggest optimal batch size for a model.\n\n    Args:\n        model: Model identifier\n        avg_prompt_tokens: Average tokens per prompt\n        safety_margin: Use only this fraction of context window\n\n    Returns:\n        Suggested batch size\n\n    Example:\n        &gt;&gt;&gt; suggest_optimal_batch_size(\"gpt-4o-mini\", 500)\n        204  # (128000 * 0.8) / 500\n    \"\"\"\n    context_limit = get_context_limit(model)\n    safe_limit = int(context_limit * safety_margin)\n\n    suggested = safe_limit // avg_prompt_tokens\n\n    # Cap at reasonable maximum\n    return min(suggested, 500)\n</code></pre>"},{"location":"api/utils/rate_limiter/","title":"rate_limiter","text":""},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter","title":"rate_limiter","text":"<p>Token bucket rate limiter for API calls.</p> <p>Implements token bucket algorithm for rate limiting.</p>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter","title":"RateLimiter","text":"<pre><code>RateLimiter(requests_per_minute: int, burst_size: int | None = None)\n</code></pre> <p>Token bucket rate limiter for controlling API request rates.</p> <p>Thread-safe implementation.</p> <p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_minute</code> <code>int</code> <p>Maximum requests per minute</p> required <code>burst_size</code> <code>int | None</code> <p>Maximum burst size (default: requests_per_minute)</p> <code>None</code> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def __init__(self, requests_per_minute: int, burst_size: int | None = None):\n    \"\"\"\n    Initialize rate limiter.\n\n    Args:\n        requests_per_minute: Maximum requests per minute\n        burst_size: Maximum burst size (default: requests_per_minute)\n    \"\"\"\n    self.rpm = requests_per_minute\n    self.capacity = burst_size or requests_per_minute\n    self.tokens = float(self.capacity)\n    self.last_update = time.time()\n    self.lock = threading.Lock()\n\n    # Calculate refill rate (tokens per second)\n    self.refill_rate = requests_per_minute / 60.0\n</code></pre>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter.available_tokens","title":"available_tokens  <code>property</code>","text":"<pre><code>available_tokens: float\n</code></pre> <p>Get current available tokens.</p>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter.acquire","title":"acquire","text":"<pre><code>acquire(tokens: int = 1, timeout: float | None = None) -&gt; bool\n</code></pre> <p>Acquire tokens for making requests.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>int</code> <p>Number of tokens to acquire</p> <code>1</code> <code>timeout</code> <code>float | None</code> <p>Maximum wait time in seconds (None = wait forever)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if tokens acquired, False if timeout</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokens &gt; capacity</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def acquire(self, tokens: int = 1, timeout: float | None = None) -&gt; bool:\n    \"\"\"\n    Acquire tokens for making requests.\n\n    Args:\n        tokens: Number of tokens to acquire\n        timeout: Maximum wait time in seconds (None = wait forever)\n\n    Returns:\n        True if tokens acquired, False if timeout\n\n    Raises:\n        ValueError: If tokens &gt; capacity\n    \"\"\"\n    if tokens &gt; self.capacity:\n        raise ValueError(\n            f\"Requested {tokens} tokens exceeds capacity {self.capacity}\"\n        )\n\n    deadline = None if timeout is None else time.time() + timeout\n\n    while True:\n        with self.lock:\n            self._refill()\n\n            if self.tokens &gt;= tokens:\n                self.tokens -= tokens\n                return True\n\n        # Check timeout\n        if deadline is not None and time.time() &gt;= deadline:\n            return False\n\n        # Sleep before retry\n        time.sleep(0.1)\n</code></pre>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset rate limiter to full capacity.</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset rate limiter to full capacity.\"\"\"\n    with self.lock:\n        self.tokens = float(self.capacity)\n        self.last_update = time.time()\n</code></pre>"},{"location":"api/utils/retry_handler/","title":"retry_handler","text":""},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler","title":"retry_handler","text":"<p>Retry handling with exponential backoff.</p> <p>Provides robust retry logic for transient failures.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryableError","title":"RetryableError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors that should be retried.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RateLimitError","title":"RateLimitError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Rate limit exceeded error.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.NetworkError","title":"NetworkError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Network-related error.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryHandler","title":"RetryHandler","text":"<pre><code>RetryHandler(max_attempts: int = 3, initial_delay: float = 1.0, max_delay: float = 60.0, exponential_base: int = 2, retryable_exceptions: tuple[type[Exception], ...] | None = None)\n</code></pre> <p>Request-level retry with exponential backoff (for transient errors).</p> <p>Scope: Single LLM API call or operation Use when: Transient errors (rate limits, network timeouts, API hiccups) NOT for: Row-level quality issues (use Pipeline.auto_retry_failed for that)</p> <p>Retry Strategy: - Exponential backoff (1s, 2s, 4s, 8s, ...) - Configurable max attempts (default: 3) - Only retries specific exception types</p> Example <p>handler = RetryHandler(max_attempts=3, initial_delay=1.0) result = handler.execute(lambda: call_llm_api())</p> <p>See Also: - ErrorHandler: Orchestrates retry decisions based on policy - Pipeline._auto_retry_failed_rows(): Row-level quality retry - docs/architecture/decisions/ADR-006-retry-levels.md</p> <p>Initialize retry handler.</p> <p>Parameters:</p> Name Type Description Default <code>max_attempts</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay in seconds</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds</p> <code>60.0</code> <code>exponential_base</code> <code>int</code> <p>Base for exponential backoff</p> <code>2</code> <code>retryable_exceptions</code> <code>tuple[type[Exception], ...] | None</code> <p>Exception types to retry</p> <code>None</code> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def __init__(\n    self,\n    max_attempts: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: int = 2,\n    retryable_exceptions: tuple[type[Exception], ...] | None = None,\n):\n    \"\"\"\n    Initialize retry handler.\n\n    Args:\n        max_attempts: Maximum retry attempts\n        initial_delay: Initial delay in seconds\n        max_delay: Maximum delay in seconds\n        exponential_base: Base for exponential backoff\n        retryable_exceptions: Exception types to retry\n    \"\"\"\n    self.max_attempts = max_attempts\n    self.initial_delay = initial_delay\n    self.max_delay = max_delay\n    self.exponential_base = exponential_base\n\n    if retryable_exceptions is None:\n        self.retryable_exceptions = (\n            RetryableError,\n            RateLimitError,\n            NetworkError,\n        )\n    else:\n        self.retryable_exceptions = retryable_exceptions\n</code></pre>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryHandler.execute","title":"execute","text":"<pre><code>execute(func: Callable[[], T]) -&gt; T\n</code></pre> <p>Execute function with retry logic.</p> <p>Only retries exceptions in self.retryable_exceptions tuple. NonRetryableError and its subclasses are re-raised immediately.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[], T]</code> <p>Function to execute</p> required <p>Returns:</p> Type Description <code>T</code> <p>Result from function</p> <p>Raises:</p> Type Description <code>NonRetryableError</code> <p>Fatal errors (model not found, invalid API key, etc.)</p> <code>Exception</code> <p>If all retries exhausted for retryable errors</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def execute(self, func: Callable[[], T]) -&gt; T:\n    \"\"\"\n    Execute function with retry logic.\n\n    Only retries exceptions in self.retryable_exceptions tuple.\n    NonRetryableError and its subclasses are re-raised immediately.\n\n    Args:\n        func: Function to execute\n\n    Returns:\n        Result from function\n\n    Raises:\n        NonRetryableError: Fatal errors (model not found, invalid API key, etc.)\n        Exception: If all retries exhausted for retryable errors\n    \"\"\"\n    retryer = Retrying(\n        stop=stop_after_attempt(self.max_attempts),\n        wait=wait_exponential(\n            multiplier=self.initial_delay,\n            max=self.max_delay,\n            exp_base=self.exponential_base,\n        ),\n        retry=retry_if_exception_type(self.retryable_exceptions),\n        reraise=True,\n    )\n\n    return retryer(func)\n</code></pre>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryHandler.calculate_delay","title":"calculate_delay","text":"<pre><code>calculate_delay(attempt: int) -&gt; float\n</code></pre> <p>Calculate delay for given attempt number.</p> <p>Parameters:</p> Name Type Description Default <code>attempt</code> <code>int</code> <p>Attempt number (1-based)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def calculate_delay(self, attempt: int) -&gt; float:\n    \"\"\"\n    Calculate delay for given attempt number.\n\n    Args:\n        attempt: Attempt number (1-based)\n\n    Returns:\n        Delay in seconds\n    \"\"\"\n    delay = self.initial_delay * (self.exponential_base ** (attempt - 1))\n    return min(delay, self.max_delay)\n</code></pre>"},{"location":"architecture/technical-reference/","title":"Ondine - Complete Technical Reference","text":"<p>Version: 1.0.0 Last Updated: October 18, 2025 Purpose: Comprehensive technical documentation of every component, class, design decision, and relationship in the Ondine LLM Dataset Engine.</p> <p>Quick Navigation: - Architecture Overview &amp; Diagrams: See <code>ARCHITECTURE.md</code> (auto-generated from <code>architecture/model.yaml</code>) - Design Decisions: See <code>architecture/decisions/</code> (ADRs) - Implementation Details: This document (TECHNICAL_REFERENCE.md)</p> <p>Note: This document provides detailed implementation information. For structural relationships and visual diagrams, see <code>ARCHITECTURE.md</code>. For design rationale and trade-offs, see the ADRs.</p>"},{"location":"architecture/technical-reference/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Part 1: Architecture Overview</li> <li>Part 2: External Dependencies</li> <li>Part 3: Layer 0 - Core Utilities</li> <li>Part 4: Core Models &amp; Specifications</li> <li>Part 5: Layer 1 - Infrastructure Adapters</li> <li>Part 6: Layer 2 - Processing Stages</li> <li>Part 7: Layer 3 - Orchestration Engine</li> <li>Part 8: Layer 4 - High-Level API</li> <li>Part 9: Configuration System</li> <li>Part 10: CLI Interface</li> <li>Part 11: Framework Integrations</li> <li>Part 12: Execution Flows</li> <li>Part 13: Data Flows</li> <li>Part 14: Extension Points</li> </ul>"},{"location":"architecture/technical-reference/#part-1-architecture-overview","title":"Part 1: Architecture Overview","text":""},{"location":"architecture/technical-reference/#11-system-architecture","title":"1.1 System Architecture","text":"<p>Ondine follows a 5-layer architecture:</p> <pre><code>graph TB\n    subgraph \"Layer 4: High-Level API\"\n        API[Pipeline]\n        Builder[PipelineBuilder]\n        Composer[PipelineComposer]\n        Processor[DatasetProcessor]\n    end\n\n    subgraph \"Layer 3: Orchestration\"\n        Executor[PipelineExecutor]\n        Strategy[ExecutionStrategy]\n        SyncExec[SyncExecutor]\n        AsyncExec[AsyncExecutor]\n        StreamExec[StreamingExecutor]\n        Context[ExecutionContext]\n        StateManager[StateManager]\n        Observers[Observers]\n    end\n\n    subgraph \"Layer 2: Processing Stages\"\n        DataLoader[DataLoaderStage]\n        PromptFormatter[PromptFormatterStage]\n        BatchAgg[BatchAggregatorStage]\n        LLMInvocation[LLMInvocationStage]\n        BatchDisagg[BatchDisaggregatorStage]\n        ResponseParser[ResponseParserStage]\n        ResultWriter[ResultWriterStage]\n    end\n\n    subgraph \"Layer 1: Infrastructure Adapters\"\n        LLMClient[LLM Client]\n        DataIO[Data I/O]\n        Checkpoint[Checkpoint Storage]\n    end\n\n    subgraph \"Layer 0: Utilities\"\n        Retry[RetryHandler]\n        RateLimit[RateLimiter]\n        Cost[CostTracker]\n        Budget[BudgetController]\n        Logging[Logging Utils]\n    end\n\n    API --&gt; Executor\n    Builder --&gt; API\n    Executor --&gt; Strategy\n    Strategy --&gt; SyncExec\n    Strategy --&gt; AsyncExec\n    Strategy --&gt; StreamExec\n    Executor --&gt; Context\n    Executor --&gt; StateManager\n    Executor --&gt; Observers\n    Executor --&gt; DataLoader\n    DataLoader --&gt; PromptFormatter\n    PromptFormatter --&gt; BatchAgg\n    BatchAgg --&gt; LLMInvocation\n    LLMInvocation --&gt; BatchDisagg\n    BatchDisagg --&gt; ResponseParser\n    ResponseParser --&gt; ResultWriter\n    LLMInvocation --&gt; LLMClient\n    DataLoader --&gt; DataIO\n    ResultWriter --&gt; DataIO\n    StateManager --&gt; Checkpoint\n    LLMInvocation --&gt; Retry\n    LLMInvocation --&gt; RateLimit\n    LLMInvocation --&gt; Cost\n    Executor --&gt; Budget\n</code></pre>"},{"location":"architecture/technical-reference/#layer-responsibilities","title":"Layer Responsibilities","text":"Layer Directory Purpose Dependencies Layer 0 <code>utils/</code> Cross-cutting concerns (retry, rate limiting, cost tracking) External libraries only Layer 1 <code>adapters/</code> External system integrations (LLM providers, file I/O) Layer 0 + external APIs Layer 2 <code>stages/</code> Data transformation logic (load, format, batch, invoke, parse, write) Layers 0-1 Layer 3 <code>orchestration/</code> Execution control and state management Layers 0-2 Layer 4 <code>api/</code> User-facing interfaces (Pipeline, Builder) All layers"},{"location":"architecture/technical-reference/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Dependency Rule: Dependencies only point inward (higher layers depend on lower layers, never the reverse)</li> <li>Focused Components: Each component has one clear purpose</li> <li>Extensible: Open for extension, closed for modification</li> <li>Abstraction: Depend on abstractions, not concretions</li> <li>Simple: Keep it simple</li> </ol>"},{"location":"architecture/technical-reference/#12-design-patterns-catalog","title":"1.2 Design Patterns Catalog","text":"Pattern Where Used Purpose Implementation Facade <code>Pipeline</code> Simplify complex subsystem Hides orchestration complexity Builder <code>PipelineBuilder</code> Fluent construction API Chainable method calls Strategy <code>ExecutionStrategy</code> Pluggable execution modes <code>SyncExecutor</code>, <code>AsyncExecutor</code>, <code>StreamingExecutor</code> Template Method <code>PipelineStage.execute()</code> Standardized stage flow Base class with hooks Observer <code>ExecutionObserver</code> Monitoring hooks <code>ProgressObserver</code>, <code>CostObserver</code>, <code>LoggingObserver</code> Adapter <code>LLMClient</code>, <code>DataReader</code> Interface translation Wrap external libraries Factory <code>ParserFactory</code> Object creation Create parsers by type Composite <code>PipelineComposer</code> Tree structure Compose pipelines Singleton Config instances Single instance Via module-level state Chain of Responsibility Stage pipeline Sequential processing Each stage processes then passes Protocol <code>TextCleaner</code> Structural typing Duck typing with validation Memento <code>ExecutionContext</code> State capture Serializable state"},{"location":"architecture/technical-reference/#13-thread-safety-strategy","title":"1.3 Thread Safety Strategy","text":""},{"location":"architecture/technical-reference/#thread-safe-components","title":"Thread-Safe Components","text":"Component Mechanism Reason <code>CostTracker</code> <code>threading.Lock</code> Shared cost accumulation <code>RateLimiter</code> <code>threading.Lock</code> Token bucket state <code>CheckpointStorage</code> File-based locks Concurrent writes <code>LLMInvocationStage</code> <code>ThreadPoolExecutor</code> Concurrent LLM calls"},{"location":"architecture/technical-reference/#thread-unsafe-components-by-design","title":"Thread-Unsafe Components (By Design)","text":"Component Why Not Thread-Safe <code>PipelineBuilder</code> Construction phase only, not shared <code>Pipeline</code> (single exec) Each execution is sequential <code>DataLoaderStage</code> Reads once, no shared state"},{"location":"architecture/technical-reference/#part-2-external-dependencies","title":"Part 2: External Dependencies","text":""},{"location":"architecture/technical-reference/#21-dependency-catalog","title":"2.1 Dependency Catalog","text":""},{"location":"architecture/technical-reference/#production-dependencies-20-libraries","title":"Production Dependencies (20 libraries)","text":"Library Version Category License Why Chosen Alternatives Considered llama-index &gt;=0.12.0 LLM MIT LLM provider clients (OpenAI, Anthropic, Groq). Ondine adds batch orchestration, cost tracking, checkpointing, YAML config. LangChain (more complex), direct APIs (no abstraction) llama-index-llms-openai &gt;=0.3.0 LLM MIT Official OpenAI integration <code>openai</code> package (less abstraction) llama-index-llms-azure-openai &gt;=0.3.0 LLM MIT Enterprise Azure support Custom Azure client llama-index-llms-anthropic &gt;=0.3.0 LLM MIT Claude integration <code>anthropic</code> package (less abstraction) llama-index-llms-groq &gt;=0.3.0 LLM MIT Fast, affordable inference Direct Groq API pandas &gt;=2.0.0 Data BSD-3 Industry standard, rich API Polars (less mature ecosystem) polars &gt;=0.20.0 Data MIT Fast Parquet reading Pandas (slower for large files) pydantic &gt;=2.0.0 Validation MIT Validation + serialization + type hints dataclasses (no validation), marshmallow (complex) python-dotenv &gt;=1.0.0 Config BSD-3 Simple .env loading os.environ (manual) tqdm &gt;=4.66.0 UI MPL/MIT Simple, widely used progress bars rich.progress (overkill), progressbar2 tenacity &gt;=8.2.0 Reliability Apache-2.0 Flexible retry logic <code>backoff</code> (simpler but less flexible) openpyxl &gt;=3.1.0 Data MIT Excel file support xlrd (deprecated), pyexcel pyarrow &gt;=15.0.0 Data Apache-2.0 Fast Parquet I/O fastparquet (slower) tiktoken &gt;=0.5.0 LLM MIT Fast token counting, OpenAI-native transformers (slower), estimate (inaccurate) structlog &gt;=24.0.0 Logging MIT/Apache Structured logging, JSON output standard logging (unstructured) jinja2 &gt;=3.1.0 Templating BSD-3 Powerful prompt templating string.Template (too simple), mako prometheus-client &gt;=0.20.0 Monitoring Apache-2.0 Industry standard metrics Custom metrics (reinvent wheel) click &gt;=8.1.0 CLI BSD-3 Decorator-based, simple argparse (verbose), typer (overkill) rich &gt;=13.0.0 CLI MIT Beautiful tables and formatting colorama (basic), termcolor"},{"location":"architecture/technical-reference/#dev-dependencies-8-libraries","title":"Dev Dependencies (8 libraries)","text":"Library Purpose Why Chosen pytest Testing framework Industry standard, rich plugins pytest-cov Coverage reporting Integrates with pytest pytest-asyncio Async test support Test async executors black Code formatting Opinionated, consistent ruff Fast linting Faster than flake8/pylint mypy Type checking Catch type errors ipython Interactive shell Better REPL jupyter Notebooks Interactive exploration"},{"location":"architecture/technical-reference/#optional-dependencies-1-group","title":"Optional Dependencies (1 group)","text":"Group Libraries Purpose Platform mlx mlx&gt;=0.29.0, mlx-lm&gt;=0.28.0 Apple Silicon local inference macOS only (M1/M2/M3/M4) <p>Installation: <code>pip install ondine[mlx]</code></p>"},{"location":"architecture/technical-reference/#22-dependency-graph","title":"2.2 Dependency Graph","text":"<pre><code>graph TD\n    Ondine[Ondine Core]\n\n    Ondine --&gt; LI[llama-index]\n    Ondine --&gt; Pandas\n    Ondine --&gt; Pydantic\n    Ondine --&gt; Structlog\n    Ondine --&gt; Click\n    Ondine --&gt; Rich\n\n    LI --&gt; LIOAI[llama-index-llms-openai]\n    LI --&gt; LIAZ[llama-index-llms-azure]\n    LI --&gt; LIANT[llama-index-llms-anthropic]\n    LI --&gt; LIGR[llama-index-llms-groq]\n\n    Pandas --&gt; Openpyxl[openpyxl]\n    Pandas --&gt; Polars\n\n    Ondine --&gt; Tiktoken[tiktoken]\n    Ondine --&gt; Tenacity[tenacity]\n    Ondine --&gt; Tqdm[tqdm]\n    Ondine --&gt; Jinja2[jinja2]\n    Ondine --&gt; Prometheus[prometheus-client]\n    Ondine --&gt; Dotenv[python-dotenv]\n\n    Ondine -.-&gt; MLX[mlx + mlx-lm]\n    MLX -.-&gt; MLXMetal[mlx-metal]\n\n    Polars --&gt; PyArrow[pyarrow]\n\n    style MLX stroke-dasharray: 5 5\n    style MLXMetal stroke-dasharray: 5 5\n</code></pre> <p>Note: Dashed lines indicate optional dependencies (<code>pip install ondine[mlx]</code>)</p>"},{"location":"architecture/technical-reference/#critical-dependencies-cannot-be-removed","title":"Critical Dependencies (Cannot Be Removed)","text":"<ol> <li>llama-index - Core dependency providing:</li> <li>LLM provider clients (OpenAI, Anthropic, Groq, Azure)</li> <li>Observability instrumentation (automatic LLM call tracking)</li> <li>Future RAG capabilities (vector stores, query engines)</li> <li>Ondine wraps with: batch orchestration, cost tracking, checkpointing, YAML config</li> <li>pandas - Data manipulation backbone, used throughout</li> <li>pydantic - Configuration validation, type safety</li> <li>structlog - Structured logging, observability</li> <li>opentelemetry-api + opentelemetry-sdk - Observability infrastructure (via LlamaIndex)</li> <li>langfuse - LLM-specific observability platform (via LlamaIndex)</li> </ol>"},{"location":"architecture/technical-reference/#optional-dependencies-can-be-removed","title":"Optional Dependencies (Can Be Removed)","text":"<ol> <li>polars - Only for fast Parquet reading, pandas can handle it (slower)</li> <li>prometheus-client - Only for metrics export, can be disabled</li> <li>rich - Only for CLI pretty printing, can fall back to basic output</li> <li>jinja2 - Only for advanced templating, can use string.format()</li> <li>mlx + mlx-lm - Only for Apple Silicon local inference, use cloud providers instead</li> </ol>"},{"location":"architecture/technical-reference/#part-5-layer-1-infrastructure-adapters-llm-providers","title":"Part 5: Layer 1 - Infrastructure Adapters (LLM Providers)","text":""},{"location":"architecture/technical-reference/#51-llm-provider-overview","title":"5.1 LLM Provider Overview","text":"<p>Ondine supports multiple LLM providers through the Adapter pattern, allowing easy switching between providers without changing core logic.</p>"},{"location":"architecture/technical-reference/#supported-providers","title":"Supported Providers","text":"Provider Category Platform Cost Use Case OpenAI Cloud API All $$ Production, high quality Azure OpenAI Cloud API All $$ Enterprise, compliance, Managed Identity support Anthropic Cloud API All $$$ Claude models, long context Groq Cloud API All Free tier Fast inference, development OpenAI-Compatible Custom/Local/Cloud All Varies Ollama, vLLM, Together.AI, custom APIs"},{"location":"architecture/technical-reference/#provider-selection-guide","title":"Provider Selection Guide","text":"<p>Choose OpenAI if: Production quality, mature ecosystem, GPT-4 Choose Azure if: Enterprise compliance, private deployments, keyless authentication Choose Anthropic if: Claude models, 100K+ context Choose Groq if: Fast inference, free tier, development Choose OpenAI-Compatible if: Custom endpoints, Ollama, vLLM, Together.AI, self-hosted</p>"},{"location":"architecture/technical-reference/#52-openai-compatible-provider-custom-apis","title":"5.2 OpenAI-Compatible Provider (Custom APIs)","text":""},{"location":"architecture/technical-reference/#purpose","title":"Purpose","text":"<p>Enable integration with any LLM API that implements the OpenAI chat completions format.</p>"},{"location":"architecture/technical-reference/#class-openaicompatibleclient","title":"Class: <code>OpenAICompatibleClient</code>","text":"<p>Inheritance: <code>LLMClient</code> (Adapter pattern)</p> <p>Platform: All (platform-agnostic)</p> <p>Responsibility: Connect to custom OpenAI-compatible API endpoints</p> <p>Supports: - Ollama (local LLM server) - vLLM (self-hosted inference) - Together.AI (cloud API) - Anyscale (cloud API) - LocalAI (self-hosted) - Any custom OpenAI-compatible API</p>"},{"location":"architecture/technical-reference/#configuration","title":"Configuration","text":"<p>Required Fields: - <code>provider: openai_compatible</code> - <code>base_url</code>: Custom API endpoint URL - <code>model</code>: Model identifier</p> <p>Optional Fields: - <code>provider_name</code>: Custom name for logging/metrics - <code>api_key</code>: Authentication (or use env var, or \"dummy\" for local) - <code>input_cost_per_1k_tokens</code>: Custom pricing - <code>output_cost_per_1k_tokens</code>: Custom pricing</p>"},{"location":"architecture/technical-reference/#architecture","title":"Architecture","text":"<pre><code>class OpenAICompatibleClient(LLMClient):\n    def __init__(self, spec: LLMSpec):\n        # Validate base_url is provided\n        # Initialize OpenAI client with custom base_url\n        # Use provider_name for metrics\n\n    def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n        # Call custom API using OpenAI format\n        # Return standardized LLMResponse\n</code></pre> <p>Design Decision: Reuse OpenAI Client</p> <p>Instead of reimplementing HTTP calls, leverage llama-index's OpenAI client with custom <code>api_base</code>:</p> <pre><code>self.client = OpenAI(\n    model=spec.model,\n    api_key=api_key or \"dummy\",\n    api_base=spec.base_url,  # Custom URL\n)\n</code></pre> <p>Rationale: - DRY: Reuse existing, well-tested code - Reliability: llama-index handles edge cases - Compatibility: Ensures OpenAI format compliance - Maintainability: Updates to OpenAI client benefit us</p>"},{"location":"architecture/technical-reference/#example-configurations","title":"Example Configurations","text":""},{"location":"architecture/technical-reference/#local-ollama-free","title":"Local Ollama (Free)","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"Ollama-Local\"\n  model: llama3.1:70b\n  base_url: http://localhost:11434/v1\n  # No API key needed\n  input_cost_per_1k_tokens: 0.0\n  output_cost_per_1k_tokens: 0.0\n</code></pre>"},{"location":"architecture/technical-reference/#togetherai-cloud","title":"Together.AI (Cloud)","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"Together.AI\"\n  model: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n  base_url: https://api.together.xyz/v1\n  api_key: \\${TOGETHER_API_KEY}\n  input_cost_per_1k_tokens: 0.0006\n  output_cost_per_1k_tokens: 0.0006\n</code></pre>"},{"location":"architecture/technical-reference/#self-hosted-vllm","title":"Self-Hosted vLLM","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"vLLM-Custom\"\n  model: meta-llama/Llama-3.1-70B-Instruct\n  base_url: http://your-vllm-server:8000/v1\n  input_cost_per_1k_tokens: 0.0  # Self-hosted = free\n  output_cost_per_1k_tokens: 0.0\n</code></pre>"},{"location":"architecture/technical-reference/#validation","title":"Validation","text":"<p>Pydantic Model Validator: <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre></p> <p>Design: Fail fast with clear error message</p>"},{"location":"architecture/technical-reference/#token-estimation","title":"Token Estimation","text":"<p>Uses tiktoken with cl100k_base encoding (approximation): <pre><code>self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\ntokens = len(self.tokenizer.encode(text))\n</code></pre></p> <p>Tradeoff: Not model-specific, but consistent and fast</p>"},{"location":"architecture/technical-reference/#performance-characteristics","title":"Performance Characteristics","text":"<p>Depends on backend: - Ollama local: ~10-20 tokens/sec (70B model, single GPU) - vLLM multi-GPU: ~50-80 tokens/sec (70B model, tensor parallel) - Together.AI cloud: ~30-50 tokens/sec (network latency)</p>"},{"location":"architecture/technical-reference/#authentication-handling","title":"Authentication Handling","text":"<p>Priority order: 1. <code>spec.api_key</code> (explicit in config) 2. <code>OPENAI_COMPATIBLE_API_KEY</code> env var 3. <code>\"dummy\"</code> fallback (for local APIs without auth)</p> <p>Design: Flexible authentication for different scenarios</p>"},{"location":"architecture/technical-reference/#provider-naming","title":"Provider Naming","text":"<p>The <code>provider_name</code> field appears in metrics/logging: <pre><code>model=f\"{self.provider_name}/{self.model}\"\n# Example: \"Together.AI/meta-llama/Llama-3.1-70B\"\n</code></pre></p> <p>Benefit: Clear identification in logs and cost tracking</p>"},{"location":"architecture/technical-reference/#dependencies","title":"Dependencies","text":"<pre><code>from llama_index.llms.openai import OpenAI  # Reuse OpenAI client\nimport tiktoken  # Token estimation\n</code></pre>"},{"location":"architecture/technical-reference/#used-by","title":"Used By","text":"<ul> <li><code>create_llm_client()</code> factory</li> <li>Any pipeline with <code>provider: openai_compatible</code></li> <li>Examples: Ollama, Together.AI, vLLM configs</li> </ul>"},{"location":"architecture/technical-reference/#testing-strategy","title":"Testing Strategy","text":"<p>Unit Tests (14 tests): - Validation (base_url requirement) - Provider naming - Dummy API keys for local APIs - Cost calculation with custom pricing - Factory integration - Backward compatibility</p>"},{"location":"architecture/technical-reference/#known-limitations","title":"Known Limitations","text":"<ul> <li>Assumes OpenAI-compatible format (doesn't support exotic APIs)</li> <li>Token counting is approximate (cl100k_base encoding)</li> <li>Can't use provider-specific features (only OpenAI format)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements","title":"Future Improvements","text":"<ul> <li>[ ] Support custom headers (for exotic auth)</li> <li>[ ] Add provider-specific tokenizers</li> <li>[ ] Support streaming responses</li> <li>[ ] Add connection pooling for performance</li> </ul>"},{"location":"architecture/technical-reference/#part-3-layer-0-core-utilities","title":"Part 3: Layer 0 - Core Utilities","text":""},{"location":"architecture/technical-reference/#31-overview","title":"3.1 Overview","text":"<p>Layer 0 provides cross-cutting concerns that are used throughout the system: - Retry logic with exponential backoff - Rate limiting (token bucket algorithm) - Cost tracking and accumulation - Budget enforcement - Structured logging - Metrics export (Prometheus) - Input text preprocessing</p> <p>Design Principle: These utilities have no dependencies on higher layers, only on external libraries.</p>"},{"location":"architecture/technical-reference/#32-utilsretry_handlerpy","title":"3.2 <code>utils/retry_handler.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_1","title":"Purpose","text":"<p>Implements robust retry logic with exponential backoff for transient failures.</p>"},{"location":"architecture/technical-reference/#classes","title":"Classes","text":""},{"location":"architecture/technical-reference/#retryableerror-exception","title":"<code>RetryableError</code> (Exception)","text":"<p>Inheritance: <code>Exception</code></p> <p>Purpose: Base class for errors that should be retried</p> <p>Design Decision: Create custom exception hierarchy to distinguish retryable vs. non-retryable errors</p> <pre><code>class RetryableError(Exception):\n    \"\"\"Base class for errors that should be retried.\"\"\"\n    pass\n</code></pre>"},{"location":"architecture/technical-reference/#ratelimiterror-exception","title":"<code>RateLimitError</code> (Exception)","text":"<p>Inheritance: <code>RetryableError</code></p> <p>Purpose: Specific error for rate limit scenarios</p> <p>Usage: Raised by LLM clients when rate limits are hit</p>"},{"location":"architecture/technical-reference/#networkerror-exception","title":"<code>NetworkError</code> (Exception)","text":"<p>Inheritance: <code>RetryableError</code></p> <p>Purpose: Specific error for network-related failures</p> <p>Usage: Raised when network calls fail transiently</p>"},{"location":"architecture/technical-reference/#retryhandler-class","title":"<code>RetryHandler</code> (Class)","text":"<p>Responsibility: Handle retry logic with configurable exponential backoff</p> <p>Single Responsibility: ONLY handles retry logic, nothing else</p> <p>Attributes: <pre><code>max_attempts: int           # Maximum retry attempts (default: 3)\ninitial_delay: float        # Initial delay in seconds (default: 1.0)\nmax_delay: float            # Maximum delay cap (default: 60.0)\nexponential_base: int       # Base for exponential calculation (default: 2)\nretryable_exceptions: tuple # Which exceptions to retry\n</code></pre></p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__max_attempts-initial_delay-max_delay-exponential_base-retryable_exceptions","title":"<code>__init__(max_attempts, initial_delay, max_delay, exponential_base, retryable_exceptions)</code>","text":"<p>Purpose: Initialize retry configuration</p> <p>Parameters: - <code>max_attempts: int = 3</code> - How many times to retry - <code>initial_delay: float = 1.0</code> - Starting delay - <code>max_delay: float = 60.0</code> - Maximum delay cap - <code>exponential_base: int = 2</code> - Exponent base (delay = initial * base^attempt) - <code>retryable_exceptions: Optional[tuple] = None</code> - Which exceptions trigger retry</p> <p>Design Decision: Make all parameters configurable for flexibility across different failure scenarios</p> <p>Default Behavior: If <code>retryable_exceptions</code> is None, defaults to <code>(RetryableError, RateLimitError, NetworkError)</code></p>"},{"location":"architecture/technical-reference/#executefunc-callable-t-t","title":"<code>execute(func: Callable[[], T]) -&gt; T</code>","text":"<p>Purpose: Execute a function with retry logic</p> <p>Algorithm: 1. Create <code>Retrying</code> instance from tenacity library 2. Configure stop condition: <code>stop_after_attempt(max_attempts)</code> 3. Configure wait strategy: exponential backoff with <code>wait_exponential()</code> 4. Configure retry condition: <code>retry_if_exception_type(retryable_exceptions)</code> 5. Execute function through retryer 6. If all retries fail, reraise last exception</p> <p>Parameters: - <code>func: Callable[[], T]</code> - Zero-argument function to execute</p> <p>Returns: <code>T</code> - Result from successful function execution</p> <p>Raises: Last exception if all retries exhausted</p> <p>Example: <pre><code>retry_handler = RetryHandler(max_attempts=3, initial_delay=1.0)\n\ndef risky_operation():\n    # Might fail with RateLimitError\n    return call_llm_api()\n\nresult = retry_handler.execute(risky_operation)\n</code></pre></p> <p>Design Decision: Use tenacity library instead of custom implementation - Why: tenacity is battle-tested, handles edge cases, provides flexible configuration - Alternative: Custom retry loop (simpler but less robust) - Trade-off: Additional dependency vs. reliability</p>"},{"location":"architecture/technical-reference/#calculate_delayattempt-int-float","title":"<code>calculate_delay(attempt: int) -&gt; float</code>","text":"<p>Purpose: Calculate delay for given attempt number (for informational/testing purposes)</p> <p>Algorithm: <pre><code>delay = initial_delay * (exponential_base ** (attempt - 1))\nreturn min(delay, max_delay)\n</code></pre></p> <p>Parameters: - <code>attempt: int</code> - Attempt number (1-based)</p> <p>Returns: <code>float</code> - Delay in seconds</p> <p>Example: <pre><code>handler = RetryHandler(initial_delay=1.0, exponential_base=2, max_delay=60.0)\nhandler.calculate_delay(1)  # 1.0 second\nhandler.calculate_delay(2)  # 2.0 seconds\nhandler.calculate_delay(3)  # 4.0 seconds\nhandler.calculate_delay(4)  # 8.0 seconds\nhandler.calculate_delay(10) # 60.0 seconds (capped at max_delay)\n</code></pre></p>"},{"location":"architecture/technical-reference/#thread-safety","title":"Thread Safety","text":"<ul> <li>Thread-safe: Yes (stateless operation, no shared mutable state)</li> <li>Each <code>execute()</code> call is independent</li> <li>tenacity handles thread safety internally</li> </ul>"},{"location":"architecture/technical-reference/#dependencies_1","title":"Dependencies","text":"<pre><code>import time                  # For delays\nfrom typing import Callable  # Type hints\nfrom tenacity import (       # Retry library\n    Retrying,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_1","title":"Used By","text":"<ul> <li><code>LLMInvocationStage</code> - Retries LLM API calls</li> <li>Any component needing retry logic</li> </ul>"},{"location":"architecture/technical-reference/#design-patterns","title":"Design Patterns","text":"<ol> <li>Strategy Pattern: Configurable retry strategy via parameters</li> <li>Dependency Inversion: Uses abstract <code>Callable</code> type, not specific implementations</li> </ol>"},{"location":"architecture/technical-reference/#time-complexity","title":"Time Complexity","text":"<ul> <li>Exponential backoff: O(2^n) time in worst case (where n = max_attempts)</li> <li>Actual delays: 1s, 2s, 4s, 8s, 16s, ...</li> </ul>"},{"location":"architecture/technical-reference/#testing-considerations","title":"Testing Considerations","text":"<ul> <li>Test with mocked functions that fail predictably</li> <li>Verify exponential backoff timing</li> <li>Test max_attempts enforcement</li> <li>Test exception filtering (retryable vs. non-retryable)</li> </ul>"},{"location":"architecture/technical-reference/#known-limitations_1","title":"Known Limitations","text":"<ul> <li>No jitter (could add randomness to prevent thundering herd)</li> <li>Fixed exponential formula (could support other backoff strategies)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_1","title":"Future Improvements","text":"<ul> <li>[ ] Add jitter support for distributed systems</li> <li>[ ] Support custom backoff strategies (linear, polynomial)</li> <li>[ ] Add metrics for retry count tracking</li> </ul>"},{"location":"architecture/technical-reference/#33-utilsrate_limiterpy","title":"3.3 <code>utils/rate_limiter.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_2","title":"Purpose","text":"<p>Implements token bucket algorithm for rate limiting API calls.</p>"},{"location":"architecture/technical-reference/#classes_1","title":"Classes","text":""},{"location":"architecture/technical-reference/#ratelimiter-class","title":"<code>RateLimiter</code> (Class)","text":"<p>Responsibility: Control request rate using token bucket algorithm</p> <p>Algorithm: Token Bucket - Bucket has maximum capacity (burst_size) - Tokens refill at constant rate (requests_per_minute / 60) - Each request consumes tokens - If no tokens available, wait until refilled</p> <p>Thread Safety: Thread-safe via <code>threading.Lock</code></p> <p>Attributes: <pre><code>rpm: int                    # Requests per minute limit\ncapacity: int               # Maximum burst size\ntokens: float               # Current available tokens\nlast_update: float          # Last refill timestamp\nlock: threading.Lock        # Thread safety\nrefill_rate: float          # Tokens per second\n</code></pre></p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__requests_per_minute-int-burst_size-optionalint-none","title":"<code>__init__(requests_per_minute: int, burst_size: Optional[int] = None)</code>","text":"<p>Purpose: Initialize rate limiter with capacity and refill rate</p> <p>Parameters: - <code>requests_per_minute: int</code> - Maximum requests allowed per minute - <code>burst_size: Optional[int] = None</code> - Maximum burst size (defaults to <code>requests_per_minute</code>)</p> <p>Design Decision: Separate burst_size from requests_per_minute - Why: Allow bursts up to capacity, then throttle to sustained rate - Example: <code>rpm=60, burst=120</code> allows 120 immediate requests, then throttles to 1/second</p> <p>Initialization: <pre><code>self.rpm = requests_per_minute\nself.capacity = burst_size or requests_per_minute\nself.tokens = float(self.capacity)  # Start with full capacity\nself.last_update = time.time()\nself.lock = threading.Lock()\nself.refill_rate = requests_per_minute / 60.0  # Tokens per second\n</code></pre></p>"},{"location":"architecture/technical-reference/#acquiretokens-int-1-timeout-optionalfloat-none-bool","title":"<code>acquire(tokens: int = 1, timeout: Optional[float] = None) -&gt; bool</code>","text":"<p>Purpose: Acquire tokens for making requests (blocks until available)</p> <p>Algorithm: 1. Check if requested tokens &lt;= capacity (raise ValueError if not) 2. Loop:    a. Acquire lock    b. Refill tokens based on elapsed time    c. If sufficient tokens available:       - Deduct tokens       - Return True    d. Release lock    e. Check timeout    f. Sleep briefly (0.1s) before retry</p> <p>Parameters: - <code>tokens: int = 1</code> - Number of tokens to acquire - <code>timeout: Optional[float] = None</code> - Maximum wait time (None = wait forever)</p> <p>Returns: <code>bool</code> - <code>True</code> if tokens acquired - <code>False</code> if timeout reached</p> <p>Raises: <code>ValueError</code> if <code>tokens &gt; capacity</code></p> <p>Thread Safety: Lock protects token bucket state during check-and-decrement</p> <p>Example: <pre><code>limiter = RateLimiter(requests_per_minute=60, burst_size=120)\n\n# Acquire 1 token (default)\nif limiter.acquire():\n    make_api_call()\n\n# Acquire with timeout\nif limiter.acquire(tokens=1, timeout=5.0):\n    make_api_call()\nelse:\n    print(\"Timeout waiting for rate limit\")\n\n# Acquire multiple tokens\nif limiter.acquire(tokens=5):\n    make_batch_api_call(batch_size=5)\n</code></pre></p>"},{"location":"architecture/technical-reference/#_refill-none","title":"<code>_refill() -&gt; None</code>","text":"<p>Purpose: Refill tokens based on elapsed time (internal method)</p> <p>Algorithm: <pre><code>now = time.time()\nelapsed = now - self.last_update\ntokens_to_add = elapsed * self.refill_rate\nself.tokens = min(self.capacity, self.tokens + tokens_to_add)\nself.last_update = now\n</code></pre></p> <p>Design Decision: Continuous refill vs. discrete intervals - Chosen: Continuous refill (calculate tokens based on elapsed time) - Why: More accurate, smoother rate limiting - Alternative: Refill in discrete chunks (simpler but less accurate)</p> <p>Thread Safety: Called only while holding lock</p>"},{"location":"architecture/technical-reference/#available_tokens-property","title":"<code>available_tokens</code> (Property)","text":"<p>Purpose: Get current available tokens (thread-safe)</p> <p>Returns: <code>float</code> - Current token count</p> <p>Thread Safety: Acquires lock, refills, returns tokens</p> <p>Example: <pre><code>limiter = RateLimiter(requests_per_minute=60)\nprint(f\"Available: {limiter.available_tokens}\")  # e.g., 45.3\n</code></pre></p>"},{"location":"architecture/technical-reference/#reset-none","title":"<code>reset() -&gt; None</code>","text":"<p>Purpose: Reset rate limiter to full capacity</p> <p>Thread Safety: Acquires lock, resets tokens and timestamp</p> <p>Use Case: Manual reset after known idle period</p>"},{"location":"architecture/technical-reference/#token-bucket-algorithm","title":"Token Bucket Algorithm","text":"<p>Visual: <pre><code>Bucket (capacity = 100)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593    \u2502  Current: 80 tokens\n\u2502             \u2502  Refill rate: 60/minute = 1/second\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAfter 5 seconds:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593 \u2502  Current: 85 tokens (80 + 5*1)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAfter acquire(10):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593    \u2502  Current: 75 tokens\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Properties: - Allows bursts up to capacity - Refills continuously at constant rate - Smooth rate limiting (no hard boundaries)</p>"},{"location":"architecture/technical-reference/#dependencies_2","title":"Dependencies","text":"<pre><code>import threading          # Lock for thread safety\nimport time              # Timestamp tracking\nfrom typing import Optional\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_2","title":"Used By","text":"<ul> <li><code>LLMInvocationStage</code> - Rate limit API calls</li> <li>Any component needing request throttling</li> </ul>"},{"location":"architecture/technical-reference/#thread-safety-details","title":"Thread Safety Details","text":"<pre><code>with self.lock:  # Critical section\n    self._refill()\n    if self.tokens &gt;= tokens:\n        self.tokens -= tokens\n        return True\n</code></pre> <p>Race Condition Prevention: - Without lock: Two threads could both see sufficient tokens, both decrement, exceed capacity - With lock: Only one thread can check-and-decrement at a time</p>"},{"location":"architecture/technical-reference/#performance","title":"Performance","text":"<ul> <li>Time Complexity: O(1) per acquire</li> <li>Space Complexity: O(1) - constant space</li> <li>Lock Contention: Low (critical section is very short)</li> </ul>"},{"location":"architecture/technical-reference/#configuration-examples","title":"Configuration Examples","text":"<pre><code># Conservative (prevent rate limits)\nlimiter = RateLimiter(requests_per_minute=30, burst_size=30)\n\n# Aggressive (maximize throughput)\nlimiter = RateLimiter(requests_per_minute=100, burst_size=200)\n\n# Bursty workload (allow initial burst, then throttle)\nlimiter = RateLimiter(requests_per_minute=60, burst_size=180)\n</code></pre>"},{"location":"architecture/technical-reference/#testing","title":"Testing","text":"<pre><code>def test_rate_limiter():\n    limiter = RateLimiter(requests_per_minute=60)\n\n    # Should acquire immediately\n    assert limiter.acquire(timeout=0.1)\n\n    # Deplete tokens\n    for _ in range(59):\n        limiter.acquire()\n\n    # Should timeout (no tokens left)\n    assert not limiter.acquire(timeout=0.1)\n\n    # Wait for refill\n    time.sleep(2.0)\n    assert limiter.acquire()  # Should have ~2 tokens refilled\n</code></pre>"},{"location":"architecture/technical-reference/#known-limitations_2","title":"Known Limitations","text":"<ul> <li>No distributed rate limiting (single-process only)</li> <li>Fixed refill rate (can't adjust dynamically)</li> <li>Polling-based waiting (could use condition variables)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_2","title":"Future Improvements","text":"<ul> <li>[ ] Add distributed rate limiting (Redis-based)</li> <li>[ ] Support dynamic rate adjustment</li> <li>[ ] Use condition variables instead of polling</li> <li>[ ] Add metrics for rate limit hits</li> </ul>"},{"location":"architecture/technical-reference/#34-utilscost_trackerpy","title":"3.4 <code>utils/cost_tracker.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_3","title":"Purpose","text":"<p>Thread-safe cost tracking for LLM API usage with detailed breakdowns.</p>"},{"location":"architecture/technical-reference/#classes_2","title":"Classes","text":""},{"location":"architecture/technical-reference/#costentry-dataclass","title":"<code>CostEntry</code> (Dataclass)","text":"<p>Purpose: Single cost tracking entry</p> <p>Attributes: <pre><code>tokens_in: int      # Input tokens consumed\ntokens_out: int     # Output tokens generated\ncost: Decimal       # Cost for this entry\nmodel: str          # Model identifier\ntimestamp: float    # When request occurred\n</code></pre></p> <p>Design Decision: Use dataclass for simplicity and immutability</p>"},{"location":"architecture/technical-reference/#costtracker-class","title":"<code>CostTracker</code> (Class)","text":"<p>Responsibility: Accumulate and track costs across LLM calls</p> <p>Single Responsibility: ONLY handles cost accounting, not enforcement (that's BudgetController's job)</p> <p>Thread Safety: Thread-safe via <code>threading.Lock</code></p> <p>Attributes: <pre><code>input_cost_per_1k: Decimal       # Input token price per 1K tokens\noutput_cost_per_1k: Decimal      # Output token price per 1K tokens\n_total_input_tokens: int         # Cumulative input tokens\n_total_output_tokens: int        # Cumulative output tokens\n_total_cost: Decimal             # Cumulative cost (Decimal for precision)\n_entries: list[CostEntry]        # Detailed entry log\n_stage_costs: Dict[str, Decimal] # Cost breakdown by stage\n_lock: threading.Lock            # Thread safety\n</code></pre></p> <p>Design Decision: Use <code>Decimal</code> for cost (not <code>float</code>) - Why: Avoid floating-point precision errors (<code>0.1 + 0.2 != 0.3</code>) - Critical: Financial calculations must be exact - Trade-off: Slightly slower than float, but necessary for accuracy</p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__input_cost_per_1k-output_cost_per_1k","title":"<code>__init__(input_cost_per_1k, output_cost_per_1k)</code>","text":"<p>Purpose: Initialize cost tracker with pricing</p> <p>Parameters: - <code>input_cost_per_1k: Optional[Decimal] = None</code> - Price per 1K input tokens - <code>output_cost_per_1k: Optional[Decimal] = None</code> - Price per 1K output tokens</p> <p>Default: Both default to <code>Decimal(\"0.0\")</code> if not provided</p> <p>Initialization: <pre><code>self.input_cost_per_1k = input_cost_per_1k or Decimal(\"0.0\")\nself.output_cost_per_1k = output_cost_per_1k or Decimal(\"0.0\")\nself._total_input_tokens = 0\nself._total_output_tokens = 0\nself._total_cost = Decimal(\"0.0\")\nself._entries = []\nself._stage_costs = {}\nself._lock = threading.Lock()\n</code></pre></p>"},{"location":"architecture/technical-reference/#addtokens_in-tokens_out-model-timestamp-stage-decimal","title":"<code>add(tokens_in, tokens_out, model, timestamp, stage) -&gt; Decimal</code>","text":"<p>Purpose: Add cost entry and return cost for this operation</p> <p>Algorithm: 1. Calculate cost: <code>(tokens_in/1000) * input_price + (tokens_out/1000) * output_price</code> 2. Acquire lock 3. Create <code>CostEntry</code> and append to <code>_entries</code> 4. Accumulate tokens and cost 5. Update stage-specific costs 6. Release lock 7. Return calculated cost</p> <p>Parameters: - <code>tokens_in: int</code> - Input tokens used - <code>tokens_out: int</code> - Output tokens generated - <code>model: str</code> - Model identifier - <code>timestamp: float</code> - Request timestamp - <code>stage: Optional[str] = None</code> - Stage name for breakdown</p> <p>Returns: <code>Decimal</code> - Cost for this entry</p> <p>Thread Safety: Entire operation protected by lock to ensure atomic update</p> <p>Example: <pre><code>tracker = CostTracker(\n    input_cost_per_1k=Decimal(\"0.00005\"),  # $0.05 per 1M input tokens\n    output_cost_per_1k=Decimal(\"0.00008\"),  # $0.08 per 1M output tokens\n)\n\ncost = tracker.add(\n    tokens_in=1000,\n    tokens_out=500,\n    model=\"gpt-4o-mini\",\n    timestamp=time.time(),\n    stage=\"llm_invocation\"\n)\n# cost = (1000/1000 * 0.00005) + (500/1000 * 0.00008)\n#      = 0.00005 + 0.00004\n#      = 0.00009 ($0.00009)\n</code></pre></p>"},{"location":"architecture/technical-reference/#calculate_costtokens_in-tokens_out-decimal","title":"<code>calculate_cost(tokens_in, tokens_out) -&gt; Decimal</code>","text":"<p>Purpose: Calculate cost for given token counts (without recording)</p> <p>Algorithm: <pre><code>input_cost = (Decimal(tokens_in) / 1000) * self.input_cost_per_1k\noutput_cost = (Decimal(tokens_out) / 1000) * self.output_cost_per_1k\nreturn input_cost + output_cost\n</code></pre></p> <p>Use Case: Estimate cost before making request</p> <p>Example: <pre><code>estimated = tracker.calculate_cost(tokens_in=500, tokens_out=200)\nif estimated &gt; Decimal(\"0.01\"):  # More than 1 cent\n    print(\"Expensive request!\")\n</code></pre></p>"},{"location":"architecture/technical-reference/#total_cost-property","title":"<code>total_cost</code> (Property)","text":"<p>Purpose: Get total accumulated cost (thread-safe)</p> <p>Returns: <code>Decimal</code> - Total cost</p> <p>Thread Safety: Acquires lock, reads <code>_total_cost</code>, releases lock</p>"},{"location":"architecture/technical-reference/#total_tokens-input_tokens-output_tokens-properties","title":"<code>total_tokens</code>, <code>input_tokens</code>, <code>output_tokens</code> (Properties)","text":"<p>Purpose: Get token counts (thread-safe)</p> <p>Returns: <code>int</code> - Token count</p>"},{"location":"architecture/technical-reference/#get_estimaterows-costestimate","title":"<code>get_estimate(rows) -&gt; CostEstimate</code>","text":"<p>Purpose: Create cost estimate object from current tracking</p> <p>Returns: <code>CostEstimate</code> model with: - <code>total_cost</code> - <code>total_tokens</code> - <code>input_tokens</code> - <code>output_tokens</code> - <code>rows</code> - Number of rows processed - <code>breakdown_by_stage</code> - Dict of stage costs - <code>confidence=\"actual\"</code> - These are actual costs, not estimates</p>"},{"location":"architecture/technical-reference/#reset-none_1","title":"<code>reset() -&gt; None</code>","text":"<p>Purpose: Clear all tracking data (thread-safe)</p> <p>Use Case: Reset between test runs or pipeline executions</p>"},{"location":"architecture/technical-reference/#get_stage_costs-dictstr-decimal","title":"<code>get_stage_costs() -&gt; Dict[str, Decimal]</code>","text":"<p>Purpose: Get cost breakdown by stage</p> <p>Returns: Dictionary mapping stage names to costs</p> <p>Example: <pre><code>stage_costs = tracker.get_stage_costs()\n# {\"llm_invocation\": Decimal(\"0.15\"), \"retry\": Decimal(\"0.02\")}\n</code></pre></p>"},{"location":"architecture/technical-reference/#dependencies_3","title":"Dependencies","text":"<pre><code>import threading                      # Thread safety\nfrom dataclasses import dataclass     # CostEntry\nfrom decimal import Decimal           # Precise financial calculations\nfrom typing import Dict, Optional\nfrom src.core.models import CostEstimate  # Result model\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_3","title":"Used By","text":"<ul> <li><code>LLMInvocationStage</code> - Track costs during execution</li> <li><code>PipelineExecutor</code> - Get total costs for result</li> <li><code>BudgetController</code> - Check against budget limits</li> </ul>"},{"location":"architecture/technical-reference/#thread-safety-example","title":"Thread Safety Example","text":"<pre><code># Two threads executing concurrently:\n# Thread 1: tracker.add(tokens_in=1000, tokens_out=500, ...)\n# Thread 2: tracker.add(tokens_in=800, tokens_out=300, ...)\n\n# Without lock: Race condition\n# 1. T1 reads _total_cost: 0.00\n# 2. T2 reads _total_cost: 0.00\n# 3. T1 writes _total_cost: 0.10\n# 4. T2 writes _total_cost: 0.08  # LOST UPDATE! T1's cost is lost\n# Final: 0.08 (WRONG, should be 0.18)\n\n# With lock: Correct\n# 1. T1 acquires lock\n# 2. T1 reads _total_cost: 0.00, calculates 0.10, writes 0.10\n# 3. T1 releases lock\n# 4. T2 acquires lock\n# 5. T2 reads _total_cost: 0.10, calculates 0.08, writes 0.18\n# 6. T2 releases lock\n# Final: 0.18 (CORRECT)\n</code></pre>"},{"location":"architecture/technical-reference/#decimal-precision-example","title":"Decimal Precision Example","text":"<pre><code># Why Decimal is necessary:\nfrom decimal import Decimal\n\n# Float (WRONG):\nfloat_cost = 0.1 + 0.2  # 0.30000000000000004 (WRONG!)\n\n# Decimal (CORRECT):\ndecimal_cost = Decimal(\"0.1\") + Decimal(\"0.2\")  # 0.3 (CORRECT!)\n\n# Real-world impact:\n# If processing 1,000,000 rows with $0.0001 per row:\n# Float: $100.00000000014 (accumulated error)\n# Decimal: $100.00 (exact)\n</code></pre>"},{"location":"architecture/technical-reference/#testing_1","title":"Testing","text":"<pre><code>def test_cost_tracker():\n    tracker = CostTracker(\n        input_cost_per_1k=Decimal(\"0.00005\"),\n        output_cost_per_1k=Decimal(\"0.00008\"),\n    )\n\n    # Add cost\n    cost1 = tracker.add(tokens_in=1000, tokens_out=500, model=\"test\", timestamp=0.0)\n    assert cost1 == Decimal(\"0.00009\")\n\n    # Check accumulation\n    assert tracker.total_cost == Decimal(\"0.00009\")\n    assert tracker.total_tokens == 1500\n\n    # Add more\n    cost2 = tracker.add(tokens_in=2000, tokens_out=1000, model=\"test\", timestamp=1.0)\n    assert tracker.total_cost == Decimal(\"0.00027\")  # 0.00009 + 0.00018\n</code></pre>"},{"location":"architecture/technical-reference/#known-limitations_3","title":"Known Limitations","text":"<ul> <li>Stores all entries in memory (could grow large for long runs)</li> <li>No persistence (lost on crash)</li> <li>No cost cap enforcement (that's BudgetController's job)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_3","title":"Future Improvements","text":"<ul> <li>[ ] Add entry pruning for long-running pipelines</li> <li>[ ] Optional persistence to disk</li> <li>[ ] Add cost forecasting based on trends</li> <li>[ ] Support tiered pricing (cost changes with volume)</li> </ul>"},{"location":"architecture/technical-reference/#35-utilsbudget_controllerpy","title":"3.5 <code>utils/budget_controller.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_4","title":"Purpose","text":"<p>Enforce budget limits and provide warnings during execution.</p>"},{"location":"architecture/technical-reference/#classes_3","title":"Classes","text":""},{"location":"architecture/technical-reference/#budgetexceedederror-exception","title":"<code>BudgetExceededError</code> (Exception)","text":"<p>Inheritance: <code>Exception</code></p> <p>Purpose: Raised when budget limit is exceeded</p> <p>Usage: Allows caller to catch and handle budget overruns</p>"},{"location":"architecture/technical-reference/#_1","title":"Technical Reference","text":"<p><code>BudgetController</code> (Class)</p> <p>Responsibility: Monitor costs and enforce budget limits</p> <p>Single Responsibility: ONLY handles budget management (cost tracking is CostTracker's job)</p> <p>Separation of Concerns: - <code>CostTracker</code>: Accumulates costs - <code>BudgetController</code>: Enforces limits</p> <p>Attributes: <pre><code>max_budget: Optional[Decimal]  # Maximum allowed budget (None = no limit)\nwarn_at_75: bool              # Warn at 75% of budget\nwarn_at_90: bool              # Warn at 90% of budget\nfail_on_exceed: bool          # Raise error if budget exceeded\n_warned_75: bool              # Track if 75% warning already shown\n_warned_90: bool              # Track if 90% warning already shown\n</code></pre></p> <p>Design Decision: Separate warning flags from budget - Why: Show each warning only once - Alternative: Check every time (would spam logs)</p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__max_budget-warn_at_75-warn_at_90-fail_on_exceed","title":"<code>__init__(max_budget, warn_at_75, warn_at_90, fail_on_exceed)</code>","text":"<p>Purpose: Initialize budget controller with limits</p> <p>Parameters: - <code>max_budget: Optional[Decimal] = None</code> - Maximum budget in USD - <code>warn_at_75: bool = True</code> - Warn at 75% usage - <code>warn_at_90: bool = True</code> - Warn at 90% usage - <code>fail_on_exceed: bool = True</code> - Raise error when exceeded</p> <p>Default Behavior: Warn twice, then fail</p> <p>Flexible Configuration: <pre><code># Strict (default): Fail on exceed\ncontroller = BudgetController(max_budget=Decimal(\"10.0\"))\n\n# Warn only: Don't fail\ncontroller = BudgetController(max_budget=Decimal(\"10.0\"), fail_on_exceed=False)\n\n# No warnings: Just hard limit\ncontroller = BudgetController(\n    max_budget=Decimal(\"10.0\"),\n    warn_at_75=False,\n    warn_at_90=False,\n)\n</code></pre></p>"},{"location":"architecture/technical-reference/#check_budgetcurrent_cost-decimal-none","title":"<code>check_budget(current_cost: Decimal) -&gt; None</code>","text":"<p>Purpose: Check if cost is within budget, warn or fail as configured</p> <p>Algorithm: 1. If <code>max_budget</code> is None, return (no limit) 2. Calculate usage ratio: <code>current_cost / max_budget</code> 3. If &gt;= 0.75 and not warned: Log warning, set <code>_warned_75 = True</code> 4. If &gt;= 0.90 and not warned: Log warning, set <code>_warned_90 = True</code> 5. If &gt; <code>max_budget</code>:    - Log error    - If <code>fail_on_exceed</code>: raise <code>BudgetExceededError</code></p> <p>Parameters: - <code>current_cost: Decimal</code> - Current accumulated cost</p> <p>Raises: <code>BudgetExceededError</code> if cost exceeds budget and <code>fail_on_exceed=True</code></p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\n\n# Check after each operation\ncontroller.check_budget(Decimal(\"5.0\"))   # OK\ncontroller.check_budget(Decimal(\"7.5\"))   # Logs: \"75% used\"\ncontroller.check_budget(Decimal(\"9.0\"))   # Logs: \"90% used\"\ncontroller.check_budget(Decimal(\"10.5\"))  # Raises BudgetExceededError\n</code></pre></p> <p>Design Decision: Warnings at 75% and 90% - Why: Give user time to react before hitting limit - 75%: Early warning - 90%: Final warning before failure - Alternative: More granular warnings (every 10%) would spam logs</p>"},{"location":"architecture/technical-reference/#get_remainingcurrent_cost-decimal-optionaldecimal","title":"<code>get_remaining(current_cost: Decimal) -&gt; Optional[Decimal]</code>","text":"<p>Purpose: Calculate remaining budget</p> <p>Returns: - <code>Decimal</code> - Remaining budget - <code>None</code> if no budget limit set</p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\nremaining = controller.get_remaining(Decimal(\"7.5\"))\n# remaining = Decimal(\"2.5\")\n</code></pre></p>"},{"location":"architecture/technical-reference/#get_usage_percentagecurrent_cost-decimal-optionalfloat","title":"<code>get_usage_percentage(current_cost: Decimal) -&gt; Optional[float]</code>","text":"<p>Purpose: Calculate budget usage as percentage</p> <p>Returns: - <code>float</code> - Usage percentage (0-100+) - <code>None</code> if no budget limit set</p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\npct = controller.get_usage_percentage(Decimal(\"7.5\"))\n# pct = 75.0\n</code></pre></p>"},{"location":"architecture/technical-reference/#can_affordestimated_cost-decimal-current_cost-decimal-bool","title":"<code>can_afford(estimated_cost: Decimal, current_cost: Decimal) -&gt; bool</code>","text":"<p>Purpose: Check if estimated additional cost fits within budget</p> <p>Algorithm: <pre><code>if self.max_budget is None:\n    return True\nreturn (current_cost + estimated_cost) &lt;= self.max_budget\n</code></pre></p> <p>Use Case: Pre-flight check before expensive operation</p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\ncurrent = Decimal(\"9.0\")\nestimated_next = Decimal(\"0.5\")\n\nif controller.can_afford(estimated_next, current):\n    proceed_with_operation()\nelse:\n    print(\"Would exceed budget, skipping\")\n</code></pre></p>"},{"location":"architecture/technical-reference/#dependencies_4","title":"Dependencies","text":"<pre><code>from decimal import Decimal       # Precise financial calculations\nfrom typing import Optional\nimport structlog                  # Structured logging\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_4","title":"Used By","text":"<ul> <li><code>PipelineExecutor</code> - Check budget during execution</li> <li>Any component needing cost enforcement</li> </ul>"},{"location":"architecture/technical-reference/#integration-with-costtracker","title":"Integration with CostTracker","text":"<pre><code># Typical usage pattern:\ncost_tracker = CostTracker(...)\nbudget_controller = BudgetController(max_budget=Decimal(\"10.0\"))\n\nfor row in data:\n    # Make LLM call\n    response = llm.invoke(prompt)\n\n    # Track cost\n    cost = cost_tracker.add(tokens_in=..., tokens_out=...)\n\n    # Check budget\n    budget_controller.check_budget(cost_tracker.total_cost)\n</code></pre>"},{"location":"architecture/technical-reference/#testing_2","title":"Testing","text":"<pre><code>def test_budget_warnings():\n    controller = BudgetController(max_budget=Decimal(\"10.0\"))\n\n    # Should not warn\n    controller.check_budget(Decimal(\"5.0\"))\n\n    # Should warn at 75%\n    controller.check_budget(Decimal(\"7.5\"))\n\n    # Should warn at 90%\n    controller.check_budget(Decimal(\"9.0\"))\n\n    # Should raise error\n    with pytest.raises(BudgetExceededError):\n        controller.check_budget(Decimal(\"10.1\"))\n</code></pre>"},{"location":"architecture/technical-reference/#error-messages","title":"Error Messages","text":"<pre><code># 75% warning:\n\"Budget warning: 75% used ($7.50 / $10.00)\"\n\n# 90% warning:\n\"Budget warning: 90% used ($9.00 / $10.00)\"\n\n# Budget exceeded:\n\"Budget exceeded: $10.50 &gt; $10.00\"\n</code></pre>"},{"location":"architecture/technical-reference/#known-limitations_4","title":"Known Limitations","text":"<ul> <li>No budget rollover (each execution is independent)</li> <li>No budget sharing across pipelines</li> <li>Fixed warning thresholds (75%, 90%)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_4","title":"Future Improvements","text":"<ul> <li>[ ] Configurable warning thresholds</li> <li>[ ] Support budget pools (shared across pipelines)</li> <li>[ ] Budget rollover support</li> <li>[ ] Budget forecasting (estimate when limit will be hit)</li> </ul> <p>This is Part 1 of the Technical Reference. Continue reading for Layer 0 (remaining utils), Core Models, and all other layers...</p>"},{"location":"architecture/technical-reference/#part-5-layer-1-infrastructure-adapters-llm-providers_1","title":"Part 5: Layer 1 - Infrastructure Adapters (LLM Providers)","text":""},{"location":"architecture/technical-reference/#51-llm-provider-overview_1","title":"5.1 LLM Provider Overview","text":"<p>Ondine supports multiple LLM providers through the Adapter pattern, allowing easy switching between providers without changing core logic.</p>"},{"location":"architecture/technical-reference/#supported-providers_1","title":"Supported Providers","text":"Provider Category Platform Cost Use Case OpenAI Cloud API All $$ Production, high quality Azure OpenAI Cloud API All $$ Enterprise, compliance, Managed Identity support Anthropic Cloud API All $$$ Claude models, long context Groq Cloud API All Free tier Fast inference, development MLX Local macOS (Apple Silicon) Free Privacy, offline, no costs"},{"location":"architecture/technical-reference/#provider-selection-guide_1","title":"Provider Selection Guide","text":"<p>Choose OpenAI if: Production quality, mature ecosystem, GPT-4 Choose Azure if: Enterprise compliance, private deployments Choose Anthropic if: Claude models, 100K+ context Choose Groq if: Fast inference, free tier, development Choose MLX if: Apple Silicon Mac, privacy, free, offline capable</p>"},{"location":"architecture/technical-reference/#52-openai-compatible-provider-custom-apis_1","title":"5.2 OpenAI-Compatible Provider (Custom APIs)","text":""},{"location":"architecture/technical-reference/#purpose_5","title":"Purpose","text":"<p>Enable integration with any LLM API that implements the OpenAI chat completions format.</p>"},{"location":"architecture/technical-reference/#class-openaicompatibleclient_1","title":"Class: <code>OpenAICompatibleClient</code>","text":"<p>Inheritance: <code>LLMClient</code> (Adapter pattern)</p> <p>Platform: All (platform-agnostic)</p> <p>Responsibility: Connect to custom OpenAI-compatible API endpoints</p> <p>Supports: - Ollama (local LLM server) - vLLM (self-hosted inference) - Together.AI (cloud API) - Anyscale (cloud API) - LocalAI (self-hosted) - Any custom OpenAI-compatible API</p>"},{"location":"architecture/technical-reference/#configuration_1","title":"Configuration","text":"<p>Required Fields: - <code>provider: openai_compatible</code> - <code>base_url</code>: Custom API endpoint URL - <code>model</code>: Model identifier</p> <p>Optional Fields: - <code>provider_name</code>: Custom name for logging/metrics - <code>api_key</code>: Authentication (optional for local) - <code>input_cost_per_1k_tokens</code>: Custom pricing - <code>output_cost_per_1k_tokens</code>: Custom pricing</p>"},{"location":"architecture/technical-reference/#design-decision-reuse-openai-client","title":"Design Decision: Reuse OpenAI Client","text":"<p>Instead of reimplementing HTTP, leverage llama-index's OpenAI client:</p> <pre><code>self.client = OpenAI(\n    model=spec.model,\n    api_key=api_key or \"dummy\",\n    api_base=spec.base_url,  # Custom URL\n)\n</code></pre> <p>Rationale: - DRY: Reuse well-tested code - Reliability: Edge cases handled - Compatibility: Ensures OpenAI format - Maintainability: Benefit from upstream updates</p>"},{"location":"architecture/technical-reference/#example-togetherai","title":"Example: Together.AI","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"Together.AI\"\n  model: meta-llama/Llama-3.1-70B\n  base_url: https://api.together.xyz/v1\n  api_key: \\${TOGETHER_API_KEY}\n  input_cost_per_1k_tokens: 0.0006\n</code></pre>"},{"location":"architecture/technical-reference/#validation_1","title":"Validation","text":"<pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self):\n    if self.provider == OPENAI_COMPATIBLE and not self.base_url:\n        raise ValueError(\"base_url required\")\n    return self\n</code></pre>"},{"location":"architecture/technical-reference/#53-azure-openai-provider-enterprise","title":"5.3 Azure OpenAI Provider (Enterprise)","text":""},{"location":"architecture/technical-reference/#purpose_6","title":"Purpose","text":"<p>Enable enterprise-grade Azure OpenAI integration with support for both API key and Managed Identity authentication.</p>"},{"location":"architecture/technical-reference/#class-azureopenaiclient","title":"Class: <code>AzureOpenAIClient</code>","text":"<p>Inheritance: <code>LLMClient</code> (Adapter pattern)</p> <p>Platform: All (cloud-based)</p> <p>Responsibility: Azure OpenAI Service integration with enterprise authentication</p> <p>Key Features: - Managed Identity support (keyless authentication) - API key authentication (backward compatible) - Pre-fetched token support (advanced scenarios) - Multi-region deployment support - Azure RBAC integration</p>"},{"location":"architecture/technical-reference/#authentication-methods","title":"Authentication Methods","text":"<p>Azure OpenAI supports three authentication methods (in priority order):</p>"},{"location":"architecture/technical-reference/#1-managed-identity-recommended-for-production","title":"1. Managed Identity (Recommended for Production)","text":"<p>Purpose: Keyless authentication using Azure AD</p> <p>Requirements: - <code>pip install ondine[azure]</code> (installs <code>azure-identity&gt;=1.15.0</code>) - Managed Identity assigned to Azure resource - RBAC role: \"Cognitive Services OpenAI User\"</p> <p>Configuration: <pre><code>.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    use_managed_identity=True  # \u2190 Keyless!\n)\n</code></pre></p> <p>YAML Configuration: <pre><code>llm:\n  provider: \"azure_openai\"\n  model: \"gpt-4\"\n  azure_endpoint: \"https://your-resource.openai.azure.com/\"\n  azure_deployment: \"gpt-4-deployment\"\n  use_managed_identity: true  # No API key needed!\n</code></pre></p> <p>How It Works: <pre><code>from azure.identity import DefaultAzureCredential\n\n# Ondine automatically:\ncredential = DefaultAzureCredential()\ntoken = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n# Passes token to LlamaIndex\nself.client = AzureOpenAI(\n    azure_ad_token=token.token,  # \u2190 Token, not API key\n    azure_endpoint=spec.azure_endpoint,\n    deployment_name=spec.azure_deployment,\n)\n</code></pre></p> <p>DefaultAzureCredential Resolution Order: 1. Environment variables (AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET) 2. Managed Identity (if running on Azure VM/Container/Function) 3. Azure CLI credentials (if <code>az login</code> executed) 4. Visual Studio Code credentials 5. Azure PowerShell credentials</p> <p>Benefits: - \u2705 No API keys in code or environment - \u2705 Automatic credential rotation (Azure AD handles it) - \u2705 Fine-grained RBAC via Azure roles - \u2705 Audit trail through Azure AD logs - \u2705 Works across dev/staging/production</p> <p>Local Development: <pre><code># Login with Azure CLI\naz login\n\n# Run your script (uses your Azure CLI credentials)\npython your_script.py\n</code></pre></p> <p>Production Deployment: <pre><code># Assign Managed Identity to Azure resource\naz vm identity assign --name my-vm --resource-group my-rg\n\n# Grant RBAC role\naz role assignment create \\\n  --assignee &lt;managed-identity-id&gt; \\\n  --role \"Cognitive Services OpenAI User\" \\\n  --scope &lt;openai-resource-id&gt;\n\n# Run your script (uses Managed Identity automatically)\npython your_script.py\n</code></pre></p>"},{"location":"architecture/technical-reference/#2-pre-fetched-azure-ad-token-advanced","title":"2. Pre-fetched Azure AD Token (Advanced)","text":"<p>Purpose: Manual token management for custom scenarios</p> <p>Configuration: <pre><code>from azure.identity import DefaultAzureCredential\n\n# Fetch token manually\ncredential = DefaultAzureCredential()\ntoken = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n\n.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    azure_ad_token=token.token  # \u2190 Pre-fetched token\n)\n</code></pre></p> <p>Use Cases: - Custom token lifecycle management - Token caching strategies - Integration with existing auth systems</p>"},{"location":"architecture/technical-reference/#3-api-key-backward-compatible","title":"3. API Key (Backward Compatible)","text":"<p>Purpose: Traditional authentication method</p> <p>Configuration: <pre><code>.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",\n    azure_endpoint=\"https://your-resource.openai.azure.com/\",\n    azure_deployment=\"gpt-4-deployment\",\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")  # \u2190 API key\n)\n</code></pre></p> <p>YAML Configuration: <pre><code>llm:\n  provider: \"azure_openai\"\n  model: \"gpt-4\"\n  azure_endpoint: \"https://your-resource.openai.azure.com/\"\n  azure_deployment: \"gpt-4-deployment\"\n  api_key: ${AZURE_OPENAI_API_KEY}  # From environment\n</code></pre></p> <p>Setup: <pre><code>export AZURE_OPENAI_API_KEY=\"your-key-from-azure-portal\"  # pragma: allowlist secret\n</code></pre></p>"},{"location":"architecture/technical-reference/#architecture_1","title":"Architecture","text":"<pre><code>class AzureOpenAIClient(LLMClient):\n    def __init__(self, spec: LLMSpec):\n        # Validate required fields\n        if not spec.azure_endpoint:\n            raise ValueError(\"azure_endpoint required\")\n        if not spec.azure_deployment:\n            raise ValueError(\"azure_deployment required\")\n\n        # Authentication priority:\n        if spec.use_managed_identity:\n            # 1. Managed Identity (preferred)\n            credential = DefaultAzureCredential()\n            token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n            self.client = AzureOpenAI(azure_ad_token=token.token, ...)\n\n        elif spec.azure_ad_token:\n            # 2. Pre-fetched token\n            self.client = AzureOpenAI(azure_ad_token=spec.azure_ad_token, ...)\n\n        else:\n            # 3. API key (backward compatible)\n            api_key = spec.api_key or os.getenv(\"AZURE_OPENAI_API_KEY\")\n            self.client = AzureOpenAI(api_key=api_key, ...)\n</code></pre>"},{"location":"architecture/technical-reference/#configuration-fields","title":"Configuration Fields","text":"<p>Required: - <code>provider: \"azure_openai\"</code> - <code>azure_endpoint</code>: Azure OpenAI resource endpoint - <code>azure_deployment</code>: Deployment name (maps to model)</p> <p>Authentication (choose one): - <code>use_managed_identity: bool</code> - Use Azure Managed Identity (recommended) - <code>azure_ad_token: str</code> - Pre-fetched Azure AD token - <code>api_key: str</code> - API key from Azure Portal</p> <p>Optional: - <code>api_version: str</code> - API version (default: \"2024-02-15-preview\") - <code>temperature: float</code> - Sampling temperature - <code>max_tokens: int</code> - Maximum output tokens</p>"},{"location":"architecture/technical-reference/#multi-region-support","title":"Multi-Region Support","text":"<p>Azure OpenAI is available in multiple regions for data residency and latency optimization:</p> <pre><code># Region-specific endpoints\nregions = {\n    \"eastus\": \"https://eastus-resource.openai.azure.com/\",\n    \"westeurope\": \"https://westeurope-resource.openai.azure.com/\",\n    \"swedencentral\": \"https://swedencentral-resource.openai.azure.com/\",\n    \"japaneast\": \"https://japaneast-resource.openai.azure.com/\",\n}\n\n# Environment-aware configuration\nregion = os.getenv(\"AZURE_REGION\", \"eastus\")\nendpoint = regions[region]\n\n.with_llm(\n    provider=\"azure_openai\",\n    azure_endpoint=endpoint,\n    use_managed_identity=True\n)\n</code></pre>"},{"location":"architecture/technical-reference/#error-handling","title":"Error Handling","text":"<p>Missing Dependency: <pre><code>ImportError: Azure Managed Identity requires azure-identity.\nInstall with: pip install ondine[azure]\n</code></pre></p> <p>Authentication Failure: <pre><code>ValueError: Failed to authenticate with Azure Managed Identity: &lt;error&gt;.\nEnsure the resource has a Managed Identity assigned with\n'Cognitive Services OpenAI User' role.\n</code></pre></p> <p>Missing Configuration: <pre><code>ValueError: Azure OpenAI requires either:\n  1. use_managed_identity=True (for keyless auth), or\n  2. api_key parameter, or\n  3. AZURE_OPENAI_API_KEY environment variable\n</code></pre></p>"},{"location":"architecture/technical-reference/#dependencies_5","title":"Dependencies","text":"<pre><code>from llama_index.llms.azure_openai import AzureOpenAI  # LlamaIndex wrapper\nfrom azure.identity import DefaultAzureCredential       # Optional (for Managed Identity)\nimport tiktoken                                         # Token estimation\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_5","title":"Used By","text":"<ul> <li><code>create_llm_client()</code> factory</li> <li>Any pipeline with <code>provider: azure_openai</code></li> <li>Examples: <code>azure_managed_identity.py</code>, <code>azure_managed_identity_config.yaml</code></li> </ul>"},{"location":"architecture/technical-reference/#testing-strategy_1","title":"Testing Strategy","text":"<p>Unit Tests (11 tests): - Managed Identity authentication flow - Pre-fetched token authentication - API key authentication (backward compatibility) - Error handling (missing dependencies, failed authentication) - Priority ordering (Managed Identity &gt; Token &gt; API Key) - Missing configuration validation</p> <p>Integration Tests: - Require actual Azure credentials (optional) - Test real Azure AD authentication - Verify RBAC permissions</p>"},{"location":"architecture/technical-reference/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Production: Use Managed Identity (no secrets)</li> <li>Development: Use <code>az login</code> (personal credentials)</li> <li>CI/CD: Use Service Principal or Federated Identity</li> <li>Never: Hardcode API keys in source code</li> </ol>"},{"location":"architecture/technical-reference/#performance-characteristics_1","title":"Performance Characteristics","text":"<p>Managed Identity: - Token fetch: ~100-200ms (first call) - Token cached: ~1 hour (Azure AD default) - Subsequent calls: No auth overhead</p> <p>API Key: - No auth overhead (key is static) - Manual rotation required</p>"},{"location":"architecture/technical-reference/#known-limitations_5","title":"Known Limitations","text":"<ul> <li>Managed Identity only works on Azure infrastructure</li> <li>Token refresh not automatic (pipeline must be restarted after 1 hour for long runs)</li> <li>Cannot use Managed Identity with non-Azure OpenAI providers</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_5","title":"Future Improvements","text":"<ul> <li>[ ] Automatic token refresh for long-running pipelines</li> <li>[ ] Support for User-Assigned Managed Identity (currently uses DefaultAzureCredential)</li> <li>[ ] Token caching across pipeline instances</li> <li>[ ] Integration with Azure Key Vault for API key storage</li> </ul>"},{"location":"architecture/technical-reference/#54-mlx-provider-apple-silicon","title":"5.4 MLX Provider (Apple Silicon)","text":""},{"location":"architecture/technical-reference/#purpose_7","title":"Purpose","text":"<p>Enable fast, free, local LLM inference on Apple Silicon using Apple's MLX framework.</p>"},{"location":"architecture/technical-reference/#class-mlxclient","title":"Class: <code>MLXClient</code>","text":"<p>Inheritance: <code>LLMClient</code> (Adapter pattern)</p> <p>Platform: macOS with M1/M2/M3/M4 chips only</p> <p>Responsibility: Local in-process LLM inference using MLX framework</p> <p>Key Features: - In-process (no server management) - Model caching (load once, use many times) - Lazy imports (only when MLX provider used) - Dependency injection (clean testing) - Free inference ($0 cost)</p>"},{"location":"architecture/technical-reference/#architecture_2","title":"Architecture","text":"<pre><code>class MLXClient(LLMClient):\n    def __init__(self, spec: LLMSpec, _mlx_lm_module=None):\n        # Lazy import with helpful error\n        # Load model once (cached in instance)\n\n    def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n        # Use cached model for inference\n        # No server calls, all local\n</code></pre> <p>Design Decision: Dependency Injection</p> <p>The <code>_mlx_lm_module</code> parameter enables clean testing:</p> <pre><code># Production usage (normal)\nclient = MLXClient(spec)  # Imports mlx_lm automatically\n\n# Testing usage (mocked)\nmock_mlx = MagicMock()\nclient = MLXClient(spec, _mlx_lm_module=mock_mlx)  # Inject mock\n</code></pre> <p>Rationale: - Testable: No sys.modules hacks - Clear: Explicit what's being injected - Backward compatible: Parameter is optional</p>"},{"location":"architecture/technical-reference/#model-caching-strategy","title":"Model Caching Strategy","text":"<p>Problem: Loading MLX models is expensive (~0.5-2 seconds)</p> <p>Solution: Load once in <code>__init__()</code>, cache in instance</p> <p>Impact: - First invocation: ~0.7s (load) + ~0.7s (inference) = 1.4s - Subsequent calls: ~0.7s (inference only) - For 100 rows: Save ~70 seconds vs reload each time!</p>"},{"location":"architecture/technical-reference/#token-estimation_1","title":"Token Estimation","text":"<p>Primary: Use MLX tokenizer (model-specific, accurate) <pre><code>tokens = len(self.mlx_tokenizer.encode(text))\n</code></pre></p> <p>Fallback: Word count (if tokenizer fails) <pre><code>tokens = len(text.split())\n</code></pre></p> <p>Rationale: Graceful degradation, never fail on token estimation</p>"},{"location":"architecture/technical-reference/#error-handling_1","title":"Error Handling","text":"<p>Import Error: <pre><code>ImportError: MLX not installed. Install with:\n  pip install ondine[mlx]\nor:\n  pip install mlx mlx-lm\n\nNote: MLX only works on Apple Silicon (M1/M2/M3/M4 chips)\n</code></pre></p> <p>Model Load Error: <pre><code>Exception: Failed to load MLX model 'model-name'.\nEnsure the model exists on HuggingFace and you have access.\nError: [original error]\n</code></pre></p> <p>Design: Actionable error messages with context</p>"},{"location":"architecture/technical-reference/#performance-characteristics_2","title":"Performance Characteristics","text":"<p>Tested on Apple Silicon (M2): - Model: <code>mlx-community/Qwen3-1.7B-4bit</code> - Load time: 0.74s (once per pipeline) - Inference: 0.67s/prompt - Throughput: 1.49 prompts/sec - Memory: ~2GB (model in RAM)</p> <p>Comparison: - vs Cloud APIs: 10x faster (no network), free - vs vLLM: Simpler (no server), Mac-compatible - vs Ollama: Similar speed, more control</p>"},{"location":"architecture/technical-reference/#thread-safety_1","title":"Thread Safety","text":"<ul> <li>Not thread-safe: MLX models are not thread-safe</li> <li>Recommendation: Use <code>concurrency=1</code> in ProcessingSpec</li> <li>Why: MLX optimized for Apple Neural Engine (single-threaded)</li> </ul>"},{"location":"architecture/technical-reference/#dependencies_6","title":"Dependencies","text":"<pre><code>import mlx_lm  # Lazy imported\n# mlx_lm depends on:\n# - mlx&gt;=0.29.0\n# - mlx-metal (GPU acceleration)\n# - transformers (HuggingFace)\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_6","title":"Used By","text":"<ul> <li><code>create_llm_client()</code> factory</li> <li>Any pipeline with <code>provider: mlx</code></li> <li>Examples: <code>10_mlx_qwen3_local.py</code>, <code>10_mlx_qwen3_local.yaml</code></li> </ul>"},{"location":"architecture/technical-reference/#testing-strategy_2","title":"Testing Strategy","text":"<p>Dependency Injection Pattern: <pre><code># Create mock module\nmock_mlx = MagicMock()\nmock_mlx.load.return_value = (mock_model, mock_tokenizer)\nmock_mlx.generate.return_value = \"response\"\n\n# Inject for testing\nclient = MLXClient(spec, _mlx_lm_module=mock_mlx)\n</code></pre></p> <p>Coverage: - 14 unit tests - Model loading/caching - Token estimation with fallback - Error handling - Factory integration</p>"},{"location":"architecture/technical-reference/#known-limitations_6","title":"Known Limitations","text":"<ul> <li>macOS only (MLX doesn't work on Linux/Windows)</li> <li>Single-threaded (not optimized for concurrency)</li> <li>Model download required (1-5GB per model)</li> <li>Requires HuggingFace token for some models</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_6","title":"Future Improvements","text":"<ul> <li>[ ] Add MLX streaming support</li> <li>[ ] Support custom MLX generation parameters</li> <li>[ ] Add model warmup option</li> <li>[ ] Cache models globally (shared across pipelines)</li> </ul>"},{"location":"architecture/technical-reference/#54-llm-provider-presets","title":"5.4 LLM Provider Presets","text":""},{"location":"architecture/technical-reference/#purpose_8","title":"Purpose","text":"<p>Simplify LLM provider configuration by providing pre-configured specifications for common providers, eliminating boilerplate and configuration errors.</p>"},{"location":"architecture/technical-reference/#class-llmproviderpresets","title":"Class: <code>LLMProviderPresets</code>","text":"<p>Location: <code>ondine/core/specifications.py</code> (lines 301-453)</p> <p>Responsibility: Provide pre-validated LLMSpec instances for popular providers</p> <p>Design Pattern: Static Registry Pattern</p> <p>Key Features: - Zero boilerplate for common providers (80% code reduction) - Pre-validated configurations (correct URLs, pricing) - No hardcoded API keys (security by design) - IDE autocomplete support - Pydantic validation throughout</p>"},{"location":"architecture/technical-reference/#available-presets","title":"Available Presets","text":""},{"location":"architecture/technical-reference/#openai-presets","title":"OpenAI Presets","text":"<pre><code>GPT4O_MINI = LLMSpec(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    input_cost_per_1k_tokens=Decimal(\"0.00015\"),\n    output_cost_per_1k_tokens=Decimal(\"0.0006\"),\n)\n\nGPT4O = LLMSpec(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o\",\n    input_cost_per_1k_tokens=Decimal(\"0.0025\"),\n    output_cost_per_1k_tokens=Decimal(\"0.01\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#togetherai-presets","title":"Together.AI Presets","text":"<pre><code>TOGETHER_AI_LLAMA_70B = LLMSpec(\n    provider=LLMProvider.OPENAI_COMPATIBLE,\n    provider_name=\"Together.AI\",\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n    base_url=\"https://api.together.xyz/v1\",\n    input_cost_per_1k_tokens=Decimal(\"0.0006\"),\n)\n\nTOGETHER_AI_LLAMA_8B = LLMSpec(\n    provider=LLMProvider.OPENAI_COMPATIBLE,\n    provider_name=\"Together.AI\",\n    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    base_url=\"https://api.together.xyz/v1\",\n    input_cost_per_1k_tokens=Decimal(\"0.0001\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#ollama-local-presets-free","title":"Ollama Local Presets (Free)","text":"<pre><code>OLLAMA_LLAMA_70B = LLMSpec(\n    provider=LLMProvider.OPENAI_COMPATIBLE,\n    provider_name=\"Ollama-Local\",\n    model=\"llama3.1:70b\",\n    base_url=\"http://localhost:11434/v1\",\n    input_cost_per_1k_tokens=Decimal(\"0.0\"),  # FREE!\n    output_cost_per_1k_tokens=Decimal(\"0.0\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#groq-anthropic-presets","title":"Groq &amp; Anthropic Presets","text":"<pre><code>GROQ_LLAMA_70B = LLMSpec(\n    provider=LLMProvider.GROQ,\n    model=\"llama-3.1-70b-versatile\",\n    input_cost_per_1k_tokens=Decimal(\"0.00059\"),\n)\n\nCLAUDE_SONNET_4 = LLMSpec(\n    provider=LLMProvider.ANTHROPIC,\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=8192,\n    input_cost_per_1k_tokens=Decimal(\"0.003\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#factory-method-create_custom_openai_compatible","title":"Factory Method: <code>create_custom_openai_compatible()</code>","text":"<p>Purpose: Simplify custom provider configuration (vLLM, LocalAI, etc.)</p> <pre><code>@classmethod\ndef create_custom_openai_compatible(\n    cls,\n    provider_name: str,\n    model: str,\n    base_url: str,\n    input_cost_per_1k: float = 0.0,\n    output_cost_per_1k: float = 0.0,\n    **kwargs\n) -&gt; LLMSpec:\n    \"\"\"Factory for custom OpenAI-compatible providers.\"\"\"\n    return LLMSpec(\n        provider=LLMProvider.OPENAI_COMPATIBLE,\n        provider_name=provider_name,\n        model=model,\n        base_url=base_url,\n        input_cost_per_1k_tokens=Decimal(str(input_cost_per_1k)),\n        output_cost_per_1k_tokens=Decimal(str(output_cost_per_1k)),\n        **kwargs\n    )\n</code></pre>"},{"location":"architecture/technical-reference/#usage-with-pipelinebuilderwith_llm_spec","title":"Usage with <code>PipelineBuilder.with_llm_spec()</code>","text":"<p>New Method: <code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder</code></p> <p>Location: <code>ondine/api/pipeline_builder.py</code> (lines 260-311)</p> <p>Purpose: Accept pre-built LLMSpec objects instead of individual parameters</p> <p>Example Usage:</p> <pre><code>from ondine.core.specifications import LLMProviderPresets\n\n# Simple preset usage\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n    .build()\n)\n\n# Override preset settings\ncustom = LLMProviderPresets.GPT4O_MINI.model_copy(\n    update={\"temperature\": 0.9, \"max_tokens\": 500}\n)\npipeline.with_llm_spec(custom)\n\n# Custom provider via factory\ncustom_vllm = LLMProviderPresets.create_custom_openai_compatible(\n    provider_name=\"My vLLM Server\",\n    model=\"mistral-7b-instruct\",\n    base_url=\"http://my-server:8000/v1\",\n    temperature=0.7\n)\npipeline.with_llm_spec(custom_vllm)\n</code></pre>"},{"location":"architecture/technical-reference/#comparison-before-vs-after","title":"Comparison: Before vs After","text":"<p>Before (parameter-based): <pre><code>.with_llm(\n    provider=\"openai_compatible\",\n    provider_name=\"Together.AI\",\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=\"${TOGETHER_API_KEY}\",\n    input_cost_per_1k_tokens=0.0006,\n    output_cost_per_1k_tokens=0.0006\n)\n</code></pre></p> <p>After (preset-based): <pre><code>.with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n</code></pre></p> <p>Result: 80% code reduction, zero configuration errors</p>"},{"location":"architecture/technical-reference/#design-decisions","title":"Design Decisions","text":"<p>Why Static Class Instead of Enum? - Allows class methods (factory) - Better IDE autocomplete - Can include documentation in docstrings - Easier to extend without breaking existing code</p> <p>Why No Hardcoded API Keys? - Security: API keys should never be in code - All presets have <code>api_key=None</code> by default - Users must provide via environment variables or <code>model_copy(update={\"api_key\": \"...\"})</code></p> <p>Why Pydantic <code>model_copy()</code> for Overrides? - Immutability: Original presets unchanged - Type safety: Validation on override - Pythonic: Standard Pydantic pattern - Flexible: Override any field</p>"},{"location":"architecture/technical-reference/#security-validation","title":"Security Validation","text":"<p>Test: All presets must have <code>api_key=None</code> <pre><code>def test_presets_have_no_api_keys(self):\n    \"\"\"Security requirement: No hardcoded API keys.\"\"\"\n    presets = [\n        LLMProviderPresets.GPT4O_MINI,\n        LLMProviderPresets.TOGETHER_AI_LLAMA_70B,\n        # ... all presets\n    ]\n    for preset in presets:\n        assert preset.api_key is None\n</code></pre></p>"},{"location":"architecture/technical-reference/#backward-compatibility","title":"Backward Compatibility","text":"<p>100% backward compatible: Existing <code>with_llm()</code> method unchanged <pre><code># Old way still works\npipeline.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n# New way\npipeline.with_llm_spec(LLMProviderPresets.GPT4O_MINI)\n\n# Both methods can be mixed (last call wins)\n</code></pre></p>"},{"location":"architecture/technical-reference/#testing-coverage","title":"Testing Coverage","text":"<p>26 unit tests (100% pass): - Preset configuration validation - Security checks (no API keys) - Type safety verification - <code>with_llm_spec()</code> method tests - Override via <code>model_copy()</code> tests - Factory method tests - Backward compatibility tests</p>"},{"location":"architecture/technical-reference/#examples","title":"Examples","text":"<p>Complete example: <code>examples/14_provider_presets.py</code> - Demonstrates all preset usage patterns - Shows customization via <code>model_copy()</code> - Compares old vs new approach - Lists all available presets</p>"},{"location":"architecture/technical-reference/#future-improvements_7","title":"Future Improvements","text":"<ul> <li>[ ] Add more provider presets (Cohere, AI21, Mistral)</li> <li>[ ] Add model size variants (405B, 8B, etc.)</li> <li>[ ] Version-specific presets with deprecation warnings</li> <li>[ ] Auto-update pricing from provider APIs</li> <li>[ ] YAML preset files for user-defined presets</li> </ul>"},{"location":"architecture/technical-reference/#56-observability-module","title":"5.6 Observability Module","text":""},{"location":"architecture/technical-reference/#purpose_9","title":"Purpose","text":"<p>Plugin-based observability system that leverages LlamaIndex's built-in instrumentation for automatic tracking of all LLM calls.</p>"},{"location":"architecture/technical-reference/#module-ondineobservability","title":"Module: <code>ondine/observability/</code>","text":"<p>Location: <code>ondine/observability/</code> (9 files, ~600 lines)</p> <p>Responsibility: Configure LlamaIndex observability handlers via clean Ondine API</p> <p>Architecture: Plugin-based with observer registry</p> <p>Key Features: - Powered by LlamaIndex: Leverages battle-tested LlamaIndex observability handlers - Automatic instrumentation: All LLM calls tracked automatically (zero manual instrumentation) - Multiple observers: OpenTelemetry, Langfuse, Logging (can use simultaneously) - PII sanitization: Comprehensive regex-based redaction (custom Ondine feature) - Fault-tolerant: Observer failures never crash the pipeline - Clean API: One-line observer configuration via <code>with_observer()</code></p>"},{"location":"architecture/technical-reference/#design-philosophy","title":"Design Philosophy","text":"<p>\"Don't Reinvent the Wheel\"</p> <p>Ondine delegates LLM call tracking to LlamaIndex's native handlers: - \u2705 LlamaIndex automatically instruments <code>llm.chat()</code> calls - \u2705 Ondine configures these handlers via clean API - \u2705 Minimal code (~600 lines vs. 2000+ for custom implementation) - \u2705 Production-ready, battle-tested - \u2705 Auto-updates when LlamaIndex improves</p>"},{"location":"architecture/technical-reference/#classes_4","title":"Classes","text":""},{"location":"architecture/technical-reference/#pipelineobserver-abstract-base-class","title":"<code>PipelineObserver</code> (Abstract Base Class)","text":"<p>Location: <code>ondine/observability/base.py</code></p> <p>Responsibility: Define observer interface for pipeline events</p> <p>Pattern: Observer Pattern + Plugin Architecture</p> <p>Methods: <pre><code>def on_pipeline_start(event) -&gt; None:  # Optional\ndef on_stage_start(event) -&gt; None:     # Optional\ndef on_llm_call(event) -&gt; None:        # REQUIRED (abstract)\ndef on_stage_end(event) -&gt; None:       # Optional\ndef on_error(event) -&gt; None:           # Optional\ndef on_pipeline_end(event) -&gt; None:    # Optional\ndef flush() -&gt; None:                   # Optional\ndef close() -&gt; None:                   # Optional\n</code></pre></p> <p>Design Decision: Only <code>on_llm_call()</code> is abstract - LLM calls are the most critical events - Other events optional (not all observers need them)</p>"},{"location":"architecture/technical-reference/#observerregistry-class","title":"<code>ObserverRegistry</code> (Class)","text":"<p>Location: <code>ondine/observability/registry.py</code></p> <p>Responsibility: Plugin registry for observer implementations</p> <p>Pattern: Registry Pattern with decorator-based registration</p> <p>Methods: <pre><code>@classmethod\ndef register(name, observer_class) -&gt; None:\n    \"\"\"Register an observer implementation.\"\"\"\n\n@classmethod\ndef get(name) -&gt; Type[PipelineObserver]:\n    \"\"\"Get observer class by name.\"\"\"\n\n@classmethod\ndef list_observers() -&gt; list[str]:\n    \"\"\"List all registered observers.\"\"\"\n</code></pre></p> <p>Decorator: <pre><code>@observer(\"my_observer\")\nclass MyObserver(PipelineObserver):\n    def on_llm_call(self, event):\n        print(f\"LLM called: {event.model}\")\n</code></pre></p> <p>Auto-registration: Observers register on import via <code>@observer</code> decorator</p>"},{"location":"architecture/technical-reference/#opentelemetryobserver-class","title":"<code>OpenTelemetryObserver</code> (Class)","text":"<p>Location: <code>ondine/observability/observers/opentelemetry_observer.py</code></p> <p>Responsibility: Delegate to LlamaIndex's OpenTelemetry handler</p> <p>Implementation: <pre><code>@observer(\"opentelemetry\")\nclass OpenTelemetryObserver(PipelineObserver):\n    def __init__(self, config):\n        # Configure LlamaIndex OpenTelemetry handler\n        LlamaIndexHandlerManager.configure_handler(\"opentelemetry\", config)\n\n    def on_llm_call(self, event):\n        # LlamaIndex handles this automatically!\n        pass\n</code></pre></p> <p>What LlamaIndex Tracks (automatic): - \u2705 LLM call spans with prompts, completions - \u2705 Token usage and latency - \u2705 Model information - \u2705 Export to Jaeger, Datadog, Grafana, etc.</p> <p>Lines of Code: ~70 (vs. 200+ for custom implementation)</p>"},{"location":"architecture/technical-reference/#langfuseobserver-class","title":"<code>LangfuseObserver</code> (Class)","text":"<p>Location: <code>ondine/observability/observers/langfuse_observer.py</code></p> <p>Responsibility: Delegate to LlamaIndex's Langfuse handler</p> <p>Configuration: <pre><code>{\n    \"public_key\": \"pk-lf-...\",\n    \"secret_key\": \"sk-lf-...\",  # pragma: allowlist secret\n    \"host\": \"https://cloud.langfuse.com\"  # optional\n}\n</code></pre></p> <p>What LlamaIndex Tracks (automatic): - \u2705 Full prompts and completions - \u2705 Token usage and costs - \u2705 Latency metrics - \u2705 Model information - \u2705 Trace hierarchy - \u2705 User feedback integration (future)</p> <p>Lines of Code: ~85 (vs. 240+ for custom implementation)</p>"},{"location":"architecture/technical-reference/#loggingobserver-class","title":"<code>LoggingObserver</code> (Class)","text":"<p>Location: <code>ondine/observability/observers/logging_observer.py</code></p> <p>Responsibility: Delegate to LlamaIndex's Simple handler</p> <p>What LlamaIndex Logs (automatic): - \u2705 LLM calls with prompts - \u2705 Token usage - \u2705 Latency - \u2705 Console output (no external dependencies)</p> <p>Lines of Code: ~70 (vs. 170+ for custom implementation)</p>"},{"location":"architecture/technical-reference/#llamaindexhandlermanager-class","title":"<code>LlamaIndexHandlerManager</code> (Class)","text":"<p>Location: <code>ondine/observability/llamaindex_handlers.py</code></p> <p>Responsibility: Configure LlamaIndex global handlers</p> <p>Methods: <pre><code>@classmethod\ndef configure_handler(handler_type: str, config: dict):\n    \"\"\"Configure a LlamaIndex global handler.\"\"\"\n    from llama_index.core import set_global_handler\n\n    if handler_type == \"opentelemetry\":\n        set_global_handler(\"opentelemetry\", **config)\n    elif handler_type == \"langfuse\":\n        set_global_handler(\"langfuse\",\n            public_key=config[\"public_key\"],\n            secret_key=config[\"secret_key\"])\n    elif handler_type == \"simple\":\n        set_global_handler(\"simple\")\n</code></pre></p> <p>Design Decision: Centralized handler configuration - Single place to manage LlamaIndex handlers - Consistent error handling - Future: Multi-handler support via LlamaIndex dispatcher</p>"},{"location":"architecture/technical-reference/#pii-sanitization-custom-ondine-feature","title":"PII Sanitization (Custom Ondine Feature)","text":"<p>Location: <code>ondine/observability/sanitizer.py</code></p> <p>LlamaIndex doesn't provide this, so we added it as a custom feature.</p> <p>Patterns: <pre><code>PII_PATTERNS = {\n    \"email\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n    \"ssn\": r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",\n    \"credit_card\": r\"\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b\",\n    \"phone_us\": r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",\n    \"api_key\": r\"\\b(?:api[_-]?key|secret|token)[:\\s=]+...\",\n    \"ip_address\": r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\",\n}\n</code></pre></p> <p>Functions: <pre><code>def sanitize_text(text, patterns, replacement=\"[REDACTED]\"):\n    \"\"\"Replace PII with redaction marker.\"\"\"\n\ndef sanitize_event(event, config):\n    \"\"\"Sanitize LLMCallEvent fields.\"\"\"\n</code></pre></p> <p>Usage (future): <pre><code>.with_observer(\"langfuse\", config={\n    \"public_key\": \"...\",\n    \"sanitize_prompts\": True,  # Enable PII redaction\n    \"custom_patterns\": {\"account_id\": r\"ACC-\\d{6}\"}\n})\n</code></pre></p>"},{"location":"architecture/technical-reference/#integration-with-pipeline","title":"Integration with Pipeline","text":"<p>PipelineBuilder API: <pre><code>def with_observer(self, name: str, config: dict = None):\n    \"\"\"Add observability observer.\"\"\"\n    # Validates observer is registered\n    # Stores config for later instantiation\n    self._observers.append((name, config))\n    return self\n</code></pre></p> <p>Pipeline Initialization: <pre><code># In Pipeline.execute():\nobserver_configs = self.specifications.metadata.get(\"observers\", [])\nfor observer_name, observer_config in observer_configs:\n    observer_class = ObserverRegistry.get(observer_name)\n    observer = observer_class(config=observer_config)\n    # Observer.__init__() configures LlamaIndex handler\n    # LlamaIndex automatically instruments all subsequent LLM calls!\n</code></pre></p>"},{"location":"architecture/technical-reference/#how-it-works-complete-flow","title":"How It Works (Complete Flow)","text":"<pre><code>1. User calls: .with_observer(\"langfuse\", config={...})\n   \u2514\u2500&gt; Stored in PipelineBuilder._observers\n\n2. Pipeline.execute() runs:\n   \u2514\u2500&gt; Instantiates LangfuseObserver(config)\n       \u2514\u2500&gt; LangfuseObserver.__init__() calls:\n           \u2514\u2500&gt; LlamaIndexHandlerManager.configure_handler(\"langfuse\", config)\n               \u2514\u2500&gt; set_global_handler(\"langfuse\", public_key=..., secret_key=...)\n                   \u2514\u2500&gt; LlamaIndex activates Langfuse instrumentation\n\n3. LLM calls happen:\n   \u2514\u2500&gt; llm_client.invoke(prompt)\n       \u2514\u2500&gt; self.client.chat([message])  # LlamaIndex LLM\n           \u2514\u2500&gt; LlamaIndex automatically sends to Langfuse!\n               - Prompt \u2705\n               - Completion \u2705\n               - Tokens \u2705\n               - Cost \u2705\n               - Latency \u2705\n\n4. Pipeline completes:\n   \u2514\u2500&gt; Observer.flush() and close() called\n       \u2514\u2500&gt; LlamaIndex flushes buffered events\n</code></pre>"},{"location":"architecture/technical-reference/#benefits-of-delegating-to-llamaindex","title":"Benefits of Delegating to LlamaIndex","text":"<p>Code Reduction: - Before: ~2000 lines (custom event system) - After: ~600 lines (thin wrappers) - Savings: 70% less code to maintain!</p> <p>Quality: - \u2705 Battle-tested by LlamaIndex community - \u2705 Production-ready - \u2705 Actively maintained - \u2705 Auto-updates with LlamaIndex</p> <p>Features (free from LlamaIndex): - \u2705 Automatic LLM call tracking - \u2705 OpenTelemetry integration - \u2705 Langfuse integration - \u2705 Multiple handler support - \u2705 Future: RAG instrumentation when we add it</p>"},{"location":"architecture/technical-reference/#dependencies_7","title":"Dependencies","text":"<pre><code># Core (required in pyproject.toml)\nopentelemetry-api&gt;=1.20.0\nopentelemetry-sdk&gt;=1.20.0\nlangfuse&gt;=2.0.0\n\n# Optional exporters\nopentelemetry-exporter-jaeger&gt;=1.20.0  # For Jaeger\nopentelemetry-exporter-otlp&gt;=1.20.0    # For OTLP protocol\n</code></pre>"},{"location":"architecture/technical-reference/#usage-examples","title":"Usage Examples","text":"<p>Simple logging: <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_observer(\"logging\", config={})\n    .build()\n)\n</code></pre></p> <p>Langfuse (LLM observability): <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_observer(\"langfuse\", config={\n        \"public_key\": \"pk-lf-...\",\n        \"secret_key\": \"sk-lf-...\"\n    })\n    .build()\n)\n</code></pre></p> <p>Multiple observers: <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_observer(\"langfuse\", config={...})\n    .with_observer(\"opentelemetry\", config={...})\n    .with_observer(\"logging\", config={})\n    .build()\n)\n</code></pre></p>"},{"location":"architecture/technical-reference/#thread-safety_2","title":"Thread Safety","text":"<ul> <li>Thread-safe: Yes (LlamaIndex handlers are thread-safe)</li> <li>Fault-tolerant: Observer failures don't crash pipeline</li> <li>Isolated: Each observer runs in try/except block</li> </ul>"},{"location":"architecture/technical-reference/#performance_1","title":"Performance","text":"<ul> <li>Overhead: &lt;1% (LlamaIndex handles async export)</li> <li>LLM call tracking: Automatic, no manual instrumentation</li> <li>Export: Batched, non-blocking</li> </ul>"},{"location":"architecture/technical-reference/#testing-coverage_1","title":"Testing Coverage","text":"<p>Unit tests: - Observer registry and @observer decorator - Handler manager configuration - PII sanitization patterns</p> <p>Integration tests: - Full pipeline with observers - Multiple observers simultaneously - Observer failure isolation - LlamaIndex auto-instrumentation</p>"},{"location":"architecture/technical-reference/#known-limitations_7","title":"Known Limitations","text":"<ul> <li>One global handler at a time: LlamaIndex's <code>set_global_handler()</code> is global</li> <li>Workaround: Last observer wins</li> <li>Future: Use LlamaIndex's lower-level dispatcher for true multi-observer</li> <li>No pipeline-level events: LlamaIndex tracks components, not our batch pipeline</li> <li>Future: Add custom event handlers for pipeline start/end</li> </ul>"},{"location":"architecture/technical-reference/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>[ ] Multi-observer support via LlamaIndex dispatcher API</li> <li>[ ] Pipeline-level event tracking (start, end, metrics)</li> <li>[ ] PII sanitization integration with handlers</li> <li>[ ] Custom observer tutorial in docs</li> <li>[ ] RAG retrieval tracking (when RAG is added)</li> </ul>"},{"location":"architecture/technical-reference/#57-multi-row-batching-new-november-2024","title":"5.7 Multi-Row Batching (NEW - November 2024)","text":""},{"location":"architecture/technical-reference/#purpose_10","title":"Purpose","text":"<p>Enable processing of N rows in a single API call to achieve 100\u00d7 speedup for large datasets.</p>"},{"location":"architecture/technical-reference/#architecture-strategy-pattern-new-stages","title":"Architecture: Strategy Pattern + New Stages","text":"<p>Design Decision: Use Strategy Pattern for batch formatting + new pipeline stages for aggregation/disaggregation</p> <p>Why This Design: - \u2705 Follows existing Layer 2 pattern (stages are first-class) - \u2705 Strategy Pattern for extensibility (JSON, CSV, XML formats) - \u2705 Single Responsibility (each stage has one job) - \u2705 Backward compatible (only activates when batch_size &gt; 1)</p>"},{"location":"architecture/technical-reference/#module-ondinestrategies","title":"Module: <code>ondine/strategies/</code>","text":"<p>Location: <code>ondine/strategies/</code> (4 files, ~150 lines)</p> <p>Responsibility: Batch formatting strategies for multi-row processing</p>"},{"location":"architecture/technical-reference/#classes_5","title":"Classes","text":"<p><code>BatchFormattingStrategy</code> (Abstract Base Class): <pre><code>class BatchFormattingStrategy(ABC):\n    @abstractmethod\n    def format_batch(self, prompts: list[str], metadata: dict) -&gt; str:\n        \"\"\"Format N prompts into 1 batch prompt.\"\"\"\n        pass\n\n    @abstractmethod\n    def parse_batch_response(self, response: str, expected_count: int) -&gt; list[str]:\n        \"\"\"Parse 1 batch response into N individual results.\"\"\"\n        pass\n</code></pre></p> <p><code>JsonBatchStrategy</code> (Concrete Implementation): - Formats prompts as JSON array: <code>[{\"id\": 1, \"input\": \"...\"}, ...]</code> - Parses responses as JSON array: <code>[{\"id\": 1, \"result\": \"...\"}, ...]</code> - Uses Pydantic models for validation (BatchItem, BatchResult) - Handles partial failures (PartialParseError with parsed_results + failed_ids) - Extracts JSON from markdown code blocks automatically</p> <p><code>BatchItem</code>, <code>BatchResult</code>, <code>BatchMetadata</code> (Pydantic Models): - Type-safe batch request/response structure - Validation with Pydantic - Sorting by ID, missing ID detection</p>"},{"location":"architecture/technical-reference/#stage-batchaggregatorstage","title":"Stage: <code>BatchAggregatorStage</code>","text":"<p>Location: <code>ondine/stages/batch_aggregator_stage.py</code></p> <p>Responsibility: Aggregate N prompts into 1 mega-prompt</p> <p>Flow: <pre><code>Input: PromptBatch with N prompts\n  \u2193\nGroup into chunks of batch_size\n  \u2193\nFor each chunk:\n  - Validate against context window\n  - Use strategy.format_batch() to create mega-prompt\n  - Create new PromptBatch with 1 mega-prompt\n  \u2193\nOutput: PromptBatch with 1 prompt (containing N rows)\n</code></pre></p> <p>Key Features: - Context window validation (checks model limits) - Strategy injection (supports JSON, CSV, etc.) - Preserves row IDs in metadata for disaggregation - Zero cost (aggregation is free)</p>"},{"location":"architecture/technical-reference/#stage-batchdisaggregatorstage","title":"Stage: <code>BatchDisaggregatorStage</code>","text":"<p>Location: <code>ondine/stages/batch_disaggregator_stage.py</code></p> <p>Responsibility: Split 1 mega-response into N individual responses</p> <p>Flow: <pre><code>Input: ResponseBatch with 1 mega-response\n  \u2193\nCheck if batch response (metadata.is_batch)\n  \u2193\nUse strategy.parse_batch_response()\n  \u2193\nHandle 3 scenarios:\n  1. Full success: All N results parsed\n  2. Partial success: Some results parsed (PartialParseError)\n  3. Complete failure: No results parsed (ValueError)\n  \u2193\nCreate ResponseBatch with N individual responses\n  \u2193\nOutput: ResponseBatch with N responses\n</code></pre></p> <p>Error Handling: - Full success: Returns N results - Partial failure: Parses what works, marks failed rows with <code>[PARSE_ERROR]</code> - Complete failure: Marks all rows with <code>[BATCH_PARSE_ERROR]</code></p>"},{"location":"architecture/technical-reference/#utility-model_context_limitspy","title":"Utility: <code>model_context_limits.py</code>","text":"<p>Location: <code>ondine/utils/model_context_limits.py</code></p> <p>Responsibility: Model context window registry and validation</p> <p>Registry: <pre><code>MODEL_CONTEXT_LIMITS = {\n    \"gpt-4o\": 128000,\n    \"gpt-4o-mini\": 128000,\n    \"claude-sonnet-4\": 200000,\n    \"llama-3.1-70b-versatile\": 131072,\n    # ... 50+ models\n}\n</code></pre></p> <p>Functions: - <code>get_context_limit(model)</code> - Get context window for model - <code>validate_batch_size(model, batch_size, avg_tokens)</code> - Validate batch fits in context - <code>suggest_optimal_batch_size(model, avg_tokens)</code> - Suggest optimal batch size</p>"},{"location":"architecture/technical-reference/#pipeline-integration","title":"Pipeline Integration","text":"<p>Conditional Stage Insertion (<code>ondine/api/pipeline.py</code>): <pre><code># Stage 2: Format prompts\nformatter = PromptFormatterStage(...)\nbatches = formatter.process(df, context)\n\n# Stage 2.5: Aggregate (only if batch_size &gt; 1)\nif specs.prompt.batch_size &gt; 1:\n    aggregator = BatchAggregatorStage(batch_size=specs.prompt.batch_size, ...)\n    batches = aggregator.process(batches, context)\n\n# Stage 3: Invoke LLM\nllm_stage = LLMInvocationStage(...)\nresponse_batches = llm_stage.process(batches, context)\n\n# Stage 3.5: Disaggregate (only if batch_size &gt; 1)\nif specs.prompt.batch_size &gt; 1:\n    disaggregator = BatchDisaggregatorStage(...)\n    response_batches = disaggregator.process(response_batches, context)\n\n# Stage 4: Parse responses\nparser_stage = ResponseParserStage(...)\nresults = parser_stage.process(response_batches, context)\n</code></pre></p> <p>Design Decision: Conditional insertion (not always present) - Why: Zero overhead when batch_size=1 (default) - How: Check <code>specs.prompt.batch_size &gt; 1</code> before inserting stages - Benefit: Backward compatible, opt-in only</p>"},{"location":"architecture/technical-reference/#api-pipelinebuilder","title":"API: PipelineBuilder","text":"<p>New Methods: <pre><code>def with_batch_size(self, batch_size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"Enable multi-row batching (process N rows per API call).\"\"\"\n    self._prompt_spec.batch_size = batch_size\n    return self\n\ndef with_batch_strategy(self, strategy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"Set batch formatting strategy ('json' or 'csv').\"\"\"\n    self._prompt_spec.batch_strategy = strategy\n    return self\n\ndef with_processing_batch_size(self, size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"Set internal batch size for PromptFormatterStage (internal optimization).\"\"\"\n    self._processing_spec.batch_size = size\n    return self\n</code></pre></p> <p>Naming Clarification: - <code>with_batch_size()</code> - Multi-row batching (NEW, 100\u00d7 speedup) - <code>with_processing_batch_size()</code> - Internal batching (OLD, renamed)</p>"},{"location":"architecture/technical-reference/#specifications","title":"Specifications","text":"<p>PromptSpec (added fields): <pre><code>class PromptSpec(BaseModel):\n    # ... existing fields ...\n    batch_size: int = Field(default=1, ge=1)  # Multi-row batching\n    batch_strategy: str = Field(default=\"json\")  # \"json\" or \"csv\"\n</code></pre></p>"},{"location":"architecture/technical-reference/#performance-characteristics_3","title":"Performance Characteristics","text":"<p>Benchmarks (verified with real OpenAI API): - Without batching (batch_size=1): 10 rows = 10 API calls - With batching (batch_size=5): 10 rows = 2 API calls (5\u00d7 reduction) - With batching (batch_size=100): 5M rows = 50K API calls (100\u00d7 reduction!)</p> <p>Scaling to 5M Rows: | Batch Size | API Calls | Time | Speedup | |------------|-----------|------|---------| | 1 (default) | 5,000,000 | ~69 hours | 1\u00d7 | | 10 | 500,000 | ~7 hours | 10\u00d7 | | 100 | 50,000 | ~42 minutes | 100\u00d7 | | 500 | 10,000 | ~8 minutes | 500\u00d7 |</p> <p>Limitations: - Batch size limited by model context window (auto-validated) - Larger batches = higher risk of partial failures - JSON parsing overhead (~200 tokens per batch)</p>"},{"location":"architecture/technical-reference/#testing_3","title":"Testing","text":"<p>Unit Tests (24 tests): - <code>test_batch_strategies.py</code> - 16 tests for strategies - <code>test_batch_stages.py</code> - 8 tests for stages</p> <p>Integration Tests (5 tests): - <code>test_batch_processing_openai.py</code> - Real OpenAI API tests - Verified 5\u00d7 reduction with batch_size=5 - Verified backward compatibility (batch_size=1)</p> <p>Coverage: 60% overall, 84% for batch disaggregator, 71% for batch aggregator</p>"},{"location":"architecture/technical-reference/#examples_1","title":"Examples","text":"<p>Example: <code>examples/21_multi_row_batching.py</code> - Example 1: Without batching (baseline) - Example 2: With batching (5\u00d7 speedup) - Example 3: Large dataset extrapolation (5.4M rows)</p>"},{"location":"architecture/technical-reference/#known-limitations_8","title":"Known Limitations","text":"<ul> <li>JSON strategy only (CSV planned for future)</li> <li>No automatic retry for failed rows (marks with error, user must retry)</li> <li>Batch size validation is warning-only (doesn't block)</li> <li>Requires LLM to follow JSON format instructions</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_8","title":"Future Improvements","text":"<ul> <li>[ ] Implement CsvBatchStrategy (more compact than JSON)</li> <li>[ ] Automatic retry for failed rows within batch</li> <li>[ ] Adaptive batch sizing based on prompt length</li> <li>[ ] XML/YAML batch strategies</li> <li>[ ] Batch size optimization suggestions based on historical data</li> </ul> <p>Document Status: IN PROGRESS (Layer 0 complete, Layer 1 documented with MLX, Presets, Observability, and Multi-Row Batching)</p> <p>Next Sections: - 3.6 <code>utils/logging_utils.py</code> - 3.7 <code>utils/metrics_exporter.py</code> - 3.8 <code>utils/input_preprocessing.py</code> - Part 4: Core Models &amp; Specifications - Part 5: Complete Layer 1 (remaining providers) - Part 6+: Remaining layers...</p>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding Ondine's architecture will help you build more sophisticated pipelines and debug issues effectively.</p>"},{"location":"getting-started/core-concepts/#architecture-overview","title":"Architecture Overview","text":"<p>Ondine is built on a layered architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   High-Level APIs (QuickPipeline)      \u2502  User-friendly interfaces\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Pipeline Builder &amp; Configuration     \u2502  Fluent API, YAML config\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Pipeline Orchestration               \u2502  Execution strategies\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Pipeline Stages                      \u2502  Composable processing units\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Adapters (LLM, Storage, IO)         \u2502  External integrations\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/core-concepts/#key-components","title":"Key Components","text":""},{"location":"getting-started/core-concepts/#1-pipeline","title":"1. Pipeline","text":"<p>The <code>Pipeline</code> is the central execution unit. It orchestrates the flow of data through stages.</p> <pre><code>from ondine import Pipeline\n\n# Pipelines are built via PipelineBuilder\npipeline = PipelineBuilder.create()...build()\n\n# Execute synchronously\nresult = pipeline.execute()\n\n# Execute asynchronously\nresult = await pipeline.execute_async()\n</code></pre> <p>Key characteristics: - Immutable once built (thread-safe) - Encapsulates all configuration - Handles checkpointing and recovery - Tracks costs and metrics</p>"},{"location":"getting-started/core-concepts/#2-pipeline-stages","title":"2. Pipeline Stages","text":"<p>Ondine processes data through a series of composable stages:</p> <ol> <li>DataLoaderStage - Load data from CSV, Excel, Parquet, or DataFrame</li> <li>PromptFormatterStage - Format prompts with row data</li> <li>BatchAggregatorStage - (Optional) Aggregate N prompts into 1 for multi-row batching</li> <li>LLMInvocationStage - Call LLM API with retry and rate limiting</li> <li>BatchDisaggregatorStage - (Optional) Split batch response into N results</li> <li>ResponseParserStage - Parse LLM responses (text, JSON, regex)</li> <li>ResultWriterStage - Write results to output</li> </ol> <p>Multi-Row Batching (NEW): - Stages 3 and 5 are only inserted when <code>batch_size &gt; 1</code> - Enables 100\u00d7 speedup by processing N rows per API call - Automatic context window validation - Partial failure handling</p>"},{"location":"getting-started/core-concepts/#3-pipeline-builder","title":"3. Pipeline Builder","text":"<p>The <code>PipelineBuilder</code> provides a fluent API for constructing pipelines:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    # Data source\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n\n    # Prompt configuration\n    .with_prompt(\"Process: {text}\")\n\n    # LLM configuration\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n    # Processing configuration\n    .with_batch_size(100)\n    .with_concurrency(5)\n    .with_retry_policy(max_retries=3)\n\n    # Build immutable pipeline\n    .build()\n)\n</code></pre> <p>Builder methods: - Data: <code>from_csv()</code>, <code>from_dataframe()</code>, <code>from_parquet()</code>, <code>from_excel()</code> - Prompt: <code>with_prompt()</code>, <code>with_system_prompt()</code> - LLM: <code>with_llm()</code>, <code>with_llm_spec()</code> - Processing: <code>with_batch_size()</code>, <code>with_concurrency()</code>, <code>with_rate_limit()</code> - Reliability: <code>with_retry_policy()</code>, <code>with_checkpoint()</code> - Cost: <code>with_max_budget()</code> - Execution: <code>with_async_execution()</code>, <code>with_streaming()</code></p>"},{"location":"getting-started/core-concepts/#3-pipeline-stages","title":"3. Pipeline Stages","text":"<p>Stages are composable processing units that form a pipeline:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Data     \u2502\u2500\u2500\u2500\u25b6\u2502   Prompt    \u2502\u2500\u2500\u2500\u25b6\u2502     LLM      \u2502\u2500\u2500\u2500\u25b6\u2502   Response   \u2502\n\u2502   Loader   \u2502    \u2502  Formatter  \u2502    \u2502  Invocation  \u2502    \u2502    Parser    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Built-in stages: - <code>DataLoaderStage</code>: Load data from files/dataframes - <code>PromptFormatterStage</code>: Format prompts with variables - <code>LLMInvocationStage</code>: Call LLM APIs - <code>ResponseParserStage</code>: Parse and validate responses - <code>ResultWriterStage</code>: Write results to storage</p> <p>Custom stages: You can create custom stages by extending <code>PipelineStage</code>:</p> <pre><code>from ondine.stages import PipelineStage\n\nclass MyCustomStage(PipelineStage):\n    def process(self, input_data, context):\n        # Your processing logic\n        return processed_data\n\n    def validate_input(self, input_data):\n        # Validation logic\n        return ValidationResult(valid=True)\n</code></pre>"},{"location":"getting-started/core-concepts/#4-specifications","title":"4. Specifications","text":"<p>Specifications are Pydantic models that define configuration:</p>"},{"location":"getting-started/core-concepts/#datasetspec","title":"DatasetSpec","text":"<p>Defines input data configuration:</p> <pre><code>from ondine.core.specifications import DatasetSpec\n\nspec = DatasetSpec(\n    source=\"data.csv\",\n    input_columns=[\"text\"],\n    output_columns=[\"result\"],\n    format=\"csv\"\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#promptspec","title":"PromptSpec","text":"<p>Defines prompt templates:</p> <pre><code>from ondine.core.specifications import PromptSpec\n\nspec = PromptSpec(\n    template=\"Summarize: {text}\",\n    system_prompt=\"You are a helpful assistant.\"\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#llmspec","title":"LLMSpec","text":"<p>Defines LLM provider configuration:</p> <pre><code>from ondine.core.specifications import LLMSpec\n\nspec = LLMSpec(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=1000,\n    api_key=\"sk-...\"  # Or use environment variable\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#processingspec","title":"ProcessingSpec","text":"<p>Defines execution configuration:</p> <pre><code>from ondine.core.specifications import ProcessingSpec\n\nspec = ProcessingSpec(\n    batch_size=100,\n    concurrency=5,\n    max_retries=3,\n    checkpoint_interval=500,\n    rate_limit=60  # requests per minute\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#5-execution-strategies","title":"5. Execution Strategies","text":"<p>Ondine supports multiple execution modes:</p>"},{"location":"getting-started/core-concepts/#synchronous-default","title":"Synchronous (Default)","text":"<p>Single-threaded, sequential processing:</p> <pre><code>result = pipeline.execute()\n</code></pre> <p>Use when: Dataset fits in memory, simplicity is priority.</p>"},{"location":"getting-started/core-concepts/#asynchronous","title":"Asynchronous","text":"<p>Concurrent processing with async/await:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_async_execution(max_concurrency=10)\n    .build()\n)\n\nresult = await pipeline.execute_async()\n</code></pre> <p>Use when: Need high throughput, LLM API supports async.</p>"},{"location":"getting-started/core-concepts/#streaming","title":"Streaming","text":"<p>Memory-efficient processing for large datasets:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_streaming(chunk_size=1000)\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre> <p>Use when: Dataset is large (100K+ rows), memory is limited.</p> <p>See Execution Modes Guide for detailed comparison.</p>"},{"location":"getting-started/core-concepts/#6-adapters","title":"6. Adapters","text":"<p>Adapters abstract external dependencies:</p>"},{"location":"getting-started/core-concepts/#llm-client","title":"LLM Client","text":"<p>Adapts different LLM providers to a common interface:</p> <pre><code>from ondine.adapters import LLMClient\n\n# Automatically selected based on provider\nclient = create_llm_client(llm_spec)\nresponse = client.complete(prompt, temperature=0.7)\n</code></pre> <p>Supported providers: - OpenAI - Azure OpenAI - Anthropic Claude - Groq - MLX (local Apple Silicon) - Custom OpenAI-compatible APIs</p>"},{"location":"getting-started/core-concepts/#storage","title":"Storage","text":"<p>Handles checkpoint persistence:</p> <pre><code>from ondine.adapters import CheckpointStorage\n\nstorage = CheckpointStorage(path=\"./checkpoints\")\nstorage.save(state)\nstate = storage.load()\n</code></pre>"},{"location":"getting-started/core-concepts/#data-io","title":"Data IO","text":"<p>Handles various data formats:</p> <pre><code>from ondine.adapters import DataIO\n\n# Supports CSV, Parquet, Excel, JSON\ndata = DataIO.read(\"data.csv\")\nDataIO.write(data, \"output.parquet\")\n</code></pre>"},{"location":"getting-started/core-concepts/#execution-flow","title":"Execution Flow","text":"<p>Here's what happens when you call <code>pipeline.execute()</code>:</p> <ol> <li>Validation: Validate configuration and input data</li> <li>Cost Estimation: Calculate expected cost and token usage</li> <li>Checkpoint Check: Look for existing checkpoint to resume</li> <li>Data Loading: Load input data (streaming or in-memory)</li> <li>Prompt Formatting: Format prompts with input variables</li> <li>LLM Invocation: Call LLM API with rate limiting and retries</li> <li>Response Parsing: Parse and validate LLM responses</li> <li>Result Writing: Write results to output (file or DataFrame)</li> <li>Metrics Collection: Aggregate costs, tokens, timing</li> <li>Checkpoint Cleanup: Remove checkpoint on successful completion</li> </ol>"},{"location":"getting-started/core-concepts/#error-handling","title":"Error Handling","text":"<p>Ondine provides robust error handling:</p>"},{"location":"getting-started/core-concepts/#automatic-retries","title":"Automatic Retries","text":"<p>Failed requests are automatically retried with exponential backoff:</p> <pre><code>.with_retry_policy(\n    max_retries=3,\n    backoff_factor=2.0,\n    retry_on=[RateLimitError, NetworkError]\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#checkpointing","title":"Checkpointing","text":"<p>Long-running jobs can be resumed on failure:</p> <pre><code>.with_checkpoint(\"./checkpoints\", interval=100)\n</code></pre>"},{"location":"getting-started/core-concepts/#error-policies","title":"Error Policies","text":"<p>Control how errors are handled:</p> <pre><code>.with_error_policy(\"continue\")  # Continue on errors\n.with_error_policy(\"stop\")      # Stop on first error\n</code></pre>"},{"location":"getting-started/core-concepts/#cost-tracking","title":"Cost Tracking","text":"<p>Ondine tracks costs in real-time:</p> <pre><code>result = pipeline.execute()\n\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\nprint(f\"Input tokens: {result.costs.input_tokens}\")\nprint(f\"Output tokens: {result.costs.output_tokens}\")\nprint(f\"Cost per row: ${result.costs.total_cost / result.metrics.processed_rows:.6f}\")\n</code></pre>"},{"location":"getting-started/core-concepts/#budget-control","title":"Budget Control","text":"<p>Set maximum budget limits:</p> <pre><code>from decimal import Decimal\n\npipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_max_budget(Decimal(\"10.0\"))  # Max $10 USD\n    .build()\n)\n\n# Execution stops if budget exceeded\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/core-concepts/#observability","title":"Observability","text":"<p>Monitor pipeline execution:</p>"},{"location":"getting-started/core-concepts/#progress-bars","title":"Progress Bars","text":"<p>Automatic progress tracking with tqdm:</p> <pre><code>Processing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:45&lt;00:00, 22.1rows/s]\n</code></pre>"},{"location":"getting-started/core-concepts/#structured-logging","title":"Structured Logging","text":"<p>JSON-formatted logs with structlog:</p> <pre><code>from ondine.utils import configure_logging\n\nconfigure_logging(level=\"INFO\", json_format=True)\n</code></pre>"},{"location":"getting-started/core-concepts/#metrics-export","title":"Metrics Export","text":"<p>Export metrics to Prometheus:</p> <pre><code>from ondine.utils import MetricsExporter\n\nexporter = MetricsExporter(port=9090)\nexporter.start()\n</code></pre>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Execution Modes - Choose the right execution strategy</li> <li>Structured Output - Type-safe response parsing</li> <li>Cost Control - Optimize costs and set budgets</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip or uv package manager</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install ondine\n</code></pre>"},{"location":"getting-started/installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>uv is a fast Python package installer and resolver:</p> <pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install ondine\nuv pip install ondine\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Ondine provides optional dependency groups for specific features:</p>"},{"location":"getting-started/installation/#mlx-apple-silicon-local-inference","title":"MLX (Apple Silicon Local Inference)","text":"<p>For running models locally on Apple Silicon (M1/M2/M3/M4) with MLX:</p> <pre><code>pip install ondine[mlx]\n</code></pre> <p>Requirements: - macOS with Apple Silicon (M1/M2/M3/M4) - 16GB+ RAM recommended</p> <p>Supported models: - Qwen-2.5 series - Llama models - Mistral models - Any MLX-compatible model from Hugging Face</p>"},{"location":"getting-started/installation/#observability","title":"Observability","text":"<p>For OpenTelemetry-based observability and tracing:</p> <pre><code>pip install ondine[observability]\n</code></pre> <p>Features: - Distributed tracing with Jaeger - Custom metrics export - Performance monitoring</p>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>For contributing or development:</p> <pre><code>pip install ondine[dev]\n</code></pre> <p>Includes: - pytest and test utilities - ruff for linting - mypy for type checking - pre-commit hooks - Security scanners (bandit, pip-audit)</p>"},{"location":"getting-started/installation/#install-all-optional-dependencies","title":"Install All Optional Dependencies","text":"<pre><code>pip install ondine[mlx,observability,dev]\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that Ondine is installed correctly:</p> <pre><code>import ondine\n\nprint(ondine.__version__)\nprint(\"Ondine installed successfully!\")\n</code></pre> <p>Or use the CLI:</p> <pre><code>ondine --version\n</code></pre>"},{"location":"getting-started/installation/#api-keys-setup","title":"API Keys Setup","text":"<p>Ondine requires API keys for LLM providers. Set them as environment variables:</p>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"  # pragma: allowlist secret\n</code></pre>"},{"location":"getting-started/installation/#azure-openai","title":"Azure OpenAI","text":"<p>Option 1: Managed Identity (Recommended for Production)</p> <pre><code># Install Azure dependencies\npip install ondine[azure]\n\n# For local development, login with Azure CLI\naz login\n\n# No API keys needed! Uses Managed Identity automatically.\n</code></pre> <p>Option 2: API Key (Traditional)</p> <pre><code>export AZURE_OPENAI_API_KEY=\"...\"  # pragma: allowlist secret\nexport AZURE_OPENAI_ENDPOINT=\"https://...\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-15-preview\"\n</code></pre> <p>See Azure Managed Identity Guide for detailed setup.</p>"},{"location":"getting-started/installation/#anthropic-claude","title":"Anthropic Claude","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"  # pragma: allowlist secret\n</code></pre>"},{"location":"getting-started/installation/#groq","title":"Groq","text":"<pre><code>export GROQ_API_KEY=\"gsk_...\"  # pragma: allowlist secret\n</code></pre>"},{"location":"getting-started/installation/#environment-file","title":"Environment File","text":"<p>For convenience, create a <code>.env</code> file in your project root:</p> <pre><code># .env\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGROQ_API_KEY=gsk_...\n</code></pre> <p>Ondine automatically loads <code>.env</code> files using python-dotenv.</p>"},{"location":"getting-started/installation/#upgrade","title":"Upgrade","text":"<p>To upgrade to the latest version:</p> <pre><code>pip install --upgrade ondine\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-error-no-module-named-ondine","title":"Import Error: No module named 'ondine'","text":"<p>Make sure you're in the correct Python environment:</p> <pre><code>python -c \"import sys; print(sys.executable)\"\npip list | grep ondine\n</code></pre>"},{"location":"getting-started/installation/#mlx-installation-issues","title":"MLX Installation Issues","text":"<p>MLX only works on Apple Silicon. If you get import errors:</p> <ol> <li>Verify you're on Apple Silicon: <code>uname -m</code> should show <code>arm64</code></li> <li>Install Xcode Command Line Tools: <code>xcode-select --install</code></li> <li>Try reinstalling: <code>pip uninstall mlx mlx-lm &amp;&amp; pip install ondine[mlx]</code></li> </ol>"},{"location":"getting-started/installation/#api-key-not-found","title":"API Key Not Found","text":"<p>If you get authentication errors:</p> <ol> <li>Check environment variables: <code>echo $OPENAI_API_KEY</code></li> <li>Verify <code>.env</code> file is in the correct directory</li> <li>Restart your Python session after setting environment variables</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Build your first pipeline</li> <li>Core Concepts - Understand the architecture</li> <li>Provider Configuration - Configure LLM providers</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get started with Ondine in 5 minutes. This guide walks you through your first pipeline.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Ondine installed (<code>pip install ondine</code>)</li> <li>OpenAI API key (or another LLM provider)</li> </ul>"},{"location":"getting-started/quickstart/#your-first-pipeline","title":"Your First Pipeline","text":""},{"location":"getting-started/quickstart/#1-setup","title":"1. Setup","text":"<p>Create a new Python file and set your API key:</p> <pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Or use .env file\n</code></pre>"},{"location":"getting-started/quickstart/#2-prepare-sample-data","title":"2. Prepare Sample Data","text":"<p>Create a simple CSV file or use a pandas DataFrame:</p> <pre><code>import pandas as pd\n\n# Sample data\ndata = pd.DataFrame({\n    \"product\": [\n        \"iPhone 15 Pro Max 256GB\",\n        \"Samsung Galaxy S24 Ultra\",\n        \"Google Pixel 8 Pro\"\n    ]\n})\n\n# Save to CSV\ndata.to_csv(\"products.csv\", index=False)\n</code></pre>"},{"location":"getting-started/quickstart/#3-quick-api-simplest","title":"3. Quick API (Simplest)","text":"<p>The fastest way to process data:</p> <pre><code>from ondine import QuickPipeline\n\n# Create and run pipeline\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Extract the brand name from: {product}\",\n    model=\"gpt-4o-mini\"\n)\n\nresult = pipeline.execute()\n\n# View results\nprint(result.data)\nprint(f\"Cost: ${result.costs.total_cost:.4f}\")\n</code></pre> <p>Output: <pre><code>   product                      response\n0  iPhone 15 Pro Max 256GB       Apple\n1  Samsung Galaxy S24 Ultra      Samsung\n2  Google Pixel 8 Pro            Google\n\nCost: $0.0012\n</code></pre></p>"},{"location":"getting-started/quickstart/#4-builder-api-more-control","title":"4. Builder API (More Control)","text":"<p>For explicit configuration:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"product\"],\n        output_columns=[\"brand\"]\n    )\n    .with_prompt(\"Extract the brand name from: {product}\")\n    .with_llm(\n        provider=\"openai\",\n        model=\"gpt-4o-mini\",\n        temperature=0.0\n    )\n    .with_batch_size(100)\n    .with_concurrency(5)\n    .build()\n)\n\n# Estimate cost before running\nestimate = pipeline.estimate_cost()\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\n\n# Execute\nresult = pipeline.execute()\nprint(result.data)\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#pattern-1-data-cleaning","title":"Pattern 1: Data Cleaning","text":"<pre><code>from ondine import QuickPipeline\n\npipeline = QuickPipeline.create(\n    data=\"messy_data.csv\",\n    prompt=\"\"\"\n    Clean and standardize this text:\n    {text}\n\n    Remove special characters, fix capitalization, trim whitespace.\n    \"\"\",\n    model=\"gpt-4o-mini\"\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-2-classification","title":"Pattern 2: Classification","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"reviews.csv\",\n        input_columns=[\"review_text\"],\n        output_columns=[\"sentiment\"]\n    )\n    .with_prompt(\"\"\"\n    Classify the sentiment of this review as: positive, negative, or neutral\n\n    Review: {review_text}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-3-structured-extraction","title":"Pattern 3: Structured Extraction","text":"<pre><code>from ondine import PipelineBuilder\nfrom ondine.stages.parser_factory import JSONParser\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"descriptions.csv\",\n        input_columns=[\"description\"],\n        output_columns=[\"brand\", \"model\", \"price\"]\n    )\n    .with_prompt(\"\"\"\n    Extract product information as JSON:\n    {{\n      \"brand\": \"...\",\n      \"model\": \"...\",\n      \"price\": \"...\"\n    }}\n\n    Description: {description}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(JSONParser())\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-results","title":"Understanding the Results","text":"<p>The <code>execute()</code> method returns an <code>ExecutionResult</code> object:</p> <pre><code>result = pipeline.execute()\n\n# Access the processed data\nprint(result.data)              # pandas DataFrame with results\n\n# View metrics\nprint(result.metrics.processed_rows)    # Number of rows processed\nprint(result.metrics.successful_rows)   # Successfully processed\nprint(result.metrics.failed_rows)       # Failed rows\nprint(result.metrics.elapsed_time)      # Total time in seconds\n\n# Check costs\nprint(result.costs.total_cost)          # Total cost in USD\nprint(result.costs.input_tokens)        # Input tokens used\nprint(result.costs.output_tokens)       # Output tokens generated\n</code></pre>"},{"location":"getting-started/quickstart/#cost-estimation","title":"Cost Estimation","text":"<p>Always estimate costs before processing large datasets:</p> <pre><code>pipeline = PipelineBuilder.create()...build()\n\n# Get cost estimate\nestimate = pipeline.estimate_cost()\n\nprint(f\"Estimated rows: {estimate.total_rows}\")\nprint(f\"Estimated tokens: {estimate.estimated_tokens}\")\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\n\n# Proceed only if acceptable\nif estimate.total_cost &lt; 10.0:\n    result = pipeline.execute()\nelse:\n    print(\"Cost too high, aborting\")\n</code></pre>"},{"location":"getting-started/quickstart/#error-handling","title":"Error Handling","text":"<p>Ondine automatically retries failed requests:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_retry_policy(max_retries=3, backoff_factor=2.0)\n    .build()\n)\n\nresult = pipeline.execute()\n\n# Check for failures\nif result.metrics.failed_rows &gt; 0:\n    print(f\"Failed rows: {result.metrics.failed_rows}\")\n    print(result.data[result.data['response'].isna()])\n</code></pre>"},{"location":"getting-started/quickstart/#checkpointing","title":"Checkpointing","text":"<p>For long-running jobs, enable checkpointing to resume on crashes:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"large_dataset.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_checkpoint(\"./checkpoints\", interval=100)  # Save every 100 rows\n    .build()\n)\n\n# If interrupted, re-run same command - it will resume from checkpoint\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have a working pipeline, explore:</p> <ul> <li>Core Concepts - Understand the architecture</li> <li>Execution Modes - Async and streaming execution</li> <li>Structured Output - Type-safe Pydantic models</li> <li>Cost Control - Budget limits and optimization</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"guides/azure-managed-identity/","title":"Azure Managed Identity Authentication","text":"<p>Complete guide for using Azure Managed Identity with Ondine for keyless, secure authentication.</p>"},{"location":"guides/azure-managed-identity/#overview","title":"Overview","text":"<p>Azure Managed Identity provides keyless authentication to Azure OpenAI Service, eliminating the need to store API keys in your code or environment variables. This is the recommended approach for production deployments on Azure.</p>"},{"location":"guides/azure-managed-identity/#benefits","title":"Benefits","text":"<ul> <li>\ud83d\udd12 No secrets in code - Zero API keys to manage</li> <li>\ud83d\udd04 Automatic rotation - Azure AD handles credential lifecycle</li> <li>\ud83c\udfaf Fine-grained access - Use Azure RBAC for permissions</li> <li>\ud83d\udcca Audit trail - All access logged through Azure AD</li> <li>\ud83d\ude80 Works everywhere - Production, staging, and local development</li> </ul>"},{"location":"guides/azure-managed-identity/#quick-start","title":"Quick Start","text":""},{"location":"guides/azure-managed-identity/#1-install-azure-dependencies","title":"1. Install Azure Dependencies","text":"<pre><code>pip install ondine[azure]\n</code></pre> <p>This installs <code>azure-identity&gt;=1.15.0</code> for Managed Identity support.</p>"},{"location":"guides/azure-managed-identity/#2-configure-azure-resources","title":"2. Configure Azure Resources","text":"<pre><code># Assign Managed Identity to your Azure resource\naz vm identity assign --name my-vm --resource-group my-rg\n\n# Grant \"Cognitive Services OpenAI User\" role\naz role assignment create \\\n  --assignee &lt;managed-identity-principal-id&gt; \\\n  --role \"Cognitive Services OpenAI User\" \\\n  --scope /subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.CognitiveServices/accounts/&lt;openai-resource&gt;\n</code></pre>"},{"location":"guides/azure-managed-identity/#3-use-in-your-code","title":"3. Use in Your Code","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=\"https://your-resource.openai.azure.com/\",\n        azure_deployment=\"gpt-4-deployment\",\n        use_managed_identity=True  # \u2190 No API key needed!\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/azure-managed-identity/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/azure-managed-identity/#programmatic-api-python","title":"Programmatic API (Python)","text":""},{"location":"guides/azure-managed-identity/#example-1-basic-usage","title":"Example 1: Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\nimport pandas as pd\n\n# Sample data\ndata = pd.DataFrame({\n    \"product_description\": [\n        \"Wireless Bluetooth headphones with noise cancellation\",\n        \"Stainless steel water bottle, 32oz capacity\",\n    ]\n})\n\n# Build pipeline with Managed Identity\npipeline = (\n    PipelineBuilder.create()\n    .from_dataframe(\n        data,\n        input_columns=[\"product_description\"],\n        output_columns=[\"category\", \"features\"]\n    )\n    .with_prompt(\"\"\"\nAnalyze: {product_description}\n\nProvide:\n1. category: Product category\n2. features: Key features (comma-separated)\n\nFormat as JSON: {{\"category\": \"...\", \"features\": \"...\"}}\n\"\"\")\n    .with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=\"https://your-resource.openai.azure.com/\",\n        azure_deployment=\"gpt-4-deployment\",\n        use_managed_identity=True,\n        temperature=0.7,\n    )\n    .with_batch_size(10)\n    .with_concurrency(5)\n    .build()\n)\n\nresult = pipeline.execute()\nprint(f\"Processed: {result.metrics.total_rows} rows\")\nprint(f\"Cost: ${result.costs.total_cost:.4f}\")\n</code></pre>"},{"location":"guides/azure-managed-identity/#example-2-environment-aware-configuration","title":"Example 2: Environment-Aware Configuration","text":"<pre><code>import os\nfrom ondine import PipelineBuilder\n\n# Detect if running on Azure\nis_azure = os.getenv(\"WEBSITE_INSTANCE_ID\") or os.getenv(\"MSI_ENDPOINT\")\n\nbuilder = PipelineBuilder.create().from_csv(\"data.csv\", ...)\n\nif is_azure:\n    # Production: Use Managed Identity\n    builder.with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=\"https://your-resource.openai.azure.com/\",\n        azure_deployment=\"gpt-4-deployment\",\n        use_managed_identity=True\n    )\nelse:\n    # Development: Use API Key\n    builder.with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=\"https://your-resource.openai.azure.com/\",\n        azure_deployment=\"gpt-4-deployment\"\n        # Falls back to AZURE_OPENAI_API_KEY env var\n    )\n\npipeline = builder.build()\n</code></pre>"},{"location":"guides/azure-managed-identity/#cli-with-yaml-configuration","title":"CLI with YAML Configuration","text":""},{"location":"guides/azure-managed-identity/#example-1-managed-identity-config","title":"Example 1: Managed Identity Config","text":"<p>Create <code>azure_managed_identity.yaml</code>:</p> <pre><code>name: \"product_enrichment_pipeline\"\nversion: \"1.0\"\n\ndataset:\n  source_type: \"csv\"\n  source_path: \"products.csv\"\n  input_columns:\n    - \"product_description\"\n  output_columns:\n    - \"category\"\n    - \"key_features\"\n\nprompt:\n  template: \"Analyze this product: {product_description}\\n\\nProvide category and key features as JSON.\"\n  response_format: \"json\"\n\nllm:\n  provider: \"azure_openai\"\n  model: \"gpt-4\"\n  azure_endpoint: \"https://your-resource.openai.azure.com/\"\n  azure_deployment: \"gpt-4-deployment\"\n  use_managed_identity: true  # \u2190 Keyless authentication!\n  temperature: 0.7\n\nprocessing:\n  batch_size: 50\n  concurrency: 10\n\noutput:\n  destination_type: \"csv\"\n  destination_path: \"enriched_products.csv\"\n</code></pre> <p>Run with CLI:</p> <pre><code>ondine process --config azure_managed_identity.yaml\n</code></pre>"},{"location":"guides/azure-managed-identity/#example-2-api-key-config-traditional","title":"Example 2: API Key Config (Traditional)","text":"<p>Create <code>azure_api_key.yaml</code>:</p> <pre><code>name: \"support_ticket_classification\"\nversion: \"1.0\"\n\ndataset:\n  source_type: \"csv\"\n  source_path: \"tickets.csv\"\n  input_columns:\n    - \"ticket_description\"\n  output_columns:\n    - \"category\"\n\nprompt:\n  template: \"Classify: {ticket_description}\"\n\nllm:\n  provider: \"azure_openai\"\n  model: \"gpt-4\"\n  azure_endpoint: \"https://your-resource.openai.azure.com/\"\n  azure_deployment: \"gpt-4-deployment\"\n  # No use_managed_identity \u2192 falls back to AZURE_OPENAI_API_KEY\n\nprocessing:\n  batch_size: 100\n\noutput:\n  destination_type: \"csv\"\n  destination_path: \"classified_tickets.csv\"\n</code></pre> <p>Run with CLI:</p> <pre><code>export AZURE_OPENAI_API_KEY=\"your-key-here\"  # pragma: allowlist secret\nondine process --config azure_api_key.yaml\n</code></pre>"},{"location":"guides/azure-managed-identity/#environment-setup","title":"Environment Setup","text":""},{"location":"guides/azure-managed-identity/#local-development","title":"Local Development","text":"<pre><code># 1. Install dependencies\npip install ondine[azure]\n\n# 2. Login with Azure CLI\naz login\n\n# 3. Run your pipeline\npython your_script.py\n# or\nondine process --config your_config.yaml\n</code></pre> <p>What happens: <code>DefaultAzureCredential</code> uses your Azure CLI credentials automatically.</p>"},{"location":"guides/azure-managed-identity/#azure-vm-container-apps","title":"Azure VM / Container Apps","text":"<pre><code># 1. Assign Managed Identity (one-time setup)\naz vm identity assign --name my-vm --resource-group my-rg\n\n# 2. Grant RBAC role (one-time setup)\naz role assignment create \\\n  --assignee &lt;managed-identity-principal-id&gt; \\\n  --role \"Cognitive Services OpenAI User\" \\\n  --scope &lt;openai-resource-id&gt;\n\n# 3. Deploy and run your application\n# No az login needed - Managed Identity works automatically!\n</code></pre>"},{"location":"guides/azure-managed-identity/#azure-container-apps","title":"Azure Container Apps","text":"<pre><code># Enable Managed Identity\naz containerapp identity assign \\\n  --name my-app \\\n  --resource-group my-rg \\\n  --system-assigned\n\n# Grant RBAC role\naz role assignment create \\\n  --assignee &lt;managed-identity-principal-id&gt; \\\n  --role \"Cognitive Services OpenAI User\" \\\n  --scope &lt;openai-resource-id&gt;\n</code></pre>"},{"location":"guides/azure-managed-identity/#azure-functions","title":"Azure Functions","text":"<pre><code># Enable Managed Identity\naz functionapp identity assign \\\n  --name my-function \\\n  --resource-group my-rg\n\n# Grant RBAC role\naz role assignment create \\\n  --assignee &lt;managed-identity-principal-id&gt; \\\n  --role \"Cognitive Services OpenAI User\" \\\n  --scope &lt;openai-resource-id&gt;\n</code></pre>"},{"location":"guides/azure-managed-identity/#multi-region-deployment","title":"Multi-Region Deployment","text":"<p>For global deployments, use region-specific endpoints:</p> <pre><code>import os\n\n# Define region-specific endpoints\nAZURE_REGIONS = {\n    \"eastus\": \"https://eastus-openai.openai.azure.com/\",\n    \"westeurope\": \"https://westeurope-openai.openai.azure.com/\",\n    \"swedencentral\": \"https://swedencentral-openai.openai.azure.com/\",\n    \"japaneast\": \"https://japaneast-openai.openai.azure.com/\",\n}\n\n# Get region from environment\nregion = os.getenv(\"AZURE_REGION\", \"eastus\")\nendpoint = AZURE_REGIONS[region]\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=endpoint,  # Region-specific\n        azure_deployment=\"gpt-4-deployment\",\n        use_managed_identity=True\n    )\n    .build()\n)\n</code></pre>"},{"location":"guides/azure-managed-identity/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/azure-managed-identity/#error-azure-identity-not-installed","title":"Error: \"azure-identity not installed\"","text":"<pre><code>ImportError: Azure Managed Identity requires azure-identity.\nInstall with: pip install ondine[azure]\n</code></pre> <p>Solution: <pre><code>pip install ondine[azure]\n</code></pre></p>"},{"location":"guides/azure-managed-identity/#error-failed-to-authenticate-with-azure-managed-identity","title":"Error: \"Failed to authenticate with Azure Managed Identity\"","text":"<pre><code>ValueError: Failed to authenticate with Azure Managed Identity: &lt;error&gt;.\nEnsure the resource has a Managed Identity assigned with\n'Cognitive Services OpenAI User' role.\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Check Managed Identity is assigned: <pre><code>az vm identity show --name my-vm --resource-group my-rg\n</code></pre></p> </li> <li> <p>Check RBAC role assignment: <pre><code>az role assignment list \\\n  --assignee &lt;managed-identity-principal-id&gt; \\\n  --scope &lt;openai-resource-id&gt;\n</code></pre></p> </li> <li> <p>For local development, ensure you're logged in: <pre><code>az login\naz account show  # Verify you're logged in\n</code></pre></p> </li> </ol>"},{"location":"guides/azure-managed-identity/#error-no-authentication-provided","title":"Error: \"No authentication provided\"","text":"<pre><code>ValueError: Azure OpenAI requires either:\n  1. use_managed_identity=True (for keyless auth), or\n  2. api_key parameter, or\n  3. AZURE_OPENAI_API_KEY environment variable\n</code></pre> <p>Solution: Choose one authentication method:</p> <pre><code># Option 1: Managed Identity\n.with_llm(..., use_managed_identity=True)\n\n# Option 2: API Key  # pragma: allowlist secret\n.with_llm(..., api_key=\"your-key\")  # pragma: allowlist secret\n\n# Option 3: Environment variable\nexport AZURE_OPENAI_API_KEY=\"your-key\"  # pragma: allowlist secret\n.with_llm(...)  # Reads from env var\n</code></pre>"},{"location":"guides/azure-managed-identity/#testing","title":"Testing","text":""},{"location":"guides/azure-managed-identity/#unit-tests-no-azure-required","title":"Unit Tests (No Azure Required)","text":"<pre><code># Run unit tests with mocks\nuv run pytest tests/unit/test_azure_managed_identity.py -v\n</code></pre> <p>All tests use mocks - no Azure credentials needed!</p>"},{"location":"guides/azure-managed-identity/#integration-tests-azure-required","title":"Integration Tests (Azure Required)","text":"<pre><code># Setup Azure resources first\naz login\n\n# Run integration tests\nuv run pytest tests/integration/ -k azure\n</code></pre>"},{"location":"guides/azure-managed-identity/#manual-testing","title":"Manual Testing","text":"<pre><code># 1. Login with Azure CLI\naz login\n\n# 2. Run example script\npython examples/19_azure_managed_identity_complete.py\n\n# 3. Or use CLI with YAML config\nondine process --config examples/azure_managed_identity_config.yaml\n</code></pre>"},{"location":"guides/azure-managed-identity/#security-best-practices","title":"Security Best Practices","text":""},{"location":"guides/azure-managed-identity/#dos","title":"\u2705 Do's","text":"<ol> <li>Use Managed Identity in production - No secrets to manage</li> <li>Use <code>az login</code> for local development - Personal credentials</li> <li>Grant minimum required RBAC roles - Principle of least privilege</li> <li>Use separate identities per environment - Dev, staging, prod</li> <li>Monitor Azure AD logs - Track authentication events</li> </ol>"},{"location":"guides/azure-managed-identity/#donts","title":"\u274c Don'ts","text":"<ol> <li>Don't hardcode API keys - Use Managed Identity instead</li> <li>Don't commit credentials - Never in source control</li> <li>Don't use API keys in production - Use Managed Identity</li> <li>Don't share Managed Identities - One per application</li> <li>Don't skip RBAC - Always use role-based access control</li> </ol>"},{"location":"guides/azure-managed-identity/#comparison-api-key-vs-managed-identity","title":"Comparison: API Key vs. Managed Identity","text":"Aspect API Key Managed Identity Security \u26a0\ufe0f Secret to manage \u2705 No secrets Rotation \u274c Manual \u2705 Automatic Setup \u2705 Simple \u26a0\ufe0f Requires Azure setup Local Dev \u2705 Easy (env var) \u2705 Easy (<code>az login</code>) Production \u274c Risk of exposure \u2705 Secure by design Audit Trail \u26a0\ufe0f Limited \u2705 Full Azure AD logs RBAC \u274c No \u2705 Yes Cost Free Free <p>Recommendation: Use Managed Identity for production, API key for quick prototyping.</p>"},{"location":"guides/azure-managed-identity/#examples","title":"Examples","text":""},{"location":"guides/azure-managed-identity/#complete-python-example","title":"Complete Python Example","text":"<p>See: <code>examples/19_azure_managed_identity_complete.py</code></p> <p>Includes: - Managed Identity authentication - API key authentication - Pre-fetched token - Environment-aware configuration - Multi-region setup</p>"},{"location":"guides/azure-managed-identity/#yaml-configuration-examples","title":"YAML Configuration Examples","text":"<ul> <li>Managed Identity: <code>examples/azure_managed_identity_config.yaml</code></li> <li>API Key: <code>examples/azure_api_key_config.yaml</code></li> </ul>"},{"location":"guides/azure-managed-identity/#related-documentation","title":"Related Documentation","text":"<ul> <li>Azure OpenAI Provider Guide</li> <li>Installation Guide</li> <li>Technical Reference</li> </ul>"},{"location":"guides/azure-managed-identity/#faq","title":"FAQ","text":""},{"location":"guides/azure-managed-identity/#q-do-i-need-az-login-in-production","title":"Q: Do I need <code>az login</code> in production?","text":"<p>A: No! Managed Identity works automatically on Azure infrastructure. <code>az login</code> is only for local development.</p>"},{"location":"guides/azure-managed-identity/#q-can-i-use-managed-identity-with-openai-not-azure-openai","title":"Q: Can I use Managed Identity with OpenAI (not Azure OpenAI)?","text":"<p>A: No. Managed Identity only works with Azure OpenAI Service. For OpenAI's public API, use API keys.</p>"},{"location":"guides/azure-managed-identity/#q-how-long-do-tokens-last","title":"Q: How long do tokens last?","text":"<p>A: Azure AD tokens typically last 1 hour. For pipelines running longer than 1 hour, consider using API keys or implementing token refresh (future feature).</p>"},{"location":"guides/azure-managed-identity/#q-can-i-use-user-assigned-managed-identity","title":"Q: Can I use User-Assigned Managed Identity?","text":"<p>A: Yes! <code>DefaultAzureCredential</code> supports both System-Assigned and User-Assigned Managed Identities. Set the <code>AZURE_CLIENT_ID</code> environment variable to specify a User-Assigned identity.</p>"},{"location":"guides/azure-managed-identity/#q-does-this-work-with-azure-government-or-azure-china","title":"Q: Does this work with Azure Government or Azure China?","text":"<p>A: Yes, but you may need to specify custom token scopes. This is an advanced scenario - contact support for guidance.</p>"},{"location":"guides/azure-managed-identity/#support","title":"Support","text":"<p>For issues or questions: - GitHub Issues: https://github.com/ptimizeroracle/Ondine/issues - Documentation: https://ptimizeroracle.github.io/ondine</p>"},{"location":"guides/batch-processing/","title":"Multi-Row Batching Guide","text":"<p>Process N rows in a single API call to achieve 100\u00d7 speedup for large datasets.</p>"},{"location":"guides/batch-processing/#overview","title":"Overview","text":"<p>Multi-row batching aggregates multiple prompts into a single API call, dramatically reducing: - API calls: 100\u00d7 fewer (5M \u2192 50K with batch_size=100) - Processing time: 100\u00d7 faster (69 hours \u2192 42 minutes) - Rate limit issues: Virtually eliminated</p>"},{"location":"guides/batch-processing/#quick-start","title":"Quick Start","text":""},{"location":"guides/batch-processing/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\n# Enable multi-row batching with one line\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"sentiment\"])\n    .with_prompt(\"Classify sentiment: {text}\")\n    .with_batch_size(100)  # Process 100 rows per API call!\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/batch-processing/#how-it-works","title":"How It Works","text":"<p>Without Batching (batch_size=1, default): <pre><code>Row 1: \"Product A is great\" \u2192 API call 1 \u2192 \"positive\"\nRow 2: \"Product B is bad\" \u2192 API call 2 \u2192 \"negative\"\nRow 3: \"Product C is okay\" \u2192 API call 3 \u2192 \"neutral\"\n...\n100 rows = 100 API calls\n</code></pre></p> <p>With Batching (batch_size=100): <pre><code>Batch prompt:\n[\n  {\"id\": 1, \"input\": \"Product A is great\"},\n  {\"id\": 2, \"input\": \"Product B is bad\"},\n  ...\n  {\"id\": 100, \"input\": \"Product Z\"}\n]\n\u2193 1 API call \u2193\nBatch response:\n[\n  {\"id\": 1, \"result\": \"positive\"},\n  {\"id\": 2, \"result\": \"negative\"},\n  ...\n  {\"id\": 100, \"result\": \"neutral\"}\n]\n\n100 rows = 1 API call (100\u00d7 reduction!)\n</code></pre></p>"},{"location":"guides/batch-processing/#choosing-batch-size","title":"Choosing Batch Size","text":""},{"location":"guides/batch-processing/#recommended-batch-sizes","title":"Recommended Batch Sizes","text":"Use Case Batch Size Reason Simple classification 100-500 Short prompts, low failure risk Sentiment analysis 50-100 Medium complexity Text summarization 10-50 Longer outputs, higher risk Complex extraction 10-20 Complex prompts, careful parsing"},{"location":"guides/batch-processing/#factors-to-consider","title":"Factors to Consider","text":"<p>1. Model Context Window - GPT-4o/GPT-4o-mini: 128K tokens \u2192 batch_size up to 500 - Claude Sonnet: 200K tokens \u2192 batch_size up to 800 - Llama 3.1: 131K tokens \u2192 batch_size up to 500</p> <p>Ondine automatically validates batch size against context limits.</p> <p>2. Prompt Complexity - Simple prompts (20 tokens): Larger batches (100-500) - Medium prompts (100 tokens): Medium batches (50-100) - Complex prompts (500+ tokens): Smaller batches (10-50)</p> <p>3. Failure Tolerance - Larger batches = higher risk of partial failures - Start with batch_size=10, increase gradually - Monitor partial failure rate</p>"},{"location":"guides/batch-processing/#batch-strategies","title":"Batch Strategies","text":""},{"location":"guides/batch-processing/#json-strategy-default","title":"JSON Strategy (Default)","text":"<p>Formats batches as JSON arrays:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .with_batch_size(100)\n    .with_batch_strategy(\"json\")  # Default, most reliable\n    .build()\n)\n</code></pre> <p>Pros: - Most reliable (Pydantic validation) - Handles complex outputs - Automatic error detection</p> <p>Cons: - ~200 token overhead per batch - Requires LLM to follow JSON format</p>"},{"location":"guides/batch-processing/#csv-strategy-future","title":"CSV Strategy (Future)","text":"<p>Coming soon - more compact format for simple use cases.</p>"},{"location":"guides/batch-processing/#error-handling","title":"Error Handling","text":""},{"location":"guides/batch-processing/#partial-failures","title":"Partial Failures","text":"<p>If some rows fail to parse, Ondine handles gracefully:</p> <pre><code># Batch with 100 rows\n# LLM returns 97 results (missing IDs: 23, 67, 89)\n\n# Ondine automatically:\n# 1. Parses 97 successful results\n# 2. Marks 3 failed rows with [PARSE_ERROR]\n# 3. Continues processing\n\n# Result:\n# - 97 rows: Valid results\n# - 3 rows: \"[PARSE_ERROR: Row not found in batch response]\"\n</code></pre>"},{"location":"guides/batch-processing/#complete-failures","title":"Complete Failures","text":"<p>If entire batch fails to parse:</p> <pre><code># Batch response: \"I cannot provide results in JSON format\"\n\n# Ondine automatically:\n# 1. Marks all rows with [BATCH_PARSE_ERROR]\n# 2. Logs error for debugging\n# 3. Continues with next batch\n\n# Result:\n# - All rows: \"[BATCH_PARSE_ERROR: Invalid JSON]\"\n</code></pre>"},{"location":"guides/batch-processing/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"guides/batch-processing/#scaling-to-5m-rows","title":"Scaling to 5M Rows","text":"Batch Size API Calls Time Speedup Cost Overhead 1 (default) 5,000,000 ~69 hours 1\u00d7 0% 10 500,000 ~7 hours 10\u00d7 ~2% 100 50,000 ~42 minutes 100\u00d7 ~5% 500 10,000 ~8 minutes 500\u00d7 ~10% <p>Cost overhead: JSON formatting adds ~200 tokens per batch</p>"},{"location":"guides/batch-processing/#real-world-example","title":"Real-World Example","text":"<p>Dataset: 10 rows, sentiment classification</p> <p>Without batching: - API calls: 10 - Duration: ~15 seconds - Tokens: 210 (21 per row)</p> <p>With batching (batch_size=5): - API calls: 2 (5\u00d7 reduction!) - Duration: ~6 seconds (2.5\u00d7 faster) - Tokens: 250 (25 per row, ~20% overhead)</p>"},{"location":"guides/batch-processing/#combining-with-prefix-caching","title":"Combining with Prefix Caching","text":"<p>For maximum cost savings, combine both techniques:</p> <pre><code># Shared context (cached across all rows)\nSHARED_CONTEXT = \"\"\"You are an expert data analyst.\n[1024+ tokens of general knowledge]\n\"\"\"\n\npipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"TASK: Classify\\\\nINPUT: {text}\")\n    .with_system_prompt(SHARED_CONTEXT)  # Cached (40-50% savings)\n    .with_batch_size(100)  # 100\u00d7 fewer API calls\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> <p>Combined savings: - Prefix caching: 40-50% cost reduction - Multi-row batching: 100\u00d7 fewer API calls - Total: 90%+ cost reduction + 100\u00d7 speedup!</p>"},{"location":"guides/batch-processing/#cli-configuration","title":"CLI Configuration","text":"<p>Enable batching via YAML config:</p> <pre><code>prompt:\n  template: \"Classify: {text}\"\n  batch_size: 100  # Multi-row batching\n  batch_strategy: json\n  system_message: \"You are a classifier.\"  # Cached\n\nllm:\n  provider: openai\n  model: gpt-4o-mini\n</code></pre> <p>Run with: <pre><code>ondine process --config config.yaml\n</code></pre></p>"},{"location":"guides/batch-processing/#best-practices","title":"Best Practices","text":"<ol> <li>Start small: Begin with batch_size=10, increase gradually</li> <li>Monitor failures: Check for <code>[PARSE_ERROR]</code> in results</li> <li>Validate context: Ondine auto-validates, but check logs for warnings</li> <li>Combine techniques: Use with prefix caching for maximum savings</li> <li>Test first: Run on 100-1000 rows before scaling to millions</li> </ol>"},{"location":"guides/batch-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/batch-processing/#issue-batch-parsing-failures","title":"Issue: Batch parsing failures","text":"<p>Symptom: Many rows with <code>[PARSE_ERROR]</code></p> <p>Solutions: - Reduce batch_size (try 10-20) - Simplify prompt instructions - Add more explicit JSON format examples - Check LLM is following instructions</p>"},{"location":"guides/batch-processing/#issue-context-window-exceeded","title":"Issue: Context window exceeded","text":"<p>Symptom: Warning logs about batch size validation</p> <p>Solutions: - Reduce batch_size - Use model with larger context window - Simplify prompts to reduce tokens</p>"},{"location":"guides/batch-processing/#issue-slower-than-expected","title":"Issue: Slower than expected","text":"<p>Symptom: Not seeing 100\u00d7 speedup</p> <p>Solutions: - Check batch_size is actually set (print <code>pipeline.specifications.prompt.batch_size</code>) - Verify batch aggregation logs appear - Check if rate limiting is bottleneck (increase rate_limit_rpm)</p>"},{"location":"guides/batch-processing/#examples","title":"Examples","text":"<p>See <code>examples/21_multi_row_batching.py</code> for complete working examples: - Example 1: Without batching (baseline) - Example 2: With batching (5\u00d7 speedup) - Example 3: Large dataset extrapolation (5.4M rows)</p>"},{"location":"guides/batch-processing/#api-reference","title":"API Reference","text":""},{"location":"guides/batch-processing/#with_batch_sizebatch_size-int","title":"<code>with_batch_size(batch_size: int)</code>","text":"<p>Enable multi-row batching.</p> <p>Parameters: - <code>batch_size</code> (int): Number of rows to process per API call (1-500)</p> <p>Returns: PipelineBuilder (for chaining)</p> <p>Raises: ValueError if batch_size &lt; 1 or called before with_prompt()</p>"},{"location":"guides/batch-processing/#with_batch_strategystrategy-str","title":"<code>with_batch_strategy(strategy: str)</code>","text":"<p>Set batch formatting strategy.</p> <p>Parameters: - <code>strategy</code> (str): \"json\" or \"csv\"</p> <p>Returns: PipelineBuilder (for chaining)</p> <p>Raises: ValueError if strategy not supported</p>"},{"location":"guides/batch-processing/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Combine with concurrency: Use both for maximum throughput    <pre><code>.with_batch_size(100)  # 100 rows per call\n.with_concurrency(10)  # 10 concurrent calls\n# = 1000 rows processed simultaneously!\n</code></pre></p> </li> <li> <p>Use with streaming: Process results as they arrive    <pre><code>.with_batch_size(100)\n.with_streaming(chunk_size=10000)\n</code></pre></p> </li> <li> <p>Enable prefix caching: Reduce token costs    <pre><code>.with_batch_size(100)\n.with_system_prompt(\"...\")  # Cached\n</code></pre></p> </li> </ol>"},{"location":"guides/batch-processing/#limitations","title":"Limitations","text":"<ul> <li>JSON strategy only (CSV coming soon)</li> <li>Batch size limited by model context window</li> <li>Requires LLM to follow JSON format instructions</li> <li>Larger batches = higher risk of partial failures</li> </ul>"},{"location":"guides/cost-control/","title":"Cost Control","text":"<p>Ondine provides comprehensive cost management features to prevent budget overruns and optimize spending on LLM APIs.</p>"},{"location":"guides/cost-control/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"guides/cost-control/#1-multi-row-batching-100-speedup-new","title":"1. Multi-Row Batching (100\u00d7 Speedup) - NEW!","text":"<p>Process N rows in a single API call to reduce API calls by 100\u00d7:</p> <pre><code># Traditional: 5M rows = 5M API calls (~69 hours)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Classify: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# With batching: 5M rows = 50K API calls (~42 minutes, 100\u00d7 faster!)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"])\n    .with_prompt(\"Classify: {text}\")\n    .with_batch_size(100)  # Process 100 rows per API call!\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> <p>Benefits: - 100\u00d7 fewer API calls - 100\u00d7 faster processing - Same token cost, eliminates API overhead - Automatic context window validation</p> <p>Recommended batch sizes: - Simple prompts: 50-500 rows/batch - Complex prompts: 10-50 rows/batch - Start with 10, increase based on results</p> <p>See <code>examples/21_multi_row_batching.py</code> for complete examples.</p>"},{"location":"guides/cost-control/#2-prefix-caching-40-50-cost-reduction-new","title":"2. Prefix Caching (40-50% Cost Reduction) - NEW!","text":"<p>Cache static system prompts to reduce costs by 40-50%:</p> <pre><code># Without caching: Pay full price for system prompt every row\npipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"You are a classifier. Classify: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# With caching: System prompt cached, only pay for dynamic content\npipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"Classify: {text}\")  # Dynamic part\n    .with_system_prompt(\"You are a classifier.\")  # Cached!\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre> <p>Benefits: - 40-50% cost reduction on cached tokens - 80-85% latency reduction - Automatic (OpenAI, Anthropic)</p> <p>Combine Both for Maximum Savings: <pre><code># Prefix caching + Multi-row batching = 90%+ cost reduction!\npipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"Classify: {text}\")\n    .with_system_prompt(\"You are a classifier.\")  # Cached\n    .with_batch_size(100)  # 100\u00d7 fewer API calls\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre></p>"},{"location":"guides/cost-control/#pre-execution-cost-estimation","title":"Pre-Execution Cost Estimation","text":"<p>Always estimate costs before processing large datasets:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"summary\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Get cost estimate\nestimate = pipeline.estimate_cost()\n\nprint(f\"Total rows: {estimate.total_rows}\")\nprint(f\"Estimated tokens: {estimate.estimated_tokens}\")\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\nprint(f\"Cost per row: ${estimate.cost_per_row:.6f}\")\n</code></pre>"},{"location":"guides/cost-control/#budget-limits","title":"Budget Limits","text":"<p>Set maximum budget to prevent overspending:</p> <pre><code>from decimal import Decimal\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_max_budget(Decimal(\"10.0\"))  # Max $10 USD\n    .build()\n)\n\n# Execution stops if budget exceeded\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/cost-control/#real-time-cost-tracking","title":"Real-Time Cost Tracking","text":"<p>Monitor costs during execution:</p> <pre><code>result = pipeline.execute()\n\n# View detailed costs\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\nprint(f\"Input tokens: {result.costs.input_tokens:,}\")\nprint(f\"Output tokens: {result.costs.output_tokens:,}\")\nprint(f\"Total tokens: {result.costs.total_tokens:,}\")\nprint(f\"Cost per row: ${result.costs.total_cost / result.metrics.processed_rows:.6f}\")\n</code></pre>"},{"location":"guides/cost-control/#cost-optimization-strategies_1","title":"Cost Optimization Strategies","text":""},{"location":"guides/cost-control/#1-use-prefix-caching-50-90-cost-reduction","title":"1. Use Prefix Caching (50-90% Cost Reduction) \u2b50","text":"<p>Prefix caching is the most effective cost optimization technique, reducing costs by 50-90% by caching static system prompts.</p> <p>OpenAI and Anthropic automatically cache system messages and reuse them across requests, charging only for dynamic content after the first request.</p>"},{"location":"guides/cost-control/#how-it-works","title":"How It Works","text":"<p>Separate your prompt into two parts: - System prompt (static, cached): Instructions, context, examples - User prompt (dynamic, per-row): The actual data to process</p> <pre><code># \u274c WITHOUT caching (old approach)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"reviews.csv\", input_columns=[\"text\"], output_columns=[\"sentiment\"])\n    .with_prompt(\"\"\"You are a sentiment classifier.\nClassify as: positive, negative, or neutral.\nReturn only the label.\n\nReview: {text}\nSentiment:\"\"\")  # System message embedded \u2192 sent every time \u2192 full cost\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# \u2705 WITH caching (new approach)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"reviews.csv\", input_columns=[\"text\"], output_columns=[\"sentiment\"])\n    .with_prompt(\"Review: {text}\\nSentiment:\")  # Only dynamic content\n    .with_system_prompt(\"\"\"You are a sentiment classifier.\nClassify as: positive, negative, or neutral.\nReturn only the label.\"\"\")  # Cached by provider!\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n</code></pre>"},{"location":"guides/cost-control/#cost-comparison","title":"Cost Comparison","text":"<p>For 5,000 rows with a 500-token system prompt:</p> Approach Tokens Cost (GPT-4o-mini) Savings Without caching 2.75M tokens $0.41 - With caching 250K tokens $0.04 90%"},{"location":"guides/cost-control/#provider-support","title":"Provider Support","text":"Provider Caching Support Cost Reduction Latency Reduction OpenAI \u2705 Automatic 50% on cached tokens ~50% Anthropic \u2705 Automatic 90% on cached tokens 85% Groq \u274c Not supported - - Azure OpenAI \u2705 Automatic 50% on cached tokens ~50%"},{"location":"guides/cost-control/#best-practices","title":"Best Practices","text":"<ol> <li>Keep system prompts static - No per-row variables</li> <li>Put all dynamic content in user prompt - Use template variables</li> <li>Use consistent system prompts - Caching works across requests</li> <li>Monitor token usage - Verify caching is working</li> </ol>"},{"location":"guides/cost-control/#alternative-syntax","title":"Alternative Syntax","text":"<p>You can also set the system message in <code>with_prompt()</code>:</p> <pre><code>.with_prompt(\n    template=\"Review: {text}\\nSentiment:\",\n    system_message=\"You are a sentiment classifier...\"\n)\n</code></pre> <p>Both approaches work identically - choose based on preference.</p>"},{"location":"guides/cost-control/#verification","title":"Verification","text":"<p>Check that caching is working by monitoring token counts:</p> <pre><code>result = pipeline.execute()\n\n# First few rows: ~550 tokens/row (system + user)\n# Remaining rows: ~50 tokens/row (user only, system cached)\navg_tokens = result.costs.total_tokens / result.metrics.processed_rows\nprint(f\"Average tokens/row: {avg_tokens:.0f}\")  # Should be ~50-100 with caching\n</code></pre> <p>See <code>examples/20_prefix_caching.py</code> for a complete working example.</p>"},{"location":"guides/cost-control/#2-choose-cost-effective-models","title":"2. Choose Cost-Effective Models","text":"<pre><code># Expensive: GPT-4\n.with_llm(provider=\"openai\", model=\"gpt-4\")  # ~$0.03/1K tokens\n\n# Cost-effective: GPT-4o-mini\n.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")  # ~$0.0001/1K tokens\n\n# Free: Local MLX (Apple Silicon)\n.with_llm(provider=\"mlx\", model=\"mlx-community/Qwen2.5-7B-Instruct-4bit\")  # $0\n</code></pre>"},{"location":"guides/cost-control/#2-optimize-prompts","title":"2. Optimize Prompts","text":"<p>Shorter prompts = lower costs:</p> <pre><code># Expensive: Verbose prompt\nprompt = \"\"\"\nYou are a helpful assistant specialized in text summarization.\nPlease carefully read the following text and provide a comprehensive\nsummary that captures the main points while being concise.\n\nText: {text}\n\nPlease provide your summary below:\n\"\"\"\n\n# Cost-effective: Concise prompt\nprompt = \"Summarize in 1 sentence: {text}\"\n</code></pre>"},{"location":"guides/cost-control/#3-use-temperature0-for-deterministic-tasks","title":"3. Use Temperature=0 for Deterministic Tasks","text":"<p>Lower temperature often produces shorter, more focused responses:</p> <pre><code>.with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n</code></pre>"},{"location":"guides/cost-control/#4-set-max-tokens","title":"4. Set Max Tokens","text":"<p>Limit response length:</p> <pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    max_tokens=100  # Limit response to 100 tokens\n)\n</code></pre>"},{"location":"guides/cost-control/#5-batch-processing","title":"5. Batch Processing","text":"<p>Process multiple items per request when possible:</p> <pre><code>.with_batch_size(100)  # Process 100 rows per batch\n</code></pre>"},{"location":"guides/cost-control/#6-use-cheaper-providers","title":"6. Use Cheaper Providers","text":"<p>Consider alternative providers for cost savings:</p> Provider Cost (per 1M tokens) Speed OpenAI GPT-4o-mini $0.15 Fast Groq (Llama) $0.05-0.10 Very Fast Together.AI $0.20-0.60 Fast Local MLX $0 Medium"},{"location":"guides/cost-control/#cost-reporting","title":"Cost Reporting","text":""},{"location":"guides/cost-control/#summary-report","title":"Summary Report","text":"<pre><code>result = pipeline.execute()\n\nprint(\"\\n=== Cost Summary ===\")\nprint(f\"Rows processed: {result.metrics.processed_rows}\")\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\nprint(f\"Average cost/row: ${result.costs.total_cost / result.metrics.processed_rows:.6f}\")\nprint(f\"Input tokens: {result.costs.input_tokens:,}\")\nprint(f\"Output tokens: {result.costs.output_tokens:,}\")\n</code></pre>"},{"location":"guides/cost-control/#export-to-csv","title":"Export to CSV","text":"<pre><code>import pandas as pd\n\n# Create cost report\ncost_report = pd.DataFrame([{\n    \"date\": pd.Timestamp.now(),\n    \"rows\": result.metrics.processed_rows,\n    \"total_cost\": result.costs.total_cost,\n    \"input_tokens\": result.costs.input_tokens,\n    \"output_tokens\": result.costs.output_tokens,\n    \"provider\": \"openai\",\n    \"model\": \"gpt-4o-mini\"\n}])\n\n# Append to running cost log\ncost_report.to_csv(\"cost_log.csv\", mode=\"a\", header=False, index=False)\n</code></pre>"},{"location":"guides/cost-control/#budget-aware-workflows","title":"Budget-Aware Workflows","text":""},{"location":"guides/cost-control/#estimate-before-execution","title":"Estimate Before Execution","text":"<pre><code>def safe_execute(pipeline, max_cost=10.0):\n    estimate = pipeline.estimate_cost()\n\n    if estimate.total_cost &gt; max_cost:\n        print(f\"Estimated cost ${estimate.total_cost:.2f} exceeds budget ${max_cost:.2f}\")\n        return None\n\n    print(f\"Proceeding with estimated cost: ${estimate.total_cost:.2f}\")\n    return pipeline.execute()\n\nresult = safe_execute(pipeline, max_cost=5.0)\n</code></pre>"},{"location":"guides/cost-control/#incremental-processing-with-cost-checks","title":"Incremental Processing with Cost Checks","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"large_data.csv\", ...)\n    .with_streaming(chunk_size=1000)\n    .with_max_budget(Decimal(\"20.0\"))\n    .build()\n)\n\ntotal_cost = 0.0\nfor chunk_result in pipeline.execute_stream():\n    total_cost += chunk_result.costs.total_cost\n    print(f\"Chunk cost: ${chunk_result.costs.total_cost:.4f}, \"\n          f\"Total so far: ${total_cost:.4f}\")\n\n    if total_cost &gt; 15.0:\n        print(\"Approaching budget limit, stopping\")\n        break\n</code></pre>"},{"location":"guides/cost-control/#cost-tracking-across-runs","title":"Cost Tracking Across Runs","text":"<p>Track costs across multiple pipeline runs:</p> <pre><code>from ondine.utils import CostTracker\n\ntracker = CostTracker()\n\n# Run multiple pipelines\nfor config in pipeline_configs:\n    pipeline = build_pipeline(config)\n    result = pipeline.execute()\n\n    tracker.add(\n        provider=config[\"provider\"],\n        model=config[\"model\"],\n        cost=result.costs.total_cost,\n        tokens=result.costs.total_tokens\n    )\n\n# View summary\nsummary = tracker.summary()\nprint(f\"Total spend: ${summary['total_cost']:.2f}\")\nprint(f\"Total tokens: {summary['total_tokens']:,}\")\n</code></pre>"},{"location":"guides/cost-control/#related","title":"Related","text":"<ul> <li>Execution Modes - Choose efficient execution strategy</li> <li>API Reference: CostTracker</li> <li>Structured Output - Optimize response parsing</li> </ul>"},{"location":"guides/execution-modes/","title":"Execution Modes","text":"<p>Ondine supports three execution modes optimized for different use cases. Choosing the right mode impacts performance, memory usage, and throughput.</p>"},{"location":"guides/execution-modes/#overview","title":"Overview","text":"Mode Best For Memory Usage Throughput Complexity Standard Small datasets (&lt; 50K rows) High Low-Medium Simple Async High throughput needs High High Medium Streaming Large datasets (100K+ rows) Constant Medium Medium"},{"location":"guides/execution-modes/#standard-execution-default","title":"Standard Execution (Default)","text":"<p>Synchronous, single-threaded processing. The simplest mode.</p>"},{"location":"guides/execution-modes/#usage","title":"Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = PipelineBuilder.create().from_csv(...).build()\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/execution-modes/#characteristics","title":"Characteristics","text":"<ul> <li>Execution: Sequential row-by-row or batch-by-batch</li> <li>Memory: Loads entire dataset into memory</li> <li>Concurrency: None (single-threaded)</li> <li>Return: Complete <code>ExecutionResult</code> with full DataFrame</li> </ul>"},{"location":"guides/execution-modes/#when-to-use","title":"When to Use","text":"<ul> <li>Dataset fits comfortably in memory (&lt; 50K rows typical)</li> <li>Straightforward processing without complex coordination</li> <li>Debugging or testing pipelines</li> <li>Simple scripts and notebooks</li> </ul>"},{"location":"guides/execution-modes/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Dataset is large (&gt; 100K rows)</li> <li>Need maximum throughput</li> <li>Memory is constrained</li> </ul>"},{"location":"guides/execution-modes/#example","title":"Example","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_batch_size(100)\n    .build()\n)\n\n# Simple synchronous execution\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.processed_rows} rows\")\n</code></pre>"},{"location":"guides/execution-modes/#async-execution-concurrent","title":"Async Execution (Concurrent)","text":"<p>Asynchronous processing with configurable concurrency. Maximizes throughput.</p>"},{"location":"guides/execution-modes/#usage_1","title":"Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(...)\n    .with_async_execution(max_concurrency=10)\n    .build()\n)\n\nresult = await pipeline.execute_async()\n</code></pre>"},{"location":"guides/execution-modes/#characteristics_1","title":"Characteristics","text":"<ul> <li>Execution: Concurrent async/await with controlled parallelism</li> <li>Memory: Loads entire dataset into memory</li> <li>Concurrency: Configurable (e.g., 10-50 concurrent requests)</li> <li>Return: Complete <code>ExecutionResult</code> with full DataFrame</li> </ul>"},{"location":"guides/execution-modes/#when-to-use_1","title":"When to Use","text":"<ul> <li>Need high throughput (processing many rows quickly)</li> <li>LLM API supports async (most modern APIs do)</li> <li>Running in async context (FastAPI, aiohttp, async scripts)</li> <li>Dataset fits in memory but need speed</li> <li>Provider has high rate limits</li> </ul>"},{"location":"guides/execution-modes/#when-not-to-use_1","title":"When NOT to Use","text":"<ul> <li>Running in synchronous context (use standard mode instead)</li> <li>Dataset is very large (&gt; 100K rows) - consider streaming</li> <li>Provider has strict rate limits (async may hit limits faster)</li> <li>Memory is constrained</li> </ul>"},{"location":"guides/execution-modes/#example_1","title":"Example","text":"<pre><code>import asyncio\nfrom ondine import PipelineBuilder\n\nasync def process_data():\n    pipeline = (\n        PipelineBuilder.create()\n        .from_csv(\"large_data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n        .with_prompt(\"Analyze: {text}\")\n        .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n        .with_async_execution(max_concurrency=20)  # 20 concurrent requests\n        .with_rate_limit(100)  # Respect API limits\n        .build()\n    )\n\n    result = await pipeline.execute_async()\n    print(f\"Processed {result.metrics.processed_rows} rows\")\n    print(f\"Time: {result.metrics.elapsed_time:.2f}s\")\n    return result\n\n# Run async pipeline\nresult = asyncio.run(process_data())\n</code></pre>"},{"location":"guides/execution-modes/#concurrency-guidelines","title":"Concurrency Guidelines","text":"Provider Recommended Max Concurrency OpenAI (Tier 1) 10-20 OpenAI (Tier 4+) 50-100 Anthropic 10-20 Groq 30-50 Azure OpenAI 10-30 (varies by deployment) Local MLX 1 (no concurrency benefit)"},{"location":"guides/execution-modes/#streaming-execution-memory-efficient","title":"Streaming Execution (Memory-Efficient)","text":"<p>Process data in chunks with constant memory footprint. Best for very large datasets.</p>"},{"location":"guides/execution-modes/#usage_2","title":"Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(...)\n    .with_streaming(chunk_size=1000)\n    .build()\n)\n\nfor chunk_result in pipeline.execute_stream():\n    # Process each chunk as it completes\n    print(f\"Processed chunk: {len(chunk_result.data)} rows\")\n    chunk_result.data.to_csv(\"output.csv\", mode=\"a\", header=False)\n</code></pre>"},{"location":"guides/execution-modes/#characteristics_2","title":"Characteristics","text":"<ul> <li>Execution: Processes data in fixed-size chunks</li> <li>Memory: Constant footprint (1-2 chunks in memory max)</li> <li>Concurrency: Can combine with async for concurrent chunks</li> <li>Return: Iterator yielding <code>ExecutionResult</code> per chunk</li> </ul>"},{"location":"guides/execution-modes/#when-to-use_2","title":"When to Use","text":"<ul> <li>Large datasets (100K+ rows)</li> <li>Limited memory (processing datasets larger than available RAM)</li> <li>Need constant memory footprint</li> <li>Want early/incremental results</li> <li>Processing takes hours/days</li> </ul>"},{"location":"guides/execution-modes/#when-not-to-use_2","title":"When NOT to Use","text":"<ul> <li>Dataset under 50K rows (overhead not justified)</li> <li>Need entire dataset in memory for post-processing</li> <li>Pipeline has dependencies between rows</li> <li>Checkpointing is sufficient for your use case</li> </ul>"},{"location":"guides/execution-modes/#example_2","title":"Example","text":"<pre><code>from ondine import PipelineBuilder\nimport pandas as pd\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"huge_dataset.csv\",  # 500K rows\n        input_columns=[\"text\"],\n        output_columns=[\"summary\"]\n    )\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_streaming(chunk_size=5000)  # Process 5K rows at a time\n    .build()\n)\n\n# Process in streaming fashion\nall_results = []\nfor i, chunk_result in enumerate(pipeline.execute_stream()):\n    print(f\"Chunk {i+1}: {len(chunk_result.data)} rows, \"\n          f\"Cost: ${chunk_result.costs.total_cost:.4f}\")\n\n    # Write incrementally\n    mode = \"w\" if i == 0 else \"a\"\n    header = i == 0\n    chunk_result.data.to_csv(\"output.csv\", mode=mode, header=header, index=False)\n\n    all_results.append(chunk_result)\n\n# Aggregate metrics\ntotal_rows = sum(r.metrics.processed_rows for r in all_results)\ntotal_cost = sum(r.costs.total_cost for r in all_results)\nprint(f\"Total: {total_rows} rows, ${total_cost:.2f}\")\n</code></pre>"},{"location":"guides/execution-modes/#chunk-size-guidelines","title":"Chunk Size Guidelines","text":"Dataset Size Recommended Chunk Size 10K-50K 1,000 50K-100K 2,500 100K-500K 5,000 500K-1M 10,000 1M+ 25,000 <p>Larger chunks = fewer overhead, smaller chunks = finer progress tracking.</p>"},{"location":"guides/execution-modes/#streaming-async-maximum-efficiency","title":"Streaming + Async (Maximum Efficiency)","text":"<p>Combine streaming with async for both memory efficiency and high throughput:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"huge_dataset.csv\", ...)\n    .with_streaming(chunk_size=5000)\n    .with_async_execution(max_concurrency=20)\n    .build()\n)\n\nasync for chunk_result in pipeline.execute_stream_async():\n    # Each chunk processed with 20 concurrent requests\n    print(f\"Chunk done: {len(chunk_result.data)} rows\")\n</code></pre>"},{"location":"guides/execution-modes/#comparison-example","title":"Comparison Example","text":"<p>Processing the same 10K row dataset with different modes:</p>"},{"location":"guides/execution-modes/#standard-mode","title":"Standard Mode","text":"<pre><code># Loads all 10K rows into memory, processes sequentially\nresult = pipeline.execute()\n# Time: ~120s, Memory: ~500MB peak\n</code></pre>"},{"location":"guides/execution-modes/#async-mode","title":"Async Mode","text":"<pre><code># Loads all 10K rows into memory, 20 concurrent requests\nresult = await pipeline.execute_async()\n# Time: ~15s, Memory: ~500MB peak\n</code></pre>"},{"location":"guides/execution-modes/#streaming-mode","title":"Streaming Mode","text":"<pre><code># Processes 1K rows at a time\nfor chunk in pipeline.execute_stream():\n    pass\n# Time: ~110s, Memory: ~50MB constant\n</code></pre>"},{"location":"guides/execution-modes/#streaming-async","title":"Streaming + Async","text":"<pre><code># Processes 1K rows at a time with 20 concurrent requests per chunk\nasync for chunk in pipeline.execute_stream_async():\n    pass\n# Time: ~18s, Memory: ~50MB constant\n</code></pre>"},{"location":"guides/execution-modes/#choosing-the-right-mode","title":"Choosing the Right Mode","text":"<p>Use this decision tree:</p> <pre><code>Is dataset &gt; 100K rows?\n\u251c\u2500 YES \u2192 Use Streaming\n\u2502         \u2514\u2500 Need speed? \u2192 Add Async (streaming + async)\n\u2502\n\u2514\u2500 NO \u2192 Dataset &lt; 100K rows\n         \u251c\u2500 Need maximum speed?\n         \u2502  \u2514\u2500 YES \u2192 Use Async\n         \u2502\n         \u2514\u2500 NO \u2192 Use Standard (simplest)\n</code></pre>"},{"location":"guides/execution-modes/#memory-considerations","title":"Memory Considerations","text":""},{"location":"guides/execution-modes/#standardasync-memory-usage","title":"Standard/Async Memory Usage","text":"<pre><code>Memory = Base + (Dataset Size \u00d7 Row Size)\n</code></pre> <p>Example: 50K rows \u00d7 10KB/row = ~500MB</p>"},{"location":"guides/execution-modes/#streaming-memory-usage","title":"Streaming Memory Usage","text":"<pre><code>Memory = Base + (Chunk Size \u00d7 Row Size)\n</code></pre> <p>Example: 1K chunk \u00d7 10KB/row = ~10MB (constant)</p>"},{"location":"guides/execution-modes/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/execution-modes/#for-all-modes","title":"For All Modes","text":"<ol> <li>Use appropriate batch size: Larger batches = fewer API calls</li> <li>Enable checkpointing: Resume on failures</li> <li>Set rate limits: Respect provider limits</li> <li>Monitor costs: Use budget controls</li> </ol>"},{"location":"guides/execution-modes/#for-async-mode","title":"For Async Mode","text":"<ol> <li>Tune concurrency: Start low, increase gradually</li> <li>Respect rate limits: Too much concurrency can trigger rate limiting</li> <li>Monitor memory: Each concurrent request consumes memory</li> </ol>"},{"location":"guides/execution-modes/#for-streaming-mode","title":"For Streaming Mode","text":"<ol> <li>Choose appropriate chunk size: Balance memory vs. overhead</li> <li>Write incrementally: Don't accumulate all results in memory</li> <li>Enable checkpointing per chunk: More frequent checkpoints</li> </ol>"},{"location":"guides/execution-modes/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/07_async_execution.py</code> - Async processing</li> <li><code>examples/08_streaming_large_files.py</code> - Streaming</li> <li>Cost Control Guide - Budget management</li> <li>API Reference - Complete API docs</li> </ul>"},{"location":"guides/multi-column/","title":"Multi-Column Processing","text":"<p>Generate multiple output columns from a single LLM call using JSON parsing.</p>"},{"location":"guides/multi-column/#basic-usage","title":"Basic Usage","text":"<p>Use JSON parsing to extract multiple fields:</p> <pre><code>from ondine import PipelineBuilder\nfrom ondine.stages.parser_factory import JSONParser\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"description\"],\n        output_columns=[\"brand\", \"category\", \"price\"]\n    )\n    .with_prompt(\"\"\"\n        Extract product information as JSON:\n        {{\n          \"brand\": \"...\",\n          \"category\": \"...\",\n          \"price\": \"...\"\n        }}\n\n        Description: {description}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(JSONParser())\n    .build()\n)\n\nresult = pipeline.execute()\n# Result has 3 new columns: brand, category, price\n</code></pre>"},{"location":"guides/multi-column/#with-pydantic-validation","title":"With Pydantic Validation","text":"<p>For type-safe validation:</p> <pre><code>from pydantic import BaseModel\nfrom ondine.stages.response_parser_stage import PydanticParser\n\nclass ProductInfo(BaseModel):\n    brand: str\n    category: str\n    price: float\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"products.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(PydanticParser(ProductInfo))\n    .build()\n)\n</code></pre>"},{"location":"guides/multi-column/#multiple-input-columns","title":"Multiple Input Columns","text":"<p>Use multiple input columns in your prompt:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"title\", \"description\", \"category\"],\n        output_columns=[\"brand\", \"model\", \"price\"]\n    )\n    .with_prompt(\"\"\"\n        Extract product information:\n        {{\n          \"brand\": \"...\",\n          \"model\": \"...\",\n          \"price\": 0.0\n        }}\n\n        Title: {title}\n        Description: {description}\n        Category: {category}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(JSONParser())\n    .build()\n)\n</code></pre>"},{"location":"guides/multi-column/#related","title":"Related","text":"<ul> <li>Structured Output - Pydantic models</li> <li>Pipeline Composition - Complex workflows</li> </ul>"},{"location":"guides/pipeline-composition/","title":"Pipeline Composition","text":"<p>Compose multiple pipelines to process independent columns with dependencies between them.</p>"},{"location":"guides/pipeline-composition/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder, PipelineComposer\n\n# Pipeline 1: Calculate similarity\nsimilarity_pipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"Calculate similarity (0-1): {text1} vs {text2}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Pipeline 2: Explain (depends on similarity result)\nexplanation_pipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"\"\"\n        The similarity score is {similarity}.\n        Explain why these texts are similar or different:\n        Text 1: {text1}\n        Text 2: {text2}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Compose pipelines\ncomposer = (\n    PipelineComposer(input_data=\"data.csv\")\n    .add_column(\"similarity\", similarity_pipeline)\n    .add_column(\"explanation\", explanation_pipeline, depends_on=[\"similarity\"])\n)\n\nresult = composer.execute()\n</code></pre>"},{"location":"guides/pipeline-composition/#dependencies","title":"Dependencies","text":"<p>Specify dependencies between columns:</p> <pre><code>composer = (\n    PipelineComposer(input_data=df)\n    .add_column(\"category\", category_pipeline)  # No dependencies\n    .add_column(\"sentiment\", sentiment_pipeline)  # No dependencies\n    .add_column(\"recommendation\", recommendation_pipeline, \n                depends_on=[\"category\", \"sentiment\"])  # Depends on both\n)\n</code></pre>"},{"location":"guides/pipeline-composition/#execution-order","title":"Execution Order","text":"<p>Pipelines execute in dependency order: 1. Independent pipelines run first (parallel if async) 2. Dependent pipelines wait for their dependencies 3. Results from previous pipelines are available as input columns</p>"},{"location":"guides/pipeline-composition/#related","title":"Related","text":"<ul> <li>Multi-Column Processing</li> <li>Core Concepts</li> </ul>"},{"location":"guides/structured-output/","title":"Structured Output with Pydantic","text":"<p>Ondine provides type-safe structured output parsing using Pydantic models. This ensures LLM responses conform to your expected schema with automatic validation.</p>"},{"location":"guides/structured-output/#why-use-structured-output","title":"Why Use Structured Output?","text":"<p>Without structured output: <pre><code># Response: \"The brand is Apple and model is iPhone 15 Pro\"\n# Manual parsing required, error-prone, no type safety\n</code></pre></p> <p>With structured output: <pre><code># Response automatically validated and parsed to:\nProductInfo(brand=\"Apple\", model=\"iPhone 15 Pro\", price=999.99, condition=\"new\")\n</code></pre></p> <p>Benefits:</p> <ul> <li>Type safety with Pydantic validation</li> <li>Automatic JSON parsing and error handling</li> <li>Schema enforcement (required fields, types, constraints)</li> <li>IDE autocomplete for response fields</li> <li>Validation errors caught early</li> </ul>"},{"location":"guides/structured-output/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/structured-output/#1-define-your-pydantic-model","title":"1. Define Your Pydantic Model","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass ProductInfo(BaseModel):\n    brand: str = Field(..., description=\"Manufacturer name\")\n    model: str = Field(..., description=\"Product model\")\n    price: float = Field(..., gt=0, description=\"Price in USD\")\n    condition: str = Field(..., pattern=\"^(new|used|refurbished)$\")\n</code></pre>"},{"location":"guides/structured-output/#2-use-with-pipeline","title":"2. Use with Pipeline","text":"<pre><code>from ondine import PipelineBuilder\nfrom ondine.stages.response_parser_stage import PydanticParser\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"product_description\"],\n        output_columns=[\"brand\", \"model\", \"price\", \"condition\"]\n    )\n    .with_prompt(\"\"\"\n        Extract product information and return JSON:\n        {\n          \"brand\": \"manufacturer name\",\n          \"model\": \"product model\",\n          \"price\": 999.99,\n          \"condition\": \"new|used|refurbished\"\n        }\n\n        Description: {product_description}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n    .with_parser(PydanticParser(ProductInfo, strict=True))\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/structured-output/#3-access-validated-results","title":"3. Access Validated Results","text":"<pre><code># Results are automatically validated\nprint(result.data)\n\n#    brand           model      price condition\n# 0  Apple    iPhone 15 Pro    999.99       new\n# 1  Samsung  Galaxy S24      899.99      used\n</code></pre>"},{"location":"guides/structured-output/#pydantic-model-examples","title":"Pydantic Model Examples","text":""},{"location":"guides/structured-output/#simple-model","title":"Simple Model","text":"<pre><code>from pydantic import BaseModel\n\nclass Sentiment(BaseModel):\n    label: str  # \"positive\", \"negative\", \"neutral\"\n    confidence: float  # 0.0 to 1.0\n</code></pre>"},{"location":"guides/structured-output/#model-with-validation","title":"Model with Validation","text":"<pre><code>from pydantic import BaseModel, Field, validator\n\nclass Review(BaseModel):\n    rating: int = Field(..., ge=1, le=5, description=\"Rating from 1-5\")\n    sentiment: str = Field(..., pattern=\"^(positive|negative|neutral)$\")\n    summary: str = Field(..., min_length=10, max_length=200)\n\n    @validator('rating')\n    def rating_must_match_sentiment(cls, v, values):\n        sentiment = values.get('sentiment')\n        if sentiment == 'positive' and v &lt; 4:\n            raise ValueError('Positive sentiment requires rating &gt;= 4')\n        if sentiment == 'negative' and v &gt; 2:\n            raise ValueError('Negative sentiment requires rating &lt;= 2')\n        return v\n</code></pre>"},{"location":"guides/structured-output/#nested-model","title":"Nested Model","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n    postal_code: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n    addresses: List[Address]\n</code></pre>"},{"location":"guides/structured-output/#model-with-optional-fields","title":"Model with Optional Fields","text":"<pre><code>from pydantic import BaseModel\nfrom typing import Optional\n\nclass Product(BaseModel):\n    name: str\n    brand: str\n    price: float\n    description: Optional[str] = None  # Optional field\n    sku: Optional[str] = None\n</code></pre>"},{"location":"guides/structured-output/#model-with-enums","title":"Model with Enums","text":"<pre><code>from pydantic import BaseModel\nfrom enum import Enum\n\nclass Category(str, Enum):\n    ELECTRONICS = \"electronics\"\n    CLOTHING = \"clothing\"\n    FOOD = \"food\"\n    BOOKS = \"books\"\n\nclass Item(BaseModel):\n    name: str\n    category: Category\n    price: float\n</code></pre>"},{"location":"guides/structured-output/#complete-example","title":"Complete Example","text":"<pre><code>from pydantic import BaseModel, Field\nfrom ondine import PipelineBuilder\nfrom ondine.stages.response_parser_stage import PydanticParser\nimport pandas as pd\n\n# Define schema\nclass EmailClassification(BaseModel):\n    category: str = Field(..., pattern=\"^(spam|important|promotional|personal)$\")\n    confidence: float = Field(..., ge=0.0, le=1.0)\n    priority: int = Field(..., ge=1, le=5)\n    action: str = Field(..., pattern=\"^(archive|flag|delete|respond)$\")\n\n# Sample data\ndata = pd.DataFrame({\n    \"email\": [\n        \"URGENT: You won $1,000,000! Click here now!\",\n        \"Meeting tomorrow at 2pm with the CEO\",\n        \"50% off sale this weekend only!\"\n    ]\n})\n\n# Build pipeline\npipeline = (\n    PipelineBuilder.create()\n    .from_dataframe(\n        data,\n        input_columns=[\"email\"],\n        output_columns=[\"category\", \"confidence\", \"priority\", \"action\"]\n    )\n    .with_prompt(\"\"\"\n        Classify this email and return JSON:\n        {{\n          \"category\": \"spam|important|promotional|personal\",\n          \"confidence\": 0.0-1.0,\n          \"priority\": 1-5,\n          \"action\": \"archive|flag|delete|respond\"\n        }}\n\n        Email: {email}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n    .with_parser(PydanticParser(EmailClassification, strict=True))\n    .build()\n)\n\n# Execute with type-safe validation\nresult = pipeline.execute()\nprint(result.data)\n</code></pre> <p>Output: <pre><code>   email                                   category  confidence  priority   action\n0  URGENT: You won $1,000,000! Click...     spam        0.98         5    delete\n1  Meeting tomorrow at 2pm with CEO      important      0.95         1      flag\n2  50% off sale this weekend only!    promotional      0.92         3   archive\n</code></pre></p>"},{"location":"guides/structured-output/#strict-vs-non-strict-mode","title":"Strict vs Non-Strict Mode","text":""},{"location":"guides/structured-output/#strict-mode-recommended","title":"Strict Mode (Recommended)","text":"<pre><code>.with_parser(PydanticParser(ProductInfo, strict=True))\n</code></pre> <ul> <li>Validation errors stop processing</li> <li>Failed rows are retried (if retry policy configured)</li> <li>Guarantees all results match schema</li> </ul>"},{"location":"guides/structured-output/#non-strict-mode","title":"Non-Strict Mode","text":"<pre><code>.with_parser(PydanticParser(ProductInfo, strict=False))\n</code></pre> <ul> <li>Validation errors logged but processing continues</li> <li>Invalid rows get <code>None</code> values</li> <li>Useful for exploratory analysis</li> </ul>"},{"location":"guides/structured-output/#handling-validation-errors","title":"Handling Validation Errors","text":""},{"location":"guides/structured-output/#with-retries","title":"With Retries","text":"<pre><code>pipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_parser(PydanticParser(ProductInfo, strict=True))\n    .with_retry_policy(max_retries=3)  # Retry validation failures\n    .build()\n)\n\nresult = pipeline.execute()\n\n# Check for failed validations\nif result.metrics.failed_rows &gt; 0:\n    print(f\"Failed to validate {result.metrics.failed_rows} rows\")\n    failed = result.data[result.data['brand'].isna()]\n    print(failed)\n</code></pre>"},{"location":"guides/structured-output/#custom-error-handling","title":"Custom Error Handling","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    result = pipeline.execute()\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n    # Handle validation failures\n</code></pre>"},{"location":"guides/structured-output/#prompt-engineering-for-structured-output","title":"Prompt Engineering for Structured Output","text":""},{"location":"guides/structured-output/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Show example JSON in prompt: <pre><code>prompt = \"\"\"\nExtract product info as JSON:\n{{\n  \"brand\": \"Apple\",\n  \"model\": \"iPhone 15\",\n  \"price\": 999.99\n}}\n\nDescription: {description}\n\"\"\"\n</code></pre></p> </li> <li> <p>Specify field constraints: <pre><code>prompt = \"\"\"\nAnalyze sentiment and return JSON:\n{{\n  \"label\": \"positive|negative|neutral\",\n  \"confidence\": 0.0-1.0,\n  \"keywords\": [\"word1\", \"word2\"]\n}}\n\nText: {text}\n\"\"\"\n</code></pre></p> </li> <li> <p>Use low temperature: <pre><code>.with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n</code></pre></p> </li> <li> <p>Include field descriptions: <pre><code>class Product(BaseModel):\n    brand: str = Field(..., description=\"Manufacturer name (e.g., Apple, Samsung)\")\n    price: float = Field(..., description=\"Price in USD, numeric only\")\n</code></pre></p> </li> </ol>"},{"location":"guides/structured-output/#json-vs-pydantic-parser","title":"JSON vs Pydantic Parser","text":""},{"location":"guides/structured-output/#json-parser-simple","title":"JSON Parser (Simple)","text":"<pre><code>from ondine.stages.parser_factory import JSONParser\n\n# Just parses JSON, no validation\n.with_parser(JSONParser())\n</code></pre> <p>Use when: - Schema is simple and flexible - Don't need type validation - Rapid prototyping</p>"},{"location":"guides/structured-output/#pydantic-parser-type-safe","title":"Pydantic Parser (Type-Safe)","text":"<pre><code>from ondine.stages.response_parser_stage import PydanticParser\n\n# Parses AND validates against schema\n.with_parser(PydanticParser(MyModel, strict=True))\n</code></pre> <p>Use when: - Need type safety and validation - Schema has constraints (ranges, patterns) - Production applications - API responses</p>"},{"location":"guides/structured-output/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/structured-output/#multiple-models","title":"Multiple Models","text":"<p>For different output types:</p> <pre><code>from typing import Union\n\nclass ShortSummary(BaseModel):\n    summary: str = Field(..., max_length=100)\n\nclass LongSummary(BaseModel):\n    summary: str = Field(..., max_length=500)\n    key_points: List[str]\n\n# Use Union types\nclass SummaryResponse(BaseModel):\n    content: Union[ShortSummary, LongSummary]\n    type: str\n</code></pre>"},{"location":"guides/structured-output/#post-validation-processing","title":"Post-Validation Processing","text":"<pre><code>from pydantic import BaseModel, validator\n\nclass Price(BaseModel):\n    amount: float\n    currency: str = \"USD\"\n\n    @validator('amount')\n    def round_price(cls, v):\n        return round(v, 2)\n\n    @property\n    def formatted(self) -&gt; str:\n        return f\"${self.amount:.2f}\"\n</code></pre>"},{"location":"guides/structured-output/#dynamic-schema","title":"Dynamic Schema","text":"<p>For runtime schema definition:</p> <pre><code>from pydantic import create_model\n\n# Create model dynamically\nfields = {\n    \"name\": (str, ...),\n    \"age\": (int, ...),\n    \"email\": (str, ...)\n}\n\nDynamicModel = create_model(\"DynamicModel\", **fields)\n\npipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_parser(PydanticParser(DynamicModel))\n    .build()\n)\n</code></pre>"},{"location":"guides/structured-output/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/structured-output/#validation-overhead","title":"Validation Overhead","text":"<p>Pydantic validation adds ~1-5ms per row. Negligible for most use cases.</p> <pre><code># For 10K rows:\n# - Without validation: ~120s\n# - With Pydantic: ~120.05s (0.04% overhead)\n</code></pre>"},{"location":"guides/structured-output/#complex-models","title":"Complex Models","text":"<p>Deeply nested models increase validation time:</p> <pre><code># Simple model: ~1ms\nclass Simple(BaseModel):\n    name: str\n    value: float\n\n# Complex nested: ~5ms\nclass Complex(BaseModel):\n    data: List[Dict[str, List[SubModel]]]\n</code></pre> <p>Tip: Keep models as flat as possible for best performance.</p>"},{"location":"guides/structured-output/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/structured-output/#common-validation-errors","title":"Common Validation Errors","text":"<p>Missing required field: <pre><code>ValidationError: field required (type=value_error.missing)\n</code></pre> Solution: Ensure LLM outputs all required fields in prompt.</p> <p>Type mismatch: <pre><code>ValidationError: value is not a valid float (type=type_error.float)\n</code></pre> Solution: Add type hints in prompt, use temperature=0.0.</p> <p>Pattern mismatch: <pre><code>ValidationError: string does not match regex (type=value_error.str.regex)\n</code></pre> Solution: Show valid values in prompt example.</p>"},{"location":"guides/structured-output/#debugging-tips","title":"Debugging Tips","text":"<ol> <li> <p>Test with small sample first: <pre><code>df_sample = df.head(10)\npipeline = builder.from_dataframe(df_sample, ...).build()\n</code></pre></p> </li> <li> <p>Use non-strict mode for debugging: <pre><code>.with_parser(PydanticParser(Model, strict=False))\n</code></pre></p> </li> <li> <p>Check raw responses: <pre><code># Enable debug logging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> </ol>"},{"location":"guides/structured-output/#related","title":"Related","text":"<ul> <li>API Reference: PydanticParser</li> <li>Example: 03_structured_output.py</li> <li>Cost Control - Optimize costs</li> <li>Multi-Column Processing - Multiple outputs</li> </ul>"},{"location":"guides/providers/anthropic/","title":"Anthropic Claude Provider","text":"<p>Configure and use Anthropic Claude models with Ondine.</p>"},{"location":"guides/providers/anthropic/#setup","title":"Setup","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"guides/providers/anthropic/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"analysis\"])\n    .with_prompt(\"Analyze: {text}\")\n    .with_llm(\n        provider=\"anthropic\",\n        model=\"claude-3-5-sonnet-20241022\",\n        temperature=0.0,\n        max_tokens=1024\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/anthropic/#available-models","title":"Available Models","text":"<ul> <li><code>claude-3-5-sonnet-20241022</code> - Most capable (recommended)</li> <li><code>claude-3-5-haiku-20241022</code> - Fast and cost-effective</li> <li><code>claude-3-opus-20240229</code> - Most capable, legacy</li> </ul>"},{"location":"guides/providers/anthropic/#configuration-options","title":"Configuration Options","text":"<pre><code>.with_llm(\n    provider=\"anthropic\",\n    model=\"claude-3-5-sonnet-20241022\",\n    temperature=0.7,\n    max_tokens=4096,\n    top_p=1.0\n)\n</code></pre>"},{"location":"guides/providers/anthropic/#rate-limits","title":"Rate Limits","text":"<p>Recommended concurrency: 10-20</p>"},{"location":"guides/providers/anthropic/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>Cost Control</li> </ul>"},{"location":"guides/providers/azure/","title":"Azure OpenAI Provider","text":"<p>Configure and use Azure OpenAI Service with Ondine.</p>"},{"location":"guides/providers/azure/#setup","title":"Setup","text":"<pre><code>export AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-15-preview\"\n</code></pre>"},{"location":"guides/providers/azure/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=\"https://your-resource.openai.azure.com/\",\n        azure_deployment=\"your-deployment-name\",\n        api_version=\"2024-02-15-preview\"\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/azure/#managed-identity-authentication-recommended-for-azure","title":"Managed Identity Authentication (Recommended for Azure)","text":"<p>For applications running on Azure infrastructure (VMs, Container Apps, Functions), use Managed Identity for keyless authentication:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=\"https://your-resource.openai.azure.com/\",\n        azure_deployment=\"your-deployment-name\",\n        use_managed_identity=True  # No API key needed!\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/azure/#prerequisites","title":"Prerequisites","text":"<ol> <li>Assign a Managed Identity to your Azure resource</li> <li>Grant the identity \"Cognitive Services OpenAI User\" role on your Azure OpenAI resource</li> <li>Install azure-identity: <code>pip install ondine[azure]</code></li> </ol>"},{"location":"guides/providers/azure/#how-it-works","title":"How It Works","text":"<ul> <li>Production (Azure VM/Container): Uses system-assigned or user-assigned Managed Identity</li> <li>Local Development: Uses Azure CLI credentials (<code>az login</code>)</li> <li>CI/CD: Uses service principal or federated credentials</li> </ul> <p>No API keys stored in code or environment variables!</p>"},{"location":"guides/providers/azure/#configuration","title":"Configuration","text":"<p>The deployment name in Azure OpenAI maps to the model:</p> <pre><code>.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",  # Your base model\n    azure_deployment=\"my-gpt4-deployment\",  # Your Azure deployment name\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    api_version=\"2024-02-15-preview\"\n)\n</code></pre>"},{"location":"guides/providers/azure/#rate-limits","title":"Rate Limits","text":"<p>Rate limits in Azure OpenAI are configured per deployment. Adjust concurrency based on your TPM (tokens per minute) limits.</p>"},{"location":"guides/providers/azure/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>Cost Control</li> </ul>"},{"location":"guides/providers/custom/","title":"Custom OpenAI-Compatible APIs","text":"<p>Integrate any OpenAI-compatible API with Ondine, including Together.AI, vLLM, Ollama, and custom endpoints.</p>"},{"location":"guides/providers/custom/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(\n        provider=\"openai\",  # Use openai provider\n        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n        api_base=\"https://api.together.xyz/v1\",  # Custom endpoint\n        api_key=os.getenv(\"TOGETHER_API_KEY\")\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/custom/#provider-examples","title":"Provider Examples","text":""},{"location":"guides/providers/custom/#togetherai","title":"Together.AI","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    api_base=\"https://api.together.xyz/v1\",\n    api_key=os.getenv(\"TOGETHER_API_KEY\")\n)\n</code></pre>"},{"location":"guides/providers/custom/#vllm-self-hosted","title":"vLLM (Self-Hosted)","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    api_base=\"http://localhost:8000/v1\",\n    api_key=\"dummy\"  # vLLM doesn't require auth\n)\n</code></pre>"},{"location":"guides/providers/custom/#ollama-local","title":"Ollama (Local)","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"llama2\",\n    api_base=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"  # Any non-empty string\n)\n</code></pre>"},{"location":"guides/providers/custom/#custom-endpoint","title":"Custom Endpoint","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"your-model-name\",\n    api_base=\"https://your-api.example.com/v1\",\n    api_key=os.getenv(\"YOUR_API_KEY\")\n)\n</code></pre>"},{"location":"guides/providers/custom/#using-llmspec-advanced","title":"Using LLMSpec (Advanced)","text":"<p>For more control, use <code>LLMSpec</code>:</p> <pre><code>from ondine.core.specifications import LLMSpec\n\ncustom_spec = LLMSpec(\n    provider=\"openai\",\n    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    api_base=\"https://api.together.xyz/v1\",\n    api_key=os.getenv(\"TOGETHER_API_KEY\"),\n    temperature=0.7,\n    max_tokens=1000,\n    # Pricing (optional, for cost tracking)\n    input_cost_per_million=0.20,\n    output_cost_per_million=0.20\n)\n\npipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_llm_spec(custom_spec)\n    .build()\n)\n</code></pre>"},{"location":"guides/providers/custom/#requirements","title":"Requirements","text":"<p>The custom API must be OpenAI-compatible, supporting: - <code>/v1/chat/completions</code> endpoint - Standard OpenAI request/response format - Bearer token authentication</p>"},{"location":"guides/providers/custom/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/providers/custom/#connection-errors","title":"Connection Errors","text":"<p>Check that the API endpoint is accessible: <pre><code>curl -X POST https://api.example.com/v1/chat/completions \\\n  -H \"Authorization: Bearer YOUR_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\":\"test\",\"messages\":[{\"role\":\"user\",\"content\":\"test\"}]}'\n</code></pre></p>"},{"location":"guides/providers/custom/#authentication-errors","title":"Authentication Errors","text":"<p>Verify your API key is correct and has proper permissions.</p>"},{"location":"guides/providers/custom/#model-not-found","title":"Model Not Found","text":"<p>Ensure the model name matches what the API expects.</p>"},{"location":"guides/providers/custom/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>API Reference: LLMSpec</li> </ul>"},{"location":"guides/providers/groq/","title":"Groq Provider","text":"<p>Configure and use Groq for ultra-fast inference with Ondine.</p>"},{"location":"guides/providers/groq/#setup","title":"Setup","text":"<pre><code>export GROQ_API_KEY=\"gsk_...\"\n</code></pre>"},{"location":"guides/providers/groq/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/groq/#available-models","title":"Available Models","text":"<ul> <li><code>llama-3.3-70b-versatile</code> - Best performance</li> <li><code>llama-3.1-70b-versatile</code> - Fast and capable</li> <li><code>mixtral-8x7b-32768</code> - Long context window</li> </ul>"},{"location":"guides/providers/groq/#configuration-options","title":"Configuration Options","text":"<pre><code>.with_llm(\n    provider=\"groq\",\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.7,\n    max_tokens=1000\n)\n</code></pre>"},{"location":"guides/providers/groq/#performance","title":"Performance","text":"<p>Groq is optimized for speed. Recommended concurrency: 30-50</p>"},{"location":"guides/providers/groq/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>Execution Modes</li> </ul>"},{"location":"guides/providers/local-mlx/","title":"Local MLX Provider (Apple Silicon)","text":"<p>Run models locally on Apple Silicon (M1/M2/M3/M4) with MLX - 100% free, private, offline-capable.</p>"},{"location":"guides/providers/local-mlx/#requirements","title":"Requirements","text":"<ul> <li>macOS with Apple Silicon (M1/M2/M3/M4)</li> <li>16GB+ RAM recommended</li> <li>Python 3.10+</li> </ul>"},{"location":"guides/providers/local-mlx/#installation","title":"Installation","text":"<pre><code>pip install ondine[mlx]\n</code></pre>"},{"location":"guides/providers/local-mlx/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"summary\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(\n        provider=\"mlx\",\n        model=\"mlx-community/Qwen2.5-7B-Instruct-4bit\",\n        temperature=0.7,\n        max_tokens=500\n    )\n    .build()\n)\n\nresult = pipeline.execute()\nprint(f\"Cost: ${result.costs.total_cost:.2f}\")  # Always $0.00\n</code></pre>"},{"location":"guides/providers/local-mlx/#available-models","title":"Available Models","text":"<p>Any MLX-compatible model from Hugging Face:</p> <ul> <li><code>mlx-community/Qwen2.5-7B-Instruct-4bit</code> - Recommended, fast</li> <li><code>mlx-community/Llama-3.2-3B-Instruct-4bit</code> - Lightweight</li> <li><code>mlx-community/Mistral-7B-Instruct-v0.3-4bit</code> - Good quality</li> </ul>"},{"location":"guides/providers/local-mlx/#configuration","title":"Configuration","text":"<pre><code>.with_llm(\n    provider=\"mlx\",\n    model=\"mlx-community/Qwen2.5-7B-Instruct-4bit\",\n    temperature=0.7,\n    max_tokens=1000\n)\n</code></pre>"},{"location":"guides/providers/local-mlx/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>No concurrency benefit: MLX runs on single GPU, use concurrency=1</li> <li>First run slower: Model downloads and caches on first use</li> <li>Memory usage: 4-bit models use ~4-8GB RAM</li> </ul>"},{"location":"guides/providers/local-mlx/#benefits","title":"Benefits","text":"<ul> <li>Zero cost: No API fees</li> <li>Privacy: Data never leaves your machine</li> <li>Offline: Works without internet</li> <li>No rate limits: Process as much as you want</li> </ul>"},{"location":"guides/providers/local-mlx/#limitations","title":"Limitations","text":"<ul> <li>Only works on Apple Silicon Macs</li> <li>Slower than cloud APIs (but free!)</li> <li>No concurrency benefit</li> </ul>"},{"location":"guides/providers/local-mlx/#related","title":"Related","text":"<ul> <li>Cost Control</li> <li>Custom Providers</li> </ul>"},{"location":"guides/providers/openai/","title":"OpenAI Provider","text":"<p>Configure and use OpenAI models with Ondine.</p>"},{"location":"guides/providers/openai/#setup","title":"Setup","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"guides/providers/openai/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/openai/#available-models","title":"Available Models","text":"<ul> <li><code>gpt-4o</code> - Most capable, balanced performance</li> <li><code>gpt-4o-mini</code> - Fast and cost-effective (recommended)</li> <li><code>gpt-4-turbo</code> - Advanced reasoning</li> <li><code>gpt-3.5-turbo</code> - Legacy, cost-effective</li> </ul>"},{"location":"guides/providers/openai/#configuration-options","title":"Configuration Options","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,      # 0.0-2.0\n    max_tokens=1000,      # Max response length\n    top_p=1.0,            # Nucleus sampling\n    frequency_penalty=0.0,  # -2.0 to 2.0\n    presence_penalty=0.0    # -2.0 to 2.0\n)\n</code></pre>"},{"location":"guides/providers/openai/#rate-limits","title":"Rate Limits","text":"<p>See OpenAI Rate Limits for your tier.</p> <p>Recommended concurrency: - Tier 1: 10-20 - Tier 4+: 50-100</p>"},{"location":"guides/providers/openai/#related","title":"Related","text":"<ul> <li>Azure OpenAI</li> <li>Cost Control</li> </ul>"}]}